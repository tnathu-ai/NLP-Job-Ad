{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "\n",
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Task 1. Basic Text Pre-processing</strong></h3>\n",
    "\n",
    "#### Student Name: Tran Ngoc Anh Thu\n",
    "#### Student ID: s3879312\n",
    "\n",
    "Date: \"October 2, 2022\"\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* sklearn\n",
    "* collections\n",
    "* re\n",
    "* numpy\n",
    "* nltk\n",
    "* itertools\n",
    "* pandas\n",
    "* os\n",
    "\n",
    "## Steps\n",
    "1. Load data\n",
    "2. Text Pre-processing\n",
    "    * Sentence Segmentation\n",
    "    * Word Tokenization\n",
    "    * Removing Single Character Tokens\n",
    "    * Removing Stop words\n",
    "3. Saving the Pre-processing Reviews\n",
    "\n",
    "## Introduction\n",
    "Nowadays there are many job hunting websites including seek.com.au and au.indeed.com. These job hunting sites all manage a job search system, where job hunters could search for relevant jobs based on keywords, salary, and categories. In previous years, the category of an advertised job was often manually entered by the advertiser (e.g., the employer). There were mistakes made for category assignment. As a result, the jobs in the wrong class did not get enough exposure to relevant candidate groups.\n",
    "With advances in text analysis, automated job classification has become feasible; and sensible suggestions for job categories can then be made to potential advertisers. This can help reduce human data entry error, increase the job exposure to relevant candidates, and also improve the user experience of the job hunting site. In order to do so, we need an automated job ads classification system that helps to predict the categories of newly entered job advertisements.\n",
    "\n",
    "In this **task1** notebook, we are going to explore a job advertisement data set, and focus on pre-processing the description only.\n",
    "In the next task **task2_3**, we will then use the pre-processed text reviews to generate data features and build classification models to predict label of the description.\n",
    "\n",
    "## Dataset\n",
    "+ A small collection of job advertisement documents (around 776 jobs) inside the `data` folder.\n",
    "+ Inside the data folder, there are four different sub-folders: Accounting_Finance, Engineering, Healthcare_Nursing, and Sales, representing a job category.\n",
    "+ The job advertisement text documents of a particular category are in the corresponding sub-folder.\n",
    "+ Each job advertisement document is a txt file named `Job_<ID>.txt`. It contains the title, the webindex (some will also have information on the company name, some might not), and the full description of the job advertisement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "from collections import Counter\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.21.5\n",
      "Pandas version:  1.4.2\n",
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# check the version of the main packages\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Pandas version: \",pd.__version__)\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.1 Examining and loading data</strong></h3>\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n",
    "\n",
    "\n",
    "\n",
    "Before doing any pre-processing, we need to load the data into a proper format. \n",
    "To load the data, you have to explore the data folder. Inside the `data` folder:\n",
    "+ Inside the data folder you will see 4 different subfolders, namely: `Accounting_Finance`, `Engineering`,`Healthcare_Nursing`, and `Sales`, each folder name is a job category.\n",
    "+ The job advertisement text documents of a particular category are located in the corresponding subfolder.\n",
    "+ Each job advertisement document is a txt file, named as \"Job_<ID>.txt\". It contains the title, the webindex,(some will also have information on the company name, some might not), and the full description of the job advertisement. \n",
    "\n",
    "In this case, providing that the dataset is given in a very well organised way, I would use a super handy API [`load_files`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) from `sklearn.datasets`. \n",
    "    \n",
    "**import the function by:**\n",
    "```python\n",
    "from sklearn.datasets import load_files  \n",
    "```\n",
    "\n",
    "Then you can use the function to directly load the data and labels, for example:\n",
    "```python\n",
    "df = load_files(r\"data\")  \n",
    "```\n",
    "\n",
    "The loaded `movie_data` is then a dictionary, with the following attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **DESCRIPTION**                                           |\n",
    "|--------------|---------------------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website            |\n",
    "| Title        | Title of the advertised job position                          |\n",
    "| Company      | Company (employer) of the advertised job position             |\n",
    "| Description  | the description of each job advertisement                     |\n",
    "\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each folder and file inside the data folder\n",
    "df = load_files(r\"data\")\n",
    "# type of the loaded file\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 2, 1, 2, 0, 3, 3, 0, 0, 1, 3, 1, 3, 3, 1, 3, 2, 2, 2,\n",
       "       3, 3, 0, 2, 2, 2, 0, 2, 3, 1, 2, 0, 1, 3, 3, 1, 1, 0, 2, 2, 2, 2,\n",
       "       0, 0, 2, 1, 3, 1, 1, 2, 2, 3, 0, 0, 1, 0, 2, 2, 3, 3, 3, 0, 3, 0,\n",
       "       1, 2, 3, 1, 3, 2, 3, 1, 3, 2, 1, 3, 2, 1, 3, 2, 2, 1, 0, 1, 1, 1,\n",
       "       3, 0, 3, 1, 3, 2, 2, 0, 2, 3, 2, 1, 0, 1, 1, 2, 0, 3, 0, 1, 3, 2,\n",
       "       1, 2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3,\n",
       "       2, 0, 0, 1, 3, 2, 0, 1, 0, 3, 1, 2, 1, 0, 0, 0, 3, 0, 1, 2, 3, 1,\n",
       "       1, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 0, 2, 3, 2, 2, 0, 2,\n",
       "       1, 0, 1, 1, 1, 3, 1, 3, 1, 0, 3, 1, 0, 2, 0, 0, 2, 1, 1, 0, 1, 3,\n",
       "       0, 1, 1, 3, 0, 1, 0, 2, 3, 0, 2, 0, 1, 0, 1, 3, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 2, 1, 3, 1, 2, 3, 1, 1, 2, 0, 0, 1, 2, 0, 3, 2, 3, 2, 2, 3,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 3, 1, 1, 0, 0, 2, 1, 2, 2, 2, 2, 1, 3, 1,\n",
       "       2, 1, 2, 3, 2, 3, 0, 1, 3, 0, 2, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 2,\n",
       "       2, 1, 2, 0, 2, 2, 1, 2, 0, 1, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 0, 1,\n",
       "       0, 0, 0, 0, 2, 2, 0, 0, 2, 1, 2, 0, 0, 1, 2, 3, 1, 3, 2, 0, 2, 2,\n",
       "       1, 3, 1, 0, 1, 2, 3, 1, 0, 0, 1, 1, 0, 2, 1, 3, 2, 3, 3, 1, 3, 1,\n",
       "       2, 1, 1, 2, 2, 3, 3, 2, 2, 1, 1, 2, 0, 2, 3, 0, 0, 1, 0, 1, 2, 1,\n",
       "       1, 3, 0, 2, 0, 1, 3, 1, 3, 2, 0, 2, 1, 3, 0, 0, 0, 3, 3, 0, 2, 3,\n",
       "       3, 1, 3, 0, 1, 0, 3, 1, 3, 1, 3, 2, 1, 1, 2, 3, 0, 0, 3, 3, 3, 1,\n",
       "       2, 2, 3, 0, 0, 0, 1, 0, 2, 1, 3, 2, 1, 2, 2, 1, 2, 2, 0, 0, 2, 3,\n",
       "       2, 1, 2, 3, 1, 2, 1, 0, 1, 2, 1, 2, 0, 3, 2, 3, 2, 3, 0, 0, 3, 2,\n",
       "       0, 2, 1, 3, 0, 3, 2, 1, 1, 2, 1, 2, 3, 1, 1, 1, 3, 1, 2, 1, 0, 0,\n",
       "       3, 0, 2, 2, 3, 3, 0, 3, 1, 1, 2, 2, 2, 3, 2, 1, 1, 0, 2, 3, 0, 0,\n",
       "       0, 0, 2, 1, 3, 0, 2, 1, 2, 2, 3, 1, 2, 0, 2, 0, 3, 2, 1, 2, 2, 3,\n",
       "       1, 1, 0, 2, 2, 1, 1, 3, 0, 1, 0, 0, 2, 3, 3, 0, 0, 1, 1, 2, 3, 3,\n",
       "       3, 0, 0, 2, 3, 1, 3, 1, 3, 3, 1, 0, 0, 0, 1, 0, 3, 1, 2, 2, 1, 1,\n",
       "       0, 1, 2, 2, 3, 3, 3, 1, 2, 3, 1, 0, 1, 3, 3, 3, 3, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 2, 2, 1, 3, 2, 2, 2, 1, 2, 1, 0, 2, 1, 2, 2, 0, 2,\n",
       "       0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 3, 1, 2, 1, 2, 0, 1, 1, 1, 1, 0, 2,\n",
       "       3, 1, 1, 2, 3, 2, 0, 1, 3, 3, 1, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0, 3, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,\n",
       "       1, 2, 1, 0, 3, 3, 0, 2, 1, 2, 1, 1, 3, 0, 0, 0, 3, 0, 1, 1, 0, 3,\n",
       "       2, 2, 1, 0, 0, 2, 0, 1, 0, 1, 1, 3, 2, 0, 2, 1, 1, 1, 2, 2, 0, 3,\n",
       "       0, 0, 0, 2, 2, 3, 0, 1, 3, 2, 2, 3, 3, 1, 1, 2, 0, 3, 0, 0, 2, 2,\n",
       "       3, 0, 0, 2, 3, 3, 0, 0, 2, 2, 3, 1, 1, 2, 2, 0, 2, 3, 1, 3, 0, 1,\n",
       "       3, 3, 1, 3, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] # this corresponding to the index value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the categories\n",
    "df['target_names'] # this corresponding to the name value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category at index 0: Accounting_Finance\n",
      "Category at index 1: Engineering\n",
      "Category at index 2: Healthcare_Nursing\n",
      "Category at index 3: Sales\n"
     ]
    }
   ],
   "source": [
    "print(f'Category at index 0: {df[\"target_names\"][0]}')\n",
    "print(f'Category at index 1: {df[\"target_names\"][1]}')\n",
    "print(f'Category at index 2: {df[\"target_names\"][2]}')\n",
    "print(f'Category at index 3: {df[\"target_names\"][3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/Healthcare_Nursing/Job_00491.txt', 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether it matches, just in case\n",
    "emp = 20 # an example, note we will use this example to test for the whole task outputs.\n",
    "\n",
    "df['filenames'][emp], df['target'][emp] # from the file path we know that it's the correct label too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job description: b'Title: PERM Unit Mgr RGN Kid minster Flexi ****K due\\nWebindex: 71692209\\nDescription: Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home\\xe2\\x80\\x99s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home\\xe2\\x80\\x99s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home\\xe2\\x80\\x99s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home\\xe2\\x80\\x99s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user\\xe2\\x80\\x99s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user\\xe2\\x80\\x99s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home\\xe2\\x80\\x99s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home\\xe2\\x80\\x99s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home\\xe2\\x80\\x99s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user\\xe2\\x80\\x99s stay as comfortable and enjoyable as possible.'\n",
      "\n",
      "Corresponding to the label 2 inside the data/Healthcare_Nursing/Job_00491.txt directory\n"
     ]
    }
   ],
   "source": [
    "# assign variables\n",
    "full_description, category, directory = df.data, df.target, df.filenames\n",
    "\n",
    "# the 20th job advertisement description\n",
    "print(f'Job description: {full_description[emp]}\\n\\nCorresponding to the label {category[emp]} inside the {directory[emp]} directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ------> OBSERVATION:\n",
    "As we can see the current description is in the **binary** form and read as a byte object (a `b` in front of each review text if you print it out). Therefore, we need to decode into normal string for further pre-processing\n",
    "\n",
    "However, the tokenizer cannot apply a string pattern on a bytes-like object. To resolve this, we decode each read `full_description` text using `utf-8` by writing a decode function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: PERM Unit Mgr RGN Kid minster Flexi ****K due\\nWebindex: 71692209\\nDescription: Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home’s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home’s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home’s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home’s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user’s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user’s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home’s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home’s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home’s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user’s stay as comfortable and enjoyable as possible.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(l):\n",
    "    if isinstance(l, list):\n",
    "        return [decode(x) for x in l]\n",
    "    else:\n",
    "        return l.decode('utf-8')\n",
    "\n",
    "# decode the binary description into utf-8 form and save it to full_description\n",
    "full_description = decode(full_description)\n",
    "full_description[emp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ---------------> OBSERVATION:\n",
    "The current `description` contains these attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **MEANING**                                        |\n",
    "|--------------|----------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website |\n",
    "| Title        | Title of the advertised job position               |\n",
    "| Company      | Company (employer) of the advertised job position  |\n",
    "| Description  | the description of each job advertisement          |\n",
    "\n",
    "and I only want the description itself to perform text-preprocessing and NLP on `description`. Therefore, I will perform the following pre-processing steps to the description of each job advertisement;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.2 Pre-processing</strong></h3>\n",
    "\n",
    "## 1.2.1 Extract information from each job advertisement\n",
    "1. Extract information from each job advertisement. Perform the following pre-processing steps to the description of each job advertisement;\n",
    "2. Tokenize each job advertisement description. The word tokenization must use the following regular expression, `r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"`;\n",
    "3. All the words must be converted into the lower case;\n",
    "4. Remove words with length less than 2.\n",
    "5. Remove stopwords using the provided stop words list (i.e, stopwords_en.txt). It is located inside the\n",
    "same downloaded folder.\n",
    "6. Remove the word that appears only once in the document collection, based on term frequency.\n",
    "7. Remove the top 50 most frequent words based on document frequency.\n",
    "8. Save all job advertisement text and information in txt file(s) (you have flexibility to choose what format\n",
    "you want to save the preprocessed job ads, and you will need to retrieve the pre-processed job ads\n",
    "text in Task 2 & 3);\n",
    "9. Build a vocabulary of the cleaned job advertisement descriptions, save it in a txt file (please refer to the\n",
    "required output);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job description at index 20:\n",
      "Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home’s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home’s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home’s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home’s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user’s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user’s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home’s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home’s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home’s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user’s stay as comfortable and enjoyable as possible.\n",
      "\n",
      "Job title at index 20:\n",
      "PERM Unit Mgr RGN Kid minster Flexi ****K due\n",
      "\n",
      "Job webindex at index 20:\n",
      "71692209\n",
      "\n",
      "Job company at index 20:\n",
      "NA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract description, title, webindex,  from each job advertisement and test using emp\n",
    "\n",
    "# Extract description\n",
    "def extract_description(full_description):\n",
    "    description = [re.search(r'\\nDescription: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return description\n",
    "description = extract_description(full_description)\n",
    "print(f'Job description at index {emp}:\\n{description[emp]}\\n')\n",
    "\n",
    "# Extract title\n",
    "def extract_title(full_description):\n",
    "    title = [re.search(r'Title: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return title\n",
    "title = extract_title(full_description)\n",
    "print(f'Job title at index {emp}:\\n{title[emp]}\\n')\n",
    "\n",
    "# Extract webindex\n",
    "def extract_webindex(full_description):\n",
    "    webindex = [re.search(r'Webindex: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return webindex\n",
    "webindex = extract_webindex(full_description)\n",
    "print(f'Job webindex at index {emp}:\\n{webindex[emp]}\\n')\n",
    "\n",
    "# Extract company\n",
    "def extract_company(company):\n",
    "    company = [re.search(r'Company: (.*)', str(i)).group(1) if re.search(r'Company: (.*)', str(i)) else \"NA\" for i in company]\n",
    "    return company\n",
    "company = extract_company(full_description)\n",
    "print(f'Job company at index {emp}:\\n{company[emp]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 + 1.2.3 Tokenize each job advertisement description lowing regular expression & lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw description:\n",
      " Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home’s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home’s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home’s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home’s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user’s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user’s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home’s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home’s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home’s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user’s stay as comfortable and enjoyable as possible. \n",
      "\n",
      "Tokenized description:\n",
      " ['job', 'title', 'unit', 'manager', 'reporting', 'to', 'registered', 'manager', 'job', 'purpose', 'to', 'manage', 'in', 'a', 'professional', 'manner', 'the', 'day', 'to', 'day', 'running', 'of', 'the', 'home', 's', 'administration', 'clinical', 'policies', 'and', 'procedures', 'training', 'and', 'care', 'planning', 'to', 'implement', 'working', 'practices', 'that', 'monitors', 'the', 'health', 'and', 'welfare', 'of', 'the', 'home', 's', 'service', 'users', 'and', 'staff', 'and', 'their', 'respective', 'environments', 'to', 'promote', 'quality', 'care', 'within', 'a', 'warm', 'friendly', 'ambience', 'key', 'result', 'areas', 'managing', 'to', 'work', 'with', 'the', 'directors', 'to', 'achieve', 'the', 'home', 's', 'financial', 'targets', 'to', 'manage', 'the', 'home', 'in', 'a', 'manner', 'which', 'will', 'not', 'bring', 'the', 'home', 'or', 'service', 'users', 'into', 'disrepute', 'to', 'maintain', 'confidentiality', 'on', 'all', 'aspects', 'of', 'care', 'and', 'staff', 'management', 'to', 'ensure', 'all', 'the', 'home', 's', 'policies', 'and', 'procedures', 'are', 'implemented', 'and', 'followed', 'by', 'all', 'staff', 'to', 'inform', 'the', 'registered', 'manager', 'immediately', 'if', 'a', 'serious', 'difficulty', 'or', 'event', 'occurs', 'managing', 'support', 'to', 'delegate', 'responsibility', 'effectively', 'and', 'within', 'legal', 'boundaries', 'to', 'ensure', 'through', 'clinical', 'standards', 'and', 'audits', 'that', 'good', 'practice', 'is', 'maintained', 'throughout', 'the', 'home', 'nursing', 'duties', 'to', 'perform', 'nursing', 'duties', 'that', 'reflect', 'current', 'best', 'practice', 'and', 'keep', 'abreast', 'of', 'latest', 'developments', 'to', 'inform', 'the', 'registered', 'manager', 'of', 'any', 'nursing', 'and', 'medical', 'matters', 'affecting', 'the', 'service', 'user', 's', 'care', 'to', 'assist', 'service', 'users', 'with', 'personal', 'care', 'which', 'includes', 'using', 'the', 'bath', 'bed', 'bath', 'or', 'shower', 'feet', 'and', 'nail', 'care', 'hair', 'care', 'shaving', 'mouth', 'care', 'denture', 'care', 'toileting', 'needs', 'all', 'daily', 'activities', 'as', 'per', 'individual', 'care', 'plans', 'including', 'tidying', 'bed', 'and', 'room', 'to', 'assist', 'clients', 'with', 'their', 'psychological', 'needs', 'which', 'includes', 'talking', 'listening', 'excursions', 'liaising', 'with', 'family', 'assisting', 'with', 'hobbies', 'and', 'recreation', 'activities', 'to', 'care', 'for', 'the', 'service', 'user', 'as', 'an', 'individual', 'and', 'maintain', 'a', 'high', 'level', 'of', 'care', 'for', 'their', 'particular', 'condition', 'ensuring', 'physical', 'needs', 'comfort', 'and', 'dignity', 'are', 'met', 'to', 'care', 'for', 'service', 'user', 's', 'at', 'their', 'end', 'of', 'life', 'in', 'a', 'respectful', 'and', 'dignified', 'manner', 'to', 'administer', 'medication', 'strictly', 'in', 'accordance', 'with', 'the', 'drug', 'administration', 'policy', 'and', 'nmc', 'guidelines', 'to', 'assist', 'the', 'registered', 'manager', 'in', 'ensuring', 'adequate', 'supplies', 'of', 'medication', 'and', 'correct', 'storage', 'as', 'per', 'the', 'medication', 'policy', 'to', 'assist', 'with', 'serving', 'meals', 'and', 'other', 'domestic', 'duties', 'such', 'as', 'laundry', 'occasional', 'cleaning', 'etc', 'to', 'deal', 'with', 'any', 'internal', 'or', 'external', 'communications', 'from', 'service', 'users', 'or', 'third', 'parties', 'to', 'ensure', 'all', 'documentation', 'is', 'kept', 'up', 'to', 'date', 'and', 'recorded', 'accurately', 'to', 'maintain', 'records', 'care', 'plans', 'as', 'required', 'by', 'the', 'care', 'quality', 'commission', 'business', 'management', 'to', 'work', 'both', 'with', 'the', 'directors', 'and', 'independently', 'to', 'generate', 'enquiries', 'and', 'proactively', 'deal', 'with', 'members', 'of', 'the', 'public', 'and', 'care', 'medical', 'professionals', 'to', 'ensure', 'a', 'high', 'level', 'of', 'occupancy', 'rates', 'to', 'manage', 'and', 'have', 'a', 'working', 'knowledge', 'of', 'the', 'home', 's', 'policies', 'and', 'procedures', 'and', 'ensure', 'they', 'are', 'maintained', 'at', 'all', 'times', 'to', 'promote', 'and', 'act', 'strictly', 'in', 'accordance', 'with', 'the', 'home', 's', 'health', 'and', 'safety', 'policy', 'and', 'ensure', 'good', 'health', 'and', 'safety', 'practice', 'within', 'the', 'working', 'environment', 'to', 'comply', 'with', 'coshh', 'riddor', 'environmental', 'health', 'fire', 'regulations', 'and', 'the', 'care', 'standards', 'act', 'to', 'bring', 'to', 'the', 'directors', 'immediate', 'attention', 'any', 'item', 'of', 'health', 'and', 'safety', 'importance', 'that', 'would', 'be', 'of', 'concern', 'and', 'potentially', 'needing', 'prompt', 'action', 'to', 'have', 'an', 'in', 'depth', 'operational', 'knowledge', 'of', 'all', 'emergency', 'procedures', 'and', 'ensure', 'they', 'are', 'cascaded', 'down', 'through', 'all', 'the', 'staff', 'and', 'service', 'users', 'to', 'attend', 'staff', 'meetings', 'as', 'arranged', 'and', 'to', 'ensure', 'other', 'staff', 'informed', 'of', 'items', 'discussed', 'and', 'decisions', 'made', 'to', 'attend', 'at', 'least', 'hours', 'per', 'annum', 'of', 'compulsory', 'training', 'outlined', 'by', 'your', 'personal', 'development', 'plan', 'to', 'participate', 'in', 'regular', 'reviews', 'and', 'an', 'annual', 'appraisal', 'in', 'order', 'to', 'develop', 'your', 'training', 'needs', 'in', 'relation', 'to', 'skills', 'and', 'knowledge', 'to', 'promote', 'the', 'good', 'name', 'of', 'the', 'home', 'at', 'all', 'times', 'to', 'cover', 'colleagues', 'duties', 'in', 'times', 'of', 'sickness', 'and', 'holidays', 'to', 'develop', 'and', 'share', 'ideas', 'for', 'the', 'improvement', 'of', 'practice', 'within', 'the', 'home', 'to', 'implement', 'change', 'sensitively', 'but', 'effectively', 'to', 'replenish', 'stocks', 'and', 'exercise', 'cost', 'stock', 'control', 'to', 'operate', 'within', 'the', 'guidelines', 'of', 'the', 'data', 'protection', 'act', 'and', 'ensure', 'total', 'confidentiality', 'especially', 'with', 'regards', 'to', 'staff', 'and', 'service', 'users', 'managing', 'people', 'to', 'assist', 'and', 'supervise', 'staff', 'training', 'to', 'be', 'responsible', 'for', 'covering', 'staff', 'absences', 'and', 'managing', 'disputes', 'where', 'appropriate', 'to', 'ensure', 'all', 'staff', 'contribute', 'to', 'the', 'best', 'of', 'their', 'ability', 'to', 'the', 'efficient', 'running', 'of', 'the', 'home', 'and', 'that', 'high', 'standards', 'are', 'maintained', 'to', 'ensure', 'staff', 'have', 'the', 'qualifications', 'and', 'training', 'necessary', 'for', 'the', 'duties', 'they', 'perform', 'to', 'ensure', 'all', 'staff', 'have', 'two', 'monthly', 'reviews', 'and', 'an', 'annual', 'appraisal', 'as', 'per', 'company', 'policy', 'further', 'duties', 'in', 'addition', 'to', 'the', 'responsibilities', 'listed', 'above', 'it', 'may', 'be', 'necessary', 'to', 'perform', 'other', 'duties', 'as', 'required', 'by', 'the', 'home', 's', 'management', 'or', 'senior', 'staff', 'consideration', 'will', 'be', 'given', 'to', 'your', 'skills', 'and', 'status', 'when', 'given', 'these', 'duties', 'note', 'it', 'is', 'expected', 'that', 'all', 'duties', 'carried', 'out', 'will', 'be', 'performed', 'in', 'a', 'spirit', 'of', 'cooperation', 'required', 'from', 'a', 'dedicated', 'efficient', 'team', 'whose', 'prime', 'aim', 'is', 'to', 'make', 'the', 'service', 'user', 's', 'stay', 'as', 'comfortable', 'and', 'enjoyable', 'as', 'possible'] \n",
      "\n",
      "\n",
      "The original number of Tokenized description tokens:  776\n"
     ]
    }
   ],
   "source": [
    "def tokenizeDescription(raw_description):\n",
    "    \"\"\"\n",
    "        This function first convert all words to lowercases,\n",
    "        it then segment the raw description into sentences and tokenize each sentences\n",
    "        and convert the description to a list of tokens.\n",
    "    \"\"\"\n",
    "    description = raw_description.lower() # convert all words to lowercase\n",
    "\n",
    "    # segment into sentences\n",
    "    sentences = sent_tokenize(description)\n",
    "\n",
    "    # tokenize each sentence\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "\n",
    "    # merge them into a list of tokens\n",
    "    tokenised_description = list(chain.from_iterable(token_lists))\n",
    "    return tokenised_description\n",
    "\n",
    "tk_description = [tokenizeDescription(r) for r in description]  # list comprehension, generate a list of tokenized articles\n",
    "\n",
    "print(\"Raw description:\\n\",description[emp],'\\n')\n",
    "print(\"Tokenized description:\\n\",tk_description[emp],'\\n\\n')\n",
    "print(\"The original number of Tokenized description tokens: \",len(tk_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### A Few Statistics Before Any Further Pre-processing\n",
    "\n",
    "In the following, we are interested to know a few statistics at this very begining stage, including:\n",
    "* The total number of tokens across the corpus\n",
    "* The total number of types across the corpus, i.e. the size of vocabulary \n",
    "* The so-called, [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity), referring to the ratio of different unique word stems (types) to the total number of words (tokens).  \n",
    "* The average, minimum and maximum number of token (i.e. document length) in the dataset.\n",
    "\n",
    "In the following, we wrap all these up as a function, since we will use this printing module later to compare these statistic values before and after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9834\n",
      "Total number of tokens:  186952\n",
      "Lexical diversity:  0.052601737344345076\n",
      "Total number of description: 776\n",
      "Average description length: 240.91752577319588\n",
      "Maximum description length: 815\n",
      "Minimum description length: 13\n",
      "Standard deviation of description length: 124.97750685071483\n"
     ]
    }
   ],
   "source": [
    "def stats_print(tk_description):\n",
    "    words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of description:\", len(tk_description))\n",
    "    lens = [len(article) for article in tk_description]\n",
    "    print(\"Average description length:\", np.mean(lens))\n",
    "    print(\"Maximum description length:\", np.max(lens))\n",
    "    print(\"Minimum description length:\", np.min(lens))\n",
    "    print(\"Standard deviation of description length:\", np.std(lens))\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.4 Remove words with length less than 2.\n",
    "+ We have 2 types of most frequent words is that in terms either term frequency or document frequency\n",
    "+ In this sub-task, you are required to remove any token that only contains a single character (a token that of length less than 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing, the number of words that appear with length less than 2: 4233\n",
      "After removing, the number of words that appear less than 2: 4231\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"Before removing, the number of words that appear with length less than 2:\", len([w for w in word_counts if word_counts[w] < 2]))\n",
    "\n",
    "# filter out single character tokens\n",
    "tk_description = [[w for w in description if len(w) >=2] \\\n",
    "                      for description in tk_description]\n",
    "\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"After removing, the number of words that appear less than 2:\", len([w for w in word_counts if word_counts[w] < 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Remove the top 50 most frequent words based on document frequency.\n",
    "+ We have 2 types of most frequent words is that in terms either term frequency or document frequency\n",
    "\n",
    "Explore the most frequent words in the pre-processed tokenized review text corpus:\n",
    "* explore the most frequent words (top 50) based on term frequency and document frequency, respectively\n",
    "* compare the results using different frequency measurements, which words are extracted based on both frequency measurements?\n",
    "* think and decide on whether or not you would remove some of the most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 most frequent words:\n",
      " [('and', 8309), ('the', 6487), ('to', 6265), ('of', 4630), ('in', 3290), ('for', 2832), ('with', 2306), ('will', 2021), ('you', 2011), ('be', 1869), ('is', 1793), ('as', 1425), ('this', 1393), ('an', 1361), ('are', 1332), ('experience', 1276), ('on', 1216), ('have', 1114), ('or', 1088), ('sales', 1030), ('role', 946), ('work', 861), ('business', 832), ('your', 816), ('team', 789), ('we', 724), ('working', 719), ('our', 714), ('job', 688), ('care', 675), ('within', 672), ('skills', 669), ('all', 663), ('that', 625), ('company', 614), ('client', 594), ('their', 587), ('management', 572), ('manager', 519), ('from', 514), ('please', 504), ('support', 501), ('uk', 496), ('at', 491), ('service', 481), ('looking', 474), ('excellent', 455), ('development', 431), ('new', 429), ('must', 413)] \n",
      "\n",
      "\n",
      "Top 50 most frequent words after removing:\n",
      " [('and', 8309), ('the', 6487), ('to', 6265), ('of', 4630), ('in', 3290), ('for', 2832), ('with', 2306), ('will', 2021), ('you', 2011), ('be', 1869), ('is', 1793), ('as', 1425), ('this', 1393), ('an', 1361), ('are', 1332), ('experience', 1276), ('on', 1216), ('have', 1114), ('or', 1088), ('sales', 1030), ('role', 946), ('work', 861), ('business', 832), ('your', 816), ('team', 789), ('we', 724), ('working', 719), ('our', 714), ('job', 688), ('care', 675), ('within', 672), ('skills', 669), ('all', 663), ('that', 625), ('company', 614), ('client', 594), ('their', 587), ('management', 572), ('manager', 519), ('from', 514), ('please', 504), ('support', 501), ('uk', 496), ('at', 491), ('service', 481), ('looking', 474), ('excellent', 455), ('development', 431), ('new', 429), ('must', 413)]\n"
     ]
    }
   ],
   "source": [
    "# Remove the top 50 most frequent words\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "top50 = word_counts.most_common(50) # get the top 50 most frequent words\n",
    "print(\"Top 50 most frequent words:\\n\",top50, \"\\n\\n\")\n",
    "\n",
    "tk_description = [[w for w in tk_description if w not in top50] for description in tk_description]\n",
    "top50 = word_counts.most_common(50) # get the top 50 most frequent words\n",
    "print(\"Top 50 most frequent words after removing:\\n\",top50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.5 Remove stopwords using the provided stop words list (i.e, stopwords_en.txt)\n",
    "> **Be CAREFUL**: as mentioned before, the purpose of this task is to pre-process the text reviews and later on we are going to use the pre-process text to build a sentiment analysis model. The stop word removal process requires careful consideration in this type of task.\n",
    "\n",
    "Remove the stop words from the tokenized text inside `stopwords_en.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stop words inside stopwords_en.txt is 571 including:\n",
      "\n",
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# remove the stop words inside `stopwords_en.txt` from the tokenized text\n",
    "stopwords_file = 'stopwords_en.txt'\n",
    "\n",
    "# read the stop words into a list\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    stop_words = f.read().splitlines() \n",
    "print(f'The number of stop words inside {stopwords_file} is {len(stop_words)} including:\\n\\n{stop_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------> OBSERVATION:\n",
    "+ There 571 stopwords in total, which are often function words in English, like articles (e.g. \"the\", and \"an\"), pronouns (e.g. \"he\", \"him\", and \"they\"), particles (e.g., \"well\", \"however\" and \"thus\"), etc, and universal words in all job advertisement (e.g.'ask', 'asking', 'used', and 'useful')\n",
    "\n",
    "+ Note this this is just one of the lists and we emphasize that there is no universal list of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stopword removal: 776  tokens\n",
      "After stopword removal: 776  tokens\n"
     ]
    }
   ],
   "source": [
    "# flitering stop words\n",
    "print(\"Before stopword removal:\",len(tk_description),\" tokens\")\n",
    "tk_description = [token for token in tk_description if token not in stop_words]\n",
    "print(\"After stopword removal:\",len(tk_description),\" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [token for token in tk_description if token in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ain't\",\n",
       " 'another',\n",
       " \"aren't\",\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " \"couldn't\",\n",
       " \"didn't\",\n",
       " \"doesn't\",\n",
       " \"don't\",\n",
       " 'enough',\n",
       " \"hadn't\",\n",
       " \"hasn't\",\n",
       " \"haven't\",\n",
       " 'ignored',\n",
       " \"isn't\",\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " \"shouldn't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"won't\",\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in stop_words if (\"not\" in w or \"n't\" in w or \"no\" in w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2.6 Remove the word that appears only once in the document collection, based on term frequency\n",
    "\n",
    "move on to the less frequent words:\n",
    "* find out the list of words that appear only once in the **entire corpus**\n",
    "* remove these less frequent words from each tokenized description text\n",
    "\n",
    "We first need to find out the set of less frequent words by using the `hapaxes` function applied on the **term frequency** dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chain\n\u001b[1;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(tk_description)) \u001b[38;5;66;03m# we put all the tokens in the corpus in a single list\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m term_fd \u001b[38;5;241m=\u001b[39m \u001b[43mFreqDist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/probability.py:102\u001b[0m, in \u001b[0;36mFreqDist.__init__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Construct a new frequency distribution.  If ``samples`` is\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    given, then the frequency distribution will be initialized\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    :type samples: Sequence\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mCounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Cached number of samples in this FreqDist\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/collections/__init__.py:593\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m \n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/probability.py:140\u001b[0m, in \u001b[0;36mFreqDist.update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mOverride ``Counter.update()`` to invalidate the cached N\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/collections/__init__.py:679\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(iterable)\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from nltk.probability import *\n",
    "from itertools import chain\n",
    "\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "term_fd = FreqDist(words) # compute term frequency for each unique word/type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lessFreqWords = set(term_fd.hapaxes())\n",
    "print(len(lessFreqWords))\n",
    "lessFreqWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLessFreqWords(description):\n",
    "    return [w for w in description if w not in lessFreqWords]\n",
    "\n",
    "tk_reviews = [removeLessFreqWords(description) for description in tk_description]\n",
    "\n",
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2.7 Remove the top 50 most frequent words based on document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable([set(description) for description in tk_description]))\n",
    "doc_fd = FreqDist(words)  # compute document frequency for each unique word/type\n",
    "top50MostFreqWords = doc_fd.most_common(50)\n",
    "top50MostFreqWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMostFreqWords(description):\n",
    "    return [w for w in description if w not in top50MostFreqWords]\n",
    "\n",
    "tk_reviews = [removeMostFreqWords(description) for description in tk_description]\n",
    "\n",
    "stats_print(tk_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The Updated Statistics\n",
    "\n",
    "In the above, we have done a few pre-processed steps, now let's have a look at the statistics again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# specify\n",
    "ignored_words = [w for w in stop_words if not (\"not\" in w or \"n't\" in w or \"no\" in w)]\n",
    "\n",
    "# filter out stop words\n",
    "tk_description = [[w for w in description if w not in ignored_words] \\\n",
    "                      for description in tk_description]\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall, from the beginning, we have the following:  \n",
    "_____________________________________________\n",
    "\n",
    "Vocabulary size:  9834\n",
    "\n",
    "Total number of tokens:  186952\n",
    "\n",
    "Lexical diversity:  0.052601737344345076\n",
    "\n",
    "Total number of description: 776\n",
    "\n",
    "Average description length: 240.91752577319588\n",
    "\n",
    "Maximun description length: 815\n",
    "\n",
    "Minimun description length: 13\n",
    "\n",
    "Standard deviation of description length: 124.97750685071483\n",
    "_____________________________________________\n",
    "\n",
    "We've shrunk more than 40% of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.8 Save all job advertisement text and information in txt files (we will retrieve them. in task 2 and 3)\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "\n",
    "We are going to store all the preprocessed description texts and its corresponding labels into files for task 2.\n",
    "* all the tokenized description are stored in a .txt file named `description.txt`\n",
    "    * each line is a description text, which contained all the tokens of the description text, separated by a space ' '\n",
    "* all the corresponding labels are store in a .txt file named `category.txt`\n",
    "    * each line is a label (one of these 4 values: 0,1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save description text\n",
    "def save_description(descriptionFilename,tk_description):\n",
    "    out_file = open(descriptionFilename, 'w') # creates a txt file and open to save the descriptions\n",
    "    string = \"\\n\".join([\" \".join(description) for description in tk_description])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the category corresponding with the description text\n",
    "def save_category(categoryFilename,category):\n",
    "    out_file = open(categoryFilename, 'w') # creates a txt file and open to save category\n",
    "    string = \"\\n\".join([str(s) for s in category])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the title corresponding with the description text\n",
    "def save_title(titleFilename,title):\n",
    "    out_file = open(titleFilename, 'w') # creates a txt file and open to save title\n",
    "    string = \"\\n\".join([str(s) for s in title])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save description into txt file\n",
    "descriptionFilename = \"description.txt\"\n",
    "save_description(descriptionFilename,tk_description)\n",
    "print(f'Successfully saved description into {descriptionFilename}')\n",
    "\n",
    "# save category into txt file\n",
    "categoryFilename = \"category.txt\"\n",
    "save_category(categoryFilename,category)\n",
    "print(f'Successfully saved category into {categoryFilename}')\n",
    "\n",
    "# save title into txt file\n",
    "titleFilename = \"title.txt\"\n",
    "save_title(titleFilename,title)\n",
    "print(f'Successfully saved title into {titleFilename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.9 Build a vocabulary of the cleaned job advertisement descriptions\n",
    "\n",
    "`vocab.txt`\n",
    "This file contains the unigram vocabulary, one each line, in the following format: word_string:word_integer_index. Very importantly, words in the vocabulary must be sorted in alphabetical order, and the index value starts from 0. This file is the key to interpret the sparse encoding. For instance, in the following example, the word aaron is the 20th word (the corresponding integer_index as 19) in the vocabulary (note that the index values and words in the following image are artificial and used to demonstrate the required format only, it doesn't reflect the values of the actual expected output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save all job advertisement text and information in txt file\n",
    "with open('job_ad.txt', 'w') as f:\n",
    "    f.write(\"Category: \" + str(category) + \"\\n\")\n",
    "    for i in range(len(tk_description)):\n",
    "        f.write(full_description[i] + \"\\n\")\n",
    "        f.write(\"Tokenized Description: \" + str(tk_description[i]) + \"\\n\")\n",
    "        f.write(\"Category: \" + str(df['target'][i]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    print(\"Successfully write job advertisement with the tokenized description in txt file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocab(vocab, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            f.write(word + ':' + str(i) + '\\n')\n",
    "# convert tokenized description into a alphabetically sorted list\n",
    "vocab = sorted(list(set(chain.from_iterable(tk_description))))\n",
    "write_vocab(vocab, 'vocab.txt')\n",
    "# print out the first 10 words in the vocabulary\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Category at index 0: {df[\"target_names\"][0]}')\n",
    "print(f'Category at index 1: {df[\"target_names\"][1]}')\n",
    "print(f'Category at index 2: {df[\"target_names\"][2]}')\n",
    "print(f'Category at index 3: {df[\"target_names\"][3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert job ad to a dataframe\n",
    "job_ad = pd.DataFrame({'Title': title, 'Webindex': webindex, 'Company': company, 'Description': description,'Tokenized Description': tk_description, 'Category': category})\n",
    "\n",
    "# change Tokenized Description to string separated by space\n",
    "job_ad['Tokenized Description'] = job_ad['Tokenized Description'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "\n",
    "\n",
    "# replace the value in Category column\n",
    "# Category at index 0: Accounting_Finance\n",
    "# Category at index 1: Engineering\n",
    "# Category at index 2: Healthcare_Nursing\n",
    "# Category at index 3: Sales\n",
    "job_ad['Category'] = job_ad['Category'].replace([0,1,2,3],['Accounting_Finance','Engineering','Healthcare_Nursing','Sales'])\n",
    "\n",
    "# Cast Webindex to int\n",
    "job_ad['Webindex'] = job_ad['Webindex'].astype(int)\n",
    "\n",
    "# save job ad to csv file\n",
    "job_ad.to_csv('job_ad.csv', index=False)\n",
    "\n",
    "print(job_ad.info())\n",
    "# print first 3 rows\n",
    "job_ad.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook task1.ipynb to python\n",
      "[NbConvertApp] Writing 29547 bytes to task1.py\n",
      "[NbConvertApp] Converting notebook task2_3.ipynb to python\n",
      "[NbConvertApp] Writing 18131 bytes to task2_3.py\n"
     ]
    }
   ],
   "source": [
    "# The .py format of the jupyter notebook\n",
    "for fname in os.listdir():\n",
    "    if fname.endswith('ipynb'):\n",
    "        os.system(f'jupyter nbconvert {fname} --to python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.3 Summary</strong></h3>\n",
    "\n",
    "In this activity, we have demonstrated the basic text pre-processing steps of sentence segmentation and tokenization. \n",
    "There are a couple of things that we should keep in mind:\n",
    "\n",
    "* we have covered the fundamentals of text pre-processing steps of Case Normalization, Stop Word Removing, Stemming and Lemmatization. \n",
    "\n",
    "* As mentioned before, though these steps are doing very different things to the text we have, however, one common effect among them, is the reduction on the size of the vocabulary (the list of distinct words contained in the corpus). \n",
    "\n",
    "* How we should process the text depends on the downstream analysis. Before we do any pre-processing, we should decide on the scope of the text to be used in the downstream analysis task. For instance, should we use an entire document? Or should we break the document down into sections, paragraphs, or sentences. Take another example. If we are analysing emails, should we keep the headers information? or should we focus on the email body? Choosing the proper scope depends on the goals of the analysis task. For example, you might choose to use an entire document in document classification and clustering tasks while one might choose smaller units like paragraphs or sentences in document summarization and information retrieval tasks. The scope chosen will have an  impact on the steps needed in the pre-processing process.\n",
    "\n",
    "* In this activity, we have shown you multiple ways to do tokenization. However, there is no single right way to do tokenization.  It completely depends on the corpus and the text analysis task you are going to perform. The major question of the tokenization phase is what counts as a token. In some of the text analysis task. Although word tokenization is relatively easy compared with other NLP or text mining task, errors made in this phase will propagate into later analysis and cause problems.\n",
    "\n",
    "* Case Normalization is a very simple process to do, though it is indeed very effective. \n",
    "The above is a very simple example (consisting of a very short paragraph) and it might not show much reduction on the vocabulary size. Imagine if you have a large corpus, doing case normalization will significantly reduce the vocabulary size, and thus helps the analysis algorithms to focus on different meaning of tokens rather than its cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of distinct words BEFORE further pre-processing:\",len(set(description)))\n",
    "print(\"Number of distinct words AFTER further pre-processing:\",len(set(tk_description)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Discussion\n",
    ">>In some of the text analysis tasks, we have to be mindful in the process of stopword removal. \n",
    "In some scenarios, stop words removal can wipe out relevant information and modify the context in a given sentence. \n",
    "For example, if we are performing a sentiment analysis, the word 'not', although is a stop word, it carries critical information, i.e. 'like' and 'not like' obviously are carrying completely reversed meaning.\n",
    "We might fool out our algorithm off track if we remove a stop word like “not”. \n",
    "You should always carefully consider these conditions, design the list of \"stop words\" that are to removed based on your specific objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.4 References</strong></h3>\n",
    "\n",
    "\n",
    "+ [1] Sentence boundary disambiguation. https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation\n",
    "+ [2] Tokenization. https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html  \n",
    "+ [3] Your Guide to Natural Language Processing (NLP). https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1  \n",
    "+ [4] Introduction to Natural Language Processing for Text. https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63  \n",
    "+ [5] [Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book/ch02.html): Chapter 2 of \"Natural Language Processing with Python\" By Steven Bird, Ewan Kelin & Edward Loper.  \n",
    "+ [6]. [Corpus Readers](http://www.nltk.org/howto/corpus.html#tagged-corpora): An NLTK tutorial on accessing the contents of a diverse set of corpora.\n",
    "\n",
    "+ [1] Stop words. https://en.wikipedia.org/wiki/Stop_word  \n",
    "+ [3] Bird, Steven, Edward Loper and Ewan Klein (2009), [Natural Language Processing with Python](http://www.nltk.org/book/). O’Reilly Media Inc.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
