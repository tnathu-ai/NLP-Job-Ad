{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "\n",
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Task 1. Basic Text Pre-processing</strong></h3>\n",
    "\n",
    "#### Student Name: Tran Ngoc Anh Thu\n",
    "#### Student ID: s3879312\n",
    "\n",
    "Date: \"October 2, 2022\"\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* sklearn\n",
    "* collections\n",
    "* re\n",
    "* numpy\n",
    "* nltk\n",
    "* itertools\n",
    "* pandas\n",
    "* os\n",
    "\n",
    "## Steps\n",
    "1. Load data\n",
    "2. Text Pre-processing\n",
    "    * Sentence Segmentation\n",
    "    * Word Tokenization\n",
    "    * Removing Single Character Tokens\n",
    "    * Removing Stop words\n",
    "3. Saving the Pre-processing Reviews\n",
    "\n",
    "## Introduction\n",
    "Nowadays there are many job hunting websites including seek.com.au and au.indeed.com. These job hunting sites all manage a job search system, where job hunters could search for relevant jobs based on keywords, salary, and categories. In previous years, the category of an advertised job was often manually entered by the advertiser (e.g., the employer). There were mistakes made for category assignment. As a result, the jobs in the wrong class did not get enough exposure to relevant candidate groups.\n",
    "With advances in text analysis, automated job classification has become feasible; and sensible suggestions for job categories can then be made to potential advertisers. This can help reduce human data entry error, increase the job exposure to relevant candidates, and also improve the user experience of the job hunting site. In order to do so, we need an automated job ads classification system that helps to predict the categories of newly entered job advertisements.\n",
    "\n",
    "In this **task1** notebook, we are going to explore a job advertisement data set, and focus on pre-processing the description only.\n",
    "In the next task **task2_3**, we will then use the pre-processed text reviews to generate data features and build classification models to predict label of the description.\n",
    "\n",
    "## Dataset\n",
    "+ A small collection of job advertisement documents (around 776 jobs) inside the `data` folder.\n",
    "+ Inside the data folder, there are four different sub-folders: Accounting_Finance, Engineering, Healthcare_Nursing, and Sales, representing a job category.\n",
    "+ The job advertisement text documents of a particular category are in the corresponding sub-folder.\n",
    "+ Each job advertisement document is a txt file named `Job_<ID>.txt`. It contains the title, the webindex (some will also have information on the company name, some might not), and the full description of the job advertisement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "from collections import Counter\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.21.5\n",
      "Pandas version:  1.4.2\n",
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# check the version of the main packages\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Pandas version: \",pd.__version__)\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.1 Examining and loading data</strong></h3>\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n",
    "\n",
    "\n",
    "\n",
    "Before doing any pre-processing, we need to load the data into a proper format. \n",
    "To load the data, you have to explore the data folder. Inside the `data` folder:\n",
    "+ Inside the data folder you will see 4 different subfolders, namely: `Accounting_Finance`, `Engineering`,`Healthcare_Nursing`, and `Sales`, each folder name is a job category.\n",
    "+ The job advertisement text documents of a particular category are located in the corresponding subfolder.\n",
    "+ Each job advertisement document is a txt file, named as \"Job_<ID>.txt\". It contains the title, the webindex,(some will also have information on the company name, some might not), and the full description of the job advertisement. \n",
    "\n",
    "In this case, providing that the dataset is given in a very well organised way, I would use a super handy API [`load_files`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) from `sklearn.datasets`. \n",
    "    \n",
    "**import the function by:**\n",
    "```python\n",
    "from sklearn.datasets import load_files  \n",
    "```\n",
    "\n",
    "Then you can use the function to directly load the data and labels, for example:\n",
    "```python\n",
    "df = load_files(r\"data\")  \n",
    "```\n",
    "\n",
    "The loaded `movie_data` is then a dictionary, with the following attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **DESCRIPTION**                                           |\n",
    "|--------------|---------------------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website            |\n",
    "| Title        | Title of the advertised job position                          |\n",
    "| Company      | Company (employer) of the advertised job position             |\n",
    "| Description  | the description of each job advertisement                     |\n",
    "\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load each folder and file inside the data folder\n",
    "df = load_files(r\"data\")\n",
    "# type of the loaded file\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 1, 3, 2, 3, 1, 4, 4, 1, 1, 2, 4, 2, 4, 4, 2, 4, 3, 3, 3,\n",
       "       4, 4, 1, 3, 3, 3, 1, 3, 4, 2, 3, 1, 2, 4, 4, 2, 2, 1, 3, 3, 3, 3,\n",
       "       1, 1, 3, 2, 4, 2, 2, 3, 3, 4, 1, 1, 2, 1, 3, 3, 4, 4, 4, 1, 4, 1,\n",
       "       2, 3, 4, 2, 4, 3, 4, 2, 4, 3, 2, 4, 3, 2, 4, 3, 3, 2, 1, 2, 2, 2,\n",
       "       4, 1, 4, 2, 4, 3, 3, 1, 3, 4, 3, 2, 1, 2, 2, 3, 1, 4, 1, 2, 4, 3,\n",
       "       2, 3, 1, 4, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4,\n",
       "       3, 1, 1, 2, 4, 3, 1, 2, 1, 4, 2, 3, 2, 1, 1, 1, 4, 1, 2, 3, 4, 2,\n",
       "       2, 2, 3, 2, 1, 2, 1, 2, 1, 2, 2, 3, 1, 3, 3, 1, 3, 4, 3, 3, 1, 3,\n",
       "       2, 1, 2, 2, 2, 4, 2, 4, 2, 1, 4, 2, 1, 3, 1, 1, 3, 2, 2, 1, 2, 4,\n",
       "       1, 2, 2, 4, 1, 2, 1, 3, 4, 1, 3, 1, 2, 1, 2, 4, 2, 1, 2, 2, 1, 2,\n",
       "       1, 2, 3, 2, 4, 2, 3, 4, 2, 2, 3, 1, 1, 2, 3, 1, 4, 3, 4, 3, 3, 4,\n",
       "       1, 2, 2, 2, 2, 2, 2, 1, 4, 2, 2, 1, 1, 3, 2, 3, 3, 3, 3, 2, 4, 2,\n",
       "       3, 2, 3, 4, 3, 4, 1, 2, 4, 1, 3, 2, 1, 3, 2, 3, 1, 3, 2, 2, 2, 3,\n",
       "       3, 2, 3, 1, 3, 3, 2, 3, 1, 2, 1, 1, 4, 3, 2, 4, 1, 4, 2, 3, 1, 2,\n",
       "       1, 1, 1, 1, 3, 3, 1, 1, 3, 2, 3, 1, 1, 2, 3, 4, 2, 4, 3, 1, 3, 3,\n",
       "       2, 4, 2, 1, 2, 3, 4, 2, 1, 1, 2, 2, 1, 3, 2, 4, 3, 4, 4, 2, 4, 2,\n",
       "       3, 2, 2, 3, 3, 4, 4, 3, 3, 2, 2, 3, 1, 3, 4, 1, 1, 2, 1, 2, 3, 2,\n",
       "       2, 4, 1, 3, 1, 2, 4, 2, 4, 3, 1, 3, 2, 4, 1, 1, 1, 4, 4, 1, 3, 4,\n",
       "       4, 2, 4, 1, 2, 1, 4, 2, 4, 2, 4, 3, 2, 2, 3, 4, 1, 1, 4, 4, 4, 2,\n",
       "       3, 3, 4, 1, 1, 1, 2, 1, 3, 2, 4, 3, 2, 3, 3, 2, 3, 3, 1, 1, 3, 4,\n",
       "       3, 2, 3, 4, 2, 3, 2, 1, 2, 3, 2, 3, 1, 4, 3, 4, 3, 4, 1, 1, 4, 3,\n",
       "       1, 3, 2, 4, 1, 4, 3, 2, 2, 3, 2, 3, 4, 2, 2, 2, 4, 2, 3, 2, 1, 1,\n",
       "       4, 1, 3, 3, 4, 4, 1, 4, 2, 2, 3, 3, 3, 4, 3, 2, 2, 1, 3, 4, 1, 1,\n",
       "       1, 1, 3, 2, 4, 1, 3, 2, 3, 3, 4, 2, 3, 1, 3, 1, 4, 3, 2, 3, 3, 4,\n",
       "       2, 2, 1, 3, 3, 2, 2, 4, 1, 2, 1, 1, 3, 4, 4, 1, 1, 2, 2, 3, 4, 4,\n",
       "       4, 1, 1, 3, 4, 2, 4, 2, 4, 4, 2, 1, 1, 1, 2, 1, 4, 2, 3, 3, 2, 2,\n",
       "       1, 2, 3, 3, 4, 4, 4, 2, 3, 4, 2, 1, 2, 4, 4, 4, 4, 2, 1, 1, 2, 2,\n",
       "       1, 2, 1, 1, 2, 3, 3, 2, 4, 3, 3, 3, 2, 3, 2, 1, 3, 2, 3, 3, 1, 3,\n",
       "       1, 1, 1, 1, 3, 3, 3, 1, 2, 2, 4, 2, 3, 2, 3, 1, 2, 2, 2, 2, 1, 3,\n",
       "       4, 2, 2, 3, 4, 3, 1, 2, 4, 4, 2, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       2, 3, 2, 1, 3, 1, 3, 1, 2, 1, 1, 4, 3, 2, 3, 2, 2, 2, 3, 3, 2, 2,\n",
       "       2, 3, 2, 1, 4, 4, 1, 3, 2, 3, 2, 2, 4, 1, 1, 1, 4, 1, 2, 2, 1, 4,\n",
       "       3, 3, 2, 1, 1, 3, 1, 2, 1, 2, 2, 4, 3, 1, 3, 2, 2, 2, 3, 3, 1, 4,\n",
       "       1, 1, 1, 3, 3, 4, 1, 2, 4, 3, 3, 4, 4, 2, 2, 3, 1, 4, 1, 1, 3, 3,\n",
       "       4, 1, 1, 3, 4, 4, 1, 1, 3, 3, 4, 2, 2, 3, 3, 1, 3, 4, 2, 4, 1, 2,\n",
       "       4, 4, 2, 4, 3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] # this corresponding to the index value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Accounting_Finance',\n",
       " 'Engineering',\n",
       " 'Healthcare_Nursing',\n",
       " 'Sales']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the categories\n",
    "df['target_names'] # this corresponding to the name value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category at index 0: .ipynb_checkpoints\n",
      "Category at index 1: Accounting_Finance\n",
      "Category at index 2: Engineering\n",
      "Category at index 3: Healthcare_Nursing\n"
     ]
    }
   ],
   "source": [
    "print(f'Category at index 0: {df[\"target_names\"][0]}')\n",
    "print(f'Category at index 1: {df[\"target_names\"][1]}')\n",
    "print(f'Category at index 2: {df[\"target_names\"][2]}')\n",
    "print(f'Category at index 3: {df[\"target_names\"][3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/Accounting_Finance/Job_00263.txt', 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether it matches, just in case\n",
    "emp = 10 # an example, note we will use this example throughout this exercise.\n",
    "df['filenames'][emp], df['target'][emp] # from the file path we know that it's the correct class too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# assign variables\n",
    "full_description, category = df.data, df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Title: Investments & Treasury Controller\\nWebindex: 71851935\\nCompany: August Clarke\\nDescription: Our client, based in Eastleigh, is looking for an Investments and Treasury Controller to join their team. Duties to include: Take responsibility for transactional management, analysis and oversight of the Company\\xe2\\x80\\x99s investment portfolio, including compliance with relevant sections of the relevant policies Ensure that working capital and other liquid resources and cashflow are managed efficiently Deliver consistently against relevant KPIs and KRIs, analysing any shortfalls and putting appropriate action plans in place to remediate process issues Manage day to day relationships with the Company\\xe2\\x80\\x99s outsourced Investment Managers and Custodians ensuring that there is mutual understanding of each others\\xe2\\x80\\x99 operations, systems and developments so that business is transacted efficiently and effectively Own endtoend investment processes, ensuring that processes, procedures, risks and controls are documented, effective and efficient. Regularly review and test processes and controls in accordance with Finance control and risk framework. Skills and Experience: The ability to build relationships with stakeholders at all levels both internal and external, able to challenge assumptions positively and bring people on the journey Strong written communication skills and must be fluent, articulate and confident in spoken communication Understanding of investment principles and significant experience of their practical application coupled with excellent analytical skills. A strong risks and control awareness is also required Strong organisational and planning skills coupled with ability to drive appropriate solutions. Proactive, highly motivated and change orientated, able to identify obstacles and clear them effectively IT literate, including experience of accounting systems and data warehouse architectures as well as desktop systems. Fluent with using Microsoft Excel, Word and PowerPoint Ability to understand, evaluate and interpret diverse financial data You will have experience of in a treasury/investment role, and must be ACA/ACCA/CIMA qualified or equivalent. August Clarke Limited is acting as a recruitment agency in relation to this role. We do not discriminate on the grounds of age, race, gender, disability, creed or sexual orientation and comply with all relevant UK legislation. If you do not hear back from us within **** working days please assume you have not been successful. This job was originally posted as www.totaljobs.com/JobSeeking/InvestmentsTreasuryController_job****'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 10th job advertisement description\n",
    "full_description[emp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ------> OBSERVATION:\n",
    "As we can see the current description is in the **binary** form and read as a byte object (a `b` in front of each review text if you print it out). Therefore, we need to decode into normal string for further pre-processing\n",
    "\n",
    "However, the tokenizer cannot apply a string pattern on a bytes-like object. To resolve this, we decode each read `full_description` text using `utf-8` by writing a decode function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(l):\n",
    "    if isinstance(l, list):\n",
    "        return [decode(x) for x in l]\n",
    "    else:\n",
    "        return l.decode('utf-8')\n",
    "\n",
    "# decode the binary description into utf-8 form and save it to full_description\n",
    "full_description = decode(full_description)\n",
    "full_description[emp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ---------------> OBSERVATION:\n",
    "The current `description` contains these attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **MEANING**                                        |\n",
    "|--------------|----------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website |\n",
    "| Title        | Title of the advertised job position               |\n",
    "| Company      | Company (employer) of the advertised job position  |\n",
    "| Description  | the description of each job advertisement          |\n",
    "\n",
    "and I only want the description itself to perform text-preprocessing and NLP on `description`. Therefore, I will perform the following pre-processing steps to the description of each job advertisement;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.2 Pre-processing data</strong></h3>\n",
    "\n",
    "1. Extract information from each job advertisement. Perform the following pre-processing steps to the description of each job advertisement;\n",
    "2. Tokenize each job advertisement description. The word tokenization must use the following regular expression, r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\";\n",
    "3. All the words must be converted into the lower case;\n",
    "4. Remove words with length less than 2.\n",
    "5. Remove stopwords using the provided stop words list (i.e, stopwords_en.txt). It is located inside the\n",
    "same downloaded folder.\n",
    "6. Remove the word that appears only once in the document collection, based on term frequency.\n",
    "7. Remove the top 50 most frequent words based on document frequency.\n",
    "8. Save all job advertisement text and information in txt file(s) (you have flexibility to choose what format\n",
    "you want to save the preprocessed job ads, and you will need to retrieve the pre-processed job ads\n",
    "text in Task 2 & 3);\n",
    "9. Build a vocabulary of the cleaned job advertisement descriptions, save it in a txt file (please refer to the\n",
    "required output);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract description, title, webindex,  from each job advertisement. \n",
    "\n",
    "def extract_description(full_description):\n",
    "    description = [re.search(r'\\nDescription: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return description\n",
    "description = extract_description(full_description)\n",
    "\n",
    "# Extract title\n",
    "def extract_title(full_description):\n",
    "    title = [re.search(r'Title: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return title\n",
    "title = extract_title(full_description)\n",
    "\n",
    "# Extract Webindex\n",
    "def extract_webindex(full_description):\n",
    "    webindex = [re.search(r'Webindex: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return webindex\n",
    "\n",
    "webindex = extract_webindex(full_description)\n",
    "\n",
    "# Extract company\n",
    "def extract_company(company):\n",
    "    company = [re.search(r'Company: (.*)', str(i)).group(1) if re.search(r'Company: (.*)', str(i)) else \"NA\" for i in company]\n",
    "    return company\n",
    "company = extract_company(full_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our client, based in Eastleigh, is looking for an Investments and Treasury Controller to join their team. Duties to include: Take responsibility for transactional management, analysis and oversight of the Company’s investment portfolio, including compliance with relevant sections of the relevant policies Ensure that working capital and other liquid resources and cashflow are managed efficiently Deliver consistently against relevant KPIs and KRIs, analysing any shortfalls and putting appropriate action plans in place to remediate process issues Manage day to day relationships with the Company’s outsourced Investment Managers and Custodians ensuring that there is mutual understanding of each others’ operations, systems and developments so that business is transacted efficiently and effectively Own endtoend investment processes, ensuring that processes, procedures, risks and controls are documented, effective and efficient. Regularly review and test processes and controls in accordance with Finance control and risk framework. Skills and Experience: The ability to build relationships with stakeholders at all levels both internal and external, able to challenge assumptions positively and bring people on the journey Strong written communication skills and must be fluent, articulate and confident in spoken communication Understanding of investment principles and significant experience of their practical application coupled with excellent analytical skills. A strong risks and control awareness is also required Strong organisational and planning skills coupled with ability to drive appropriate solutions. Proactive, highly motivated and change orientated, able to identify obstacles and clear them effectively IT literate, including experience of accounting systems and data warehouse architectures as well as desktop systems. Fluent with using Microsoft Excel, Word and PowerPoint Ability to understand, evaluate and interpret diverse financial data You will have experience of in a treasury/investment role, and must be ACA/ACCA/CIMA qualified or equivalent. August Clarke Limited is acting as a recruitment agency in relation to this role. We do not discriminate on the grounds of age, race, gender, disability, creed or sexual orientation and comply with all relevant UK legislation. If you do not hear back from us within **** working days please assume you have not been successful. This job was originally posted as www.totaljobs.com/JobSeeking/InvestmentsTreasuryController_job****'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description[emp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw description:\n",
      " Our client, based in Eastleigh, is looking for an Investments and Treasury Controller to join their team. Duties to include: Take responsibility for transactional management, analysis and oversight of the Company’s investment portfolio, including compliance with relevant sections of the relevant policies Ensure that working capital and other liquid resources and cashflow are managed efficiently Deliver consistently against relevant KPIs and KRIs, analysing any shortfalls and putting appropriate action plans in place to remediate process issues Manage day to day relationships with the Company’s outsourced Investment Managers and Custodians ensuring that there is mutual understanding of each others’ operations, systems and developments so that business is transacted efficiently and effectively Own endtoend investment processes, ensuring that processes, procedures, risks and controls are documented, effective and efficient. Regularly review and test processes and controls in accordance with Finance control and risk framework. Skills and Experience: The ability to build relationships with stakeholders at all levels both internal and external, able to challenge assumptions positively and bring people on the journey Strong written communication skills and must be fluent, articulate and confident in spoken communication Understanding of investment principles and significant experience of their practical application coupled with excellent analytical skills. A strong risks and control awareness is also required Strong organisational and planning skills coupled with ability to drive appropriate solutions. Proactive, highly motivated and change orientated, able to identify obstacles and clear them effectively IT literate, including experience of accounting systems and data warehouse architectures as well as desktop systems. Fluent with using Microsoft Excel, Word and PowerPoint Ability to understand, evaluate and interpret diverse financial data You will have experience of in a treasury/investment role, and must be ACA/ACCA/CIMA qualified or equivalent. August Clarke Limited is acting as a recruitment agency in relation to this role. We do not discriminate on the grounds of age, race, gender, disability, creed or sexual orientation and comply with all relevant UK legislation. If you do not hear back from us within **** working days please assume you have not been successful. This job was originally posted as www.totaljobs.com/JobSeeking/InvestmentsTreasuryController_job**** \n",
      "\n",
      "Tokenized description:\n",
      " ['our', 'client', 'based', 'in', 'eastleigh', 'is', 'looking', 'for', 'an', 'investments', 'and', 'treasury', 'controller', 'to', 'join', 'their', 'team', 'duties', 'to', 'include', 'take', 'responsibility', 'for', 'transactional', 'management', 'analysis', 'and', 'oversight', 'of', 'the', 'company', 's', 'investment', 'portfolio', 'including', 'compliance', 'with', 'relevant', 'sections', 'of', 'the', 'relevant', 'policies', 'ensure', 'that', 'working', 'capital', 'and', 'other', 'liquid', 'resources', 'and', 'cashflow', 'are', 'managed', 'efficiently', 'deliver', 'consistently', 'against', 'relevant', 'kpis', 'and', 'kris', 'analysing', 'any', 'shortfalls', 'and', 'putting', 'appropriate', 'action', 'plans', 'in', 'place', 'to', 'remediate', 'process', 'issues', 'manage', 'day', 'to', 'day', 'relationships', 'with', 'the', 'company', 's', 'outsourced', 'investment', 'managers', 'and', 'custodians', 'ensuring', 'that', 'there', 'is', 'mutual', 'understanding', 'of', 'each', 'others', 'operations', 'systems', 'and', 'developments', 'so', 'that', 'business', 'is', 'transacted', 'efficiently', 'and', 'effectively', 'own', 'endtoend', 'investment', 'processes', 'ensuring', 'that', 'processes', 'procedures', 'risks', 'and', 'controls', 'are', 'documented', 'effective', 'and', 'efficient', 'regularly', 'review', 'and', 'test', 'processes', 'and', 'controls', 'in', 'accordance', 'with', 'finance', 'control', 'and', 'risk', 'framework', 'skills', 'and', 'experience', 'the', 'ability', 'to', 'build', 'relationships', 'with', 'stakeholders', 'at', 'all', 'levels', 'both', 'internal', 'and', 'external', 'able', 'to', 'challenge', 'assumptions', 'positively', 'and', 'bring', 'people', 'on', 'the', 'journey', 'strong', 'written', 'communication', 'skills', 'and', 'must', 'be', 'fluent', 'articulate', 'and', 'confident', 'in', 'spoken', 'communication', 'understanding', 'of', 'investment', 'principles', 'and', 'significant', 'experience', 'of', 'their', 'practical', 'application', 'coupled', 'with', 'excellent', 'analytical', 'skills', 'a', 'strong', 'risks', 'and', 'control', 'awareness', 'is', 'also', 'required', 'strong', 'organisational', 'and', 'planning', 'skills', 'coupled', 'with', 'ability', 'to', 'drive', 'appropriate', 'solutions', 'proactive', 'highly', 'motivated', 'and', 'change', 'orientated', 'able', 'to', 'identify', 'obstacles', 'and', 'clear', 'them', 'effectively', 'it', 'literate', 'including', 'experience', 'of', 'accounting', 'systems', 'and', 'data', 'warehouse', 'architectures', 'as', 'well', 'as', 'desktop', 'systems', 'fluent', 'with', 'using', 'microsoft', 'excel', 'word', 'and', 'powerpoint', 'ability', 'to', 'understand', 'evaluate', 'and', 'interpret', 'diverse', 'financial', 'data', 'you', 'will', 'have', 'experience', 'of', 'in', 'a', 'treasury', 'investment', 'role', 'and', 'must', 'be', 'aca', 'acca', 'cima', 'qualified', 'or', 'equivalent', 'august', 'clarke', 'limited', 'is', 'acting', 'as', 'a', 'recruitment', 'agency', 'in', 'relation', 'to', 'this', 'role', 'we', 'do', 'not', 'discriminate', 'on', 'the', 'grounds', 'of', 'age', 'race', 'gender', 'disability', 'creed', 'or', 'sexual', 'orientation', 'and', 'comply', 'with', 'all', 'relevant', 'uk', 'legislation', 'if', 'you', 'do', 'not', 'hear', 'back', 'from', 'us', 'within', 'working', 'days', 'please', 'assume', 'you', 'have', 'not', 'been', 'successful', 'this', 'job', 'was', 'originally', 'posted', 'as', 'www', 'totaljobs', 'com', 'jobseeking', 'investmentstreasurycontroller', 'job']\n"
     ]
    }
   ],
   "source": [
    "def tokenizeDescription(raw_description):\n",
    "    \"\"\"\n",
    "        This function first convert all words to lowercases,\n",
    "        it then segment the raw description into sentences and tokenize each sentences\n",
    "        and convert the description to a list of tokens.\n",
    "    \"\"\"\n",
    "    # description = raw_description.decode('utf-8') # convert the bytes-like object to python string, need this before we apply any pattern search on it\n",
    "    description = raw_description.lower() # cover all words to lowercase\n",
    "\n",
    "    # segment into sentences\n",
    "    sentences = sent_tokenize(description)\n",
    "\n",
    "    # tokenize each sentence\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "\n",
    "    # merge them into a list of tokens\n",
    "    tokenised_description = list(chain.from_iterable(token_lists))\n",
    "    return tokenised_description\n",
    "\n",
    "tk_description = [tokenizeDescription(r) for r in description]  # list comprehension, generate a list of tokenized articles\n",
    "\n",
    "print(\"Raw description:\\n\",description[emp],'\\n')\n",
    "print(\"Tokenized description:\\n\",tk_description[emp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### A Few Statistics Before Any Further Pre-processing\n",
    "\n",
    "In the following, we are interested to know a few statistics at this very begining stage, including:\n",
    "* The total number of tokens across the corpus\n",
    "* The total number of types across the corpus, i.e. the size of vocabulary \n",
    "* The so-called, [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity), referring to the ratio of different unique word stems (types) to the total number of words (tokens).  \n",
    "* The average, minimum and maximum number of token (i.e. document length) in the dataset.\n",
    "\n",
    "In the following, we wrap all these up as a function, since we will use this printing module later to compare these statistic values before and after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9834\n",
      "Total number of tokens:  186952\n",
      "Lexical diversity:  0.052601737344345076\n",
      "Total number of description: 776\n",
      "Average description length: 240.91752577319588\n",
      "Maximun description length: 815\n",
      "Minimun description length: 13\n",
      "Standard deviation of description length: 124.97750685071483\n"
     ]
    }
   ],
   "source": [
    "def stats_print(tk_description):\n",
    "    words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of description:\", len(tk_description))\n",
    "    lens = [len(article) for article in tk_description]\n",
    "    print(\"Average description length:\", np.mean(lens))\n",
    "    print(\"Maximum description length:\", np.max(lens))\n",
    "    print(\"Minimum description length:\", np.min(lens))\n",
    "    print(\"Standard deviation of description length:\", np.std(lens))\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2.2 Remove words with length less than 2.\n",
    "\n",
    "In this sub-task, you are required to remove any token that only contains a single character (a token that of length 1).\n",
    "You need to double-check whether it has been done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appear only once: 4233\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"Number of words that appear less than 2:\", len([w for w in word_counts if word_counts[w] <= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appear less than 2: 4191\n"
     ]
    }
   ],
   "source": [
    "st_list = [[w for w in description if len(w) <= 1 ] \\\n",
    "                      for description in tk_description] # create a list of single character token for each description\n",
    "list(chain.from_iterable(st_list)) # merge them together in one list\n",
    "\n",
    "# filter out single character tokens\n",
    "tk_description = [[w for w in description if len(w) >=2] \\\n",
    "                      for description in tk_description]\n",
    "\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"Number of words that appear less than 2:\", len([w for w in word_counts if word_counts[w] <= 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 most frequent words:\n",
      " [('experience', 1276), ('sales', 1030), ('role', 946), ('work', 861), ('business', 832), ('team', 789), ('working', 719), ('job', 688), ('care', 675), ('skills', 669), ('company', 614), ('client', 594), ('management', 572), ('manager', 519), ('support', 501), ('uk', 496), ('service', 481), ('excellent', 455), ('development', 431), ('required', 399), ('based', 376), ('opportunity', 372), ('services', 369), ('knowledge', 349), ('apply', 349), ('successful', 340), ('training', 338), ('design', 337), ('engineering', 336), ('customer', 335), ('recruitment', 335), ('salary', 322), ('candidate', 319), ('clients', 310), ('high', 309), ('join', 302), ('ability', 301), ('strong', 299), ('provide', 298), ('home', 291), ('ensure', 290), ('leading', 289), ('including', 287), ('engineer', 285), ('not', 280), ('financial', 279), ('good', 274), ('staff', 271), ('position', 268), ('systems', 267)] \n",
      "\n",
      "\n",
      "Top 50 most frequent words after removing:\n",
      " [('experience', 1276), ('sales', 1030), ('role', 946), ('work', 861), ('business', 832), ('team', 789), ('working', 719), ('job', 688), ('care', 675), ('skills', 669), ('company', 614), ('client', 594), ('management', 572), ('manager', 519), ('support', 501), ('uk', 496), ('service', 481), ('excellent', 455), ('development', 431), ('required', 399), ('based', 376), ('opportunity', 372), ('services', 369), ('knowledge', 349), ('apply', 349), ('successful', 340), ('training', 338), ('design', 337), ('engineering', 336), ('customer', 335), ('recruitment', 335), ('salary', 322), ('candidate', 319), ('clients', 310), ('high', 309), ('join', 302), ('ability', 301), ('strong', 299), ('provide', 298), ('home', 291), ('ensure', 290), ('leading', 289), ('including', 287), ('engineer', 285), ('not', 280), ('financial', 279), ('good', 274), ('staff', 271), ('position', 268), ('systems', 267)]\n"
     ]
    }
   ],
   "source": [
    "# Remove the top 50 most frequent words\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "top50 = word_counts.most_common(50) # get the top 50 most frequent words\n",
    "print(\"Top 50 most frequent words:\\n\",top50, \"\\n\\n\")\n",
    "\n",
    "tk_description = [[w for w in description if w not in top50] for description in tk_description]\n",
    "top50 = word_counts.most_common(50) # get the top 50 most frequent words\n",
    "print(\"Top 50 most frequent words after removing:\\n\",top50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2.3 Removing Stop words\n",
    "\n",
    "In this sub-task, you are required to remove the stop words from the tokenized text inside `stopwords_en.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words:\n",
      " ['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# remove the stop words inside `stopwords_en.txt` from the tokenized text\n",
    "with open('stopwords_en.txt', 'r') as f:\n",
    "    stop_words = f.read().splitlines() # read the stop words into a list\n",
    "print(\"Stop words:\\n\",stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ain't\",\n",
       " 'another',\n",
       " \"aren't\",\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " \"couldn't\",\n",
       " \"didn't\",\n",
       " \"doesn't\",\n",
       " \"don't\",\n",
       " 'enough',\n",
       " \"hadn't\",\n",
       " \"hasn't\",\n",
       " \"haven't\",\n",
       " 'ignored',\n",
       " \"isn't\",\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " \"shouldn't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"won't\",\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in stop_words if (\"not\" in w or \"n't\" in w or \"no\" in w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The Updated Statistics\n",
    "\n",
    "In the above, we have done a few pre-processed steps, now let's have a look at the statistics again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9423\n",
      "Total number of tokens:  107751\n",
      "Lexical diversity:  0.08745162457889022\n",
      "Total number of description: 776\n",
      "Average description length: 138.85438144329896\n",
      "Maximun description length: 489\n",
      "Minimun description length: 12\n",
      "Standard deviation of description length: 73.42099464751045\n"
     ]
    }
   ],
   "source": [
    "# specify\n",
    "ignored_words = [w for w in stop_words if not (\"not\" in w or \"n't\" in w or \"no\" in w)]\n",
    "\n",
    "# filter out stop words\n",
    "tk_description = [[w for w in description if w not in ignored_words] \\\n",
    "                      for description in tk_description]\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall, from the beginning, we have the following:  \n",
    "_____________________________________________\n",
    "\n",
    "Vocabulary size:  9834\n",
    "\n",
    "Total number of tokens:  186952\n",
    "\n",
    "Lexical diversity:  0.052601737344345076\n",
    "\n",
    "Total number of description: 776\n",
    "\n",
    "Average description length: 240.91752577319588\n",
    "\n",
    "Maximun description length: 815\n",
    "\n",
    "Minimun description length: 13\n",
    "\n",
    "Standard deviation of description length: 124.97750685071483\n",
    "_____________________________________________\n",
    "\n",
    "We've shrunk more than 40% of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "\n",
    "We are going to store all the preprocessed description texts and its corresponding labels into files for task 2.\n",
    "* all the tokenized description are stored in a .txt file named `description.txt`\n",
    "    * each line is a description text, which contained all the tokens of the description text, separated by a space ' '\n",
    "* all the corresponding labels are store in a .txt file named `category.txt`\n",
    "    * each line is a label (one of these 4 values: 0,1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved description into description.txt\n",
      "Successfully saved category into category.txt\n",
      "Successfully saved title into title.txt\n"
     ]
    }
   ],
   "source": [
    "# save description text\n",
    "def save_description(descriptionFilename,tk_description):\n",
    "    out_file = open(descriptionFilename, 'w') # creates a txt file and open to save the descriptions\n",
    "    string = \"\\n\".join([\" \".join(description) for description in tk_description])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the category corresponding with the description text\n",
    "def save_category(categoryFilename,category):\n",
    "    out_file = open(categoryFilename, 'w') # creates a txt file and open to save category\n",
    "    string = \"\\n\".join([str(s) for s in category])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the title corresponding with the description text\n",
    "def save_title(titleFilename,title):\n",
    "    out_file = open(titleFilename, 'w') # creates a txt file and open to save title\n",
    "    string = \"\\n\".join([str(s) for s in title])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "\n",
    "# save description into txt file\n",
    "descriptionFilename = \"description.txt\"\n",
    "save_description(descriptionFilename,tk_description)\n",
    "print(f'Successfully saved description into {descriptionFilename}')\n",
    "\n",
    "# save category into txt file\n",
    "categoryFilename = \"category.txt\"\n",
    "save_category(categoryFilename,category)\n",
    "print(f'Successfully saved category into {categoryFilename}')\n",
    "\n",
    "# save title into txt file\n",
    "titleFilename = \"title.txt\"\n",
    "save_title(titleFilename,title)\n",
    "print(f'Successfully saved title into {titleFilename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`vocab.txt`\n",
    "This file contains the unigram vocabulary, one each line, in the following format: word_string:word_integer_index. Very importantly, words in the vocabulary must be sorted in alphabetical order, and the index value starts from 0. This file is the key to interpret the sparse encoding. For instance, in the following example, the word aaron is the 20th word (the corresponding integer_index as 19) in the vocabulary (note that the index values and words in the following image are artificial and used to demonstrate the required format only, it doesn't reflect the values of the actual expected output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully write job advertisement with the tokenized description in txt file\n"
     ]
    }
   ],
   "source": [
    "# Save all job advertisement text and information in txt file\n",
    "with open('job_ad.txt', 'w') as f:\n",
    "    f.write(\"Category: \" + str(category) + \"\\n\")\n",
    "    for i in range(len(tk_description)):\n",
    "        f.write(full_description[i] + \"\\n\")\n",
    "        f.write(\"Tokenized Description: \" + str(tk_description[i]) + \"\\n\")\n",
    "        f.write(\"Category: \" + str(df['target'][i]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    print(\"Successfully write job advertisement with the tokenized description in txt file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aah', 'aap', 'aaron', 'aat', 'abandonment', 'abb', 'abenefit', 'aberdeen', 'aberdenshire', 'abi']\n"
     ]
    }
   ],
   "source": [
    "def write_vocab(vocab, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            f.write(word + ':' + str(i) + '\\n')\n",
    "# convert tokenized description into a alphabetically sorted list\n",
    "vocab = sorted(list(set(chain.from_iterable(tk_description))))\n",
    "write_vocab(vocab, 'vocab.txt')\n",
    "# print out the first 10 words in the vocabulary\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category at index 0: .ipynb_checkpoints\n",
      "Category at index 1: Accounting_Finance\n",
      "Category at index 2: Engineering\n",
      "Category at index 3: Healthcare_Nursing\n"
     ]
    }
   ],
   "source": [
    "print(f'Category at index 0: {df[\"target_names\"][0]}')\n",
    "print(f'Category at index 1: {df[\"target_names\"][1]}')\n",
    "print(f'Category at index 2: {df[\"target_names\"][2]}')\n",
    "print(f'Category at index 3: {df[\"target_names\"][3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 776 entries, 0 to 775\n",
      "Data columns (total 6 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Title                  776 non-null    object\n",
      " 1   Webindex               776 non-null    int64 \n",
      " 2   Company                776 non-null    object\n",
      " 3   Description            776 non-null    object\n",
      " 4   Tokenized Description  776 non-null    object\n",
      " 5   Category               776 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 36.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Webindex</th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>Tokenized Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finance / Accounts Asst Bromley to ****k</td>\n",
       "      <td>68997528</td>\n",
       "      <td>First Recruitment Services</td>\n",
       "      <td>Accountant (partqualified) to **** p.a. South ...</td>\n",
       "      <td>accountant partqualified south east london cli...</td>\n",
       "      <td>Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fund Accountant  Hedge Fund</td>\n",
       "      <td>68063513</td>\n",
       "      <td>Austin Andrew Ltd</td>\n",
       "      <td>One of the leading Hedge Funds in London is cu...</td>\n",
       "      <td>leading hedge funds london recruiting fund acc...</td>\n",
       "      <td>Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deputy Home Manager</td>\n",
       "      <td>68700336</td>\n",
       "      <td>Caritas</td>\n",
       "      <td>An exciting opportunity has arisen to join an ...</td>\n",
       "      <td>exciting opportunity arisen join establish pro...</td>\n",
       "      <td>Sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  Webindex  \\\n",
       "0  Finance / Accounts Asst Bromley to ****k  68997528   \n",
       "1               Fund Accountant  Hedge Fund  68063513   \n",
       "2                       Deputy Home Manager  68700336   \n",
       "\n",
       "                      Company  \\\n",
       "0  First Recruitment Services   \n",
       "1           Austin Andrew Ltd   \n",
       "2                     Caritas   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Accountant (partqualified) to **** p.a. South ...   \n",
       "1  One of the leading Hedge Funds in London is cu...   \n",
       "2  An exciting opportunity has arisen to join an ...   \n",
       "\n",
       "                               Tokenized Description     Category  \n",
       "0  accountant partqualified south east london cli...  Engineering  \n",
       "1  leading hedge funds london recruiting fund acc...  Engineering  \n",
       "2  exciting opportunity arisen join establish pro...        Sales  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert job ad to a dataframe\n",
    "job_ad = pd.DataFrame({'Title': title, 'Webindex': webindex, 'Company': company, 'Description': description,'Tokenized Description': tk_description, 'Category': category})\n",
    "\n",
    "# change Tokenized Description to string separated by space\n",
    "job_ad['Tokenized Description'] = job_ad['Tokenized Description'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "\n",
    "\n",
    "# replace the value in Category column\n",
    "# Category at index 0: Accounting_Finance\n",
    "# Category at index 1: Engineering\n",
    "# Category at index 2: Healthcare_Nursing\n",
    "# Category at index 3: Sales\n",
    "job_ad['Category'] = job_ad['Category'].replace([0,1,2,3],['Accounting_Finance','Engineering','Healthcare_Nursing','Sales'])\n",
    "\n",
    "# Cast Webindex to int\n",
    "job_ad['Webindex'] = job_ad['Webindex'].astype(int)\n",
    "\n",
    "# save job ad to csv file\n",
    "job_ad.to_csv('job_ad.csv', index=False)\n",
    "\n",
    "print(job_ad.info())\n",
    "# print first 3 rows\n",
    "job_ad.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook task1.ipynb to python\n",
      "[NbConvertApp] Writing 21057 bytes to task1.py\n",
      "[NbConvertApp] Converting notebook task2_3.ipynb to python\n",
      "[NbConvertApp] Writing 18131 bytes to task2_3.py\n"
     ]
    }
   ],
   "source": [
    "# The .py format of the jupyter notebook\n",
    "for fname in os.listdir():\n",
    "    if fname.endswith('ipynb'):\n",
    "        os.system(f'jupyter nbconvert {fname} --to python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
