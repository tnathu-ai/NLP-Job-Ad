{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "\n",
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Task 1. Text Pre-processing</strong></h3>\n",
    "\n",
    "#### Student Name: Tran Ngoc Anh Thu\n",
    "#### Student ID: s3879312\n",
    "\n",
    "Date: \"October 2, 2022\"\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* sklearn\n",
    "* collections\n",
    "* re\n",
    "* numpy\n",
    "* nltk\n",
    "* itertools\n",
    "* pandas\n",
    "* os\n",
    "\n",
    "## Steps\n",
    "1. Load data\n",
    "2. Basic Text Pre-processing\n",
    "    * Sentence Segmentation\n",
    "    * Word Tokenization\n",
    "    * Removing Single Character Tokens\n",
    "    * Removing Stop words\n",
    "3. Summary\n",
    "4. References\n",
    "\n",
    "## Introduction\n",
    "Nowadays there are many job hunting websites including seek.com.au and au.indeed.com. These job hunting sites all manage a job search system, where job hunters could search for relevant jobs based on keywords, salary, and categories. In previous years, the category of an advertised job was often manually entered by the advertiser (e.g., the test_indexloyer). There were mistakes made for category assignment. As a result, the jobs in the wrong class did not get enough exposure to relevant candidate groups.\n",
    "With advances in text analysis, automated job classification has become feasible; and sensible suggestions for job categories can then be made to potential advertisers. This can help reduce human data entry error, increase the job exposure to relevant candidates, and also improve the user experience of the job hunting site. In order to do so, we need an automated job ads classification system that helps to predict the categories of newly entered job advertisements.\n",
    "\n",
    "NLP uses a hierarchy to determine which groups of words and sentences belong to each other. The smallest level of text is a token which can be a sentence or an individual word. A group of tokens is called a document, for instance each text file containing job description. A group of documents is called a corpus, in this case the job category folder which containng several job adverisement files inside . Finally, a group of corpus is called a corpora, which can be several job categories we wish to compare and evaluate.\n",
    "\n",
    "In this **task1** notebook, we are going to explore a job advertisement data set, and focus on pre-processing the description only.\n",
    "In the next task **task2_3**, we will then use the pre-processed text reviews to generate data features and build classification models to predict label of the description.\n",
    "\n",
    "## Dataset\n",
    "+ A small collection of job advertisement documents (around 776 jobs) inside the `data` folder.\n",
    "+ Inside the data folder, there are four different sub-folders: Accounting_Finance, Engineering, Healthcare_Nursing, and Sales, representing a job category.\n",
    "+ The job advertisement text documents of a particular category are in the corresponding sub-folder.\n",
    "+ Each job advertisement document is a txt file named `Job_<ID>.txt`. It contains the title, the webindex (some will also have information on the company name, some might not), and the full description of the job advertisement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "from collections import Counter\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version:  1.21.5\n",
      "Pandas version:  1.4.2\n",
      "Python 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# check the version of the main packages\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Pandas version: \",pd.__version__)\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.1 Examining and loading data</strong></h3>\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n",
    "\n",
    "\n",
    "\n",
    "Before doing any pre-processing, we need to load the data into a proper format. \n",
    "To load the data, you have to explore the data folder. Inside the `data` folder:\n",
    "+ Inside the data folder you will see 4 different subfolders, namely: `Accounting_Finance`, `Engineering`,`Healthcare_Nursing`, and `Sales`, each folder name is a job category.\n",
    "+ The job advertisement text documents of a particular category are located in the corresponding subfolder.\n",
    "+ Each job advertisement document is a txt file, named as \"Job_<ID>.txt\". It contains the title, the webindex,(some will also have information on the company name, some might not), and the full description of the job advertisement. \n",
    "\n",
    "In this case, providing that the dataset is given in a very well organised way, I would use a super handy API [`load_files`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) from `sklearn.datasets`. \n",
    "    \n",
    "**import the function by:**\n",
    "```python\n",
    "from sklearn.datasets import load_files  \n",
    "```\n",
    "\n",
    "Then you can use the function to directly load the data and labels, for example:\n",
    "```python\n",
    "df = load_files(r\"data\")  \n",
    "```\n",
    "\n",
    "The loaded `data` is then a dictionary, with the following attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **DESCRIPTION**                                           |\n",
    "|--------------|---------------------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website            |\n",
    "| Title        | Title of the advertised job position                          |\n",
    "| Company      | Company (test_indexloyer) of the advertised job position             |\n",
    "| Description  | the description of each job advertisement                     |\n",
    "| Category     | The category of the advertised job position                   |\n",
    "\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of the loaded data and labels using sklearn API: <class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "# load each folder and file inside the data folder\n",
    "df = load_files(r\"data\")\n",
    "# type of the loaded file\n",
    "print(f'Data type of the loaded data and labels using sklearn API: {type(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 2, 1, 2, 0, 3, 3, 0, 0, 1, 3, 1, 3, 3, 1, 3, 2, 2, 2,\n",
       "       3, 3, 0, 2, 2, 2, 0, 2, 3, 1, 2, 0, 1, 3, 3, 1, 1, 0, 2, 2, 2, 2,\n",
       "       0, 0, 2, 1, 3, 1, 1, 2, 2, 3, 0, 0, 1, 0, 2, 2, 3, 3, 3, 0, 3, 0,\n",
       "       1, 2, 3, 1, 3, 2, 3, 1, 3, 2, 1, 3, 2, 1, 3, 2, 2, 1, 0, 1, 1, 1,\n",
       "       3, 0, 3, 1, 3, 2, 2, 0, 2, 3, 2, 1, 0, 1, 1, 2, 0, 3, 0, 1, 3, 2,\n",
       "       1, 2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 3,\n",
       "       2, 0, 0, 1, 3, 2, 0, 1, 0, 3, 1, 2, 1, 0, 0, 0, 3, 0, 1, 2, 3, 1,\n",
       "       1, 1, 2, 1, 0, 1, 0, 1, 0, 1, 1, 2, 0, 2, 2, 0, 2, 3, 2, 2, 0, 2,\n",
       "       1, 0, 1, 1, 1, 3, 1, 3, 1, 0, 3, 1, 0, 2, 0, 0, 2, 1, 1, 0, 1, 3,\n",
       "       0, 1, 1, 3, 0, 1, 0, 2, 3, 0, 2, 0, 1, 0, 1, 3, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 2, 1, 3, 1, 2, 3, 1, 1, 2, 0, 0, 1, 2, 0, 3, 2, 3, 2, 2, 3,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 3, 1, 1, 0, 0, 2, 1, 2, 2, 2, 2, 1, 3, 1,\n",
       "       2, 1, 2, 3, 2, 3, 0, 1, 3, 0, 2, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 2,\n",
       "       2, 1, 2, 0, 2, 2, 1, 2, 0, 1, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 0, 1,\n",
       "       0, 0, 0, 0, 2, 2, 0, 0, 2, 1, 2, 0, 0, 1, 2, 3, 1, 3, 2, 0, 2, 2,\n",
       "       1, 3, 1, 0, 1, 2, 3, 1, 0, 0, 1, 1, 0, 2, 1, 3, 2, 3, 3, 1, 3, 1,\n",
       "       2, 1, 1, 2, 2, 3, 3, 2, 2, 1, 1, 2, 0, 2, 3, 0, 0, 1, 0, 1, 2, 1,\n",
       "       1, 3, 0, 2, 0, 1, 3, 1, 3, 2, 0, 2, 1, 3, 0, 0, 0, 3, 3, 0, 2, 3,\n",
       "       3, 1, 3, 0, 1, 0, 3, 1, 3, 1, 3, 2, 1, 1, 2, 3, 0, 0, 3, 3, 3, 1,\n",
       "       2, 2, 3, 0, 0, 0, 1, 0, 2, 1, 3, 2, 1, 2, 2, 1, 2, 2, 0, 0, 2, 3,\n",
       "       2, 1, 2, 3, 1, 2, 1, 0, 1, 2, 1, 2, 0, 3, 2, 3, 2, 3, 0, 0, 3, 2,\n",
       "       0, 2, 1, 3, 0, 3, 2, 1, 1, 2, 1, 2, 3, 1, 1, 1, 3, 1, 2, 1, 0, 0,\n",
       "       3, 0, 2, 2, 3, 3, 0, 3, 1, 1, 2, 2, 2, 3, 2, 1, 1, 0, 2, 3, 0, 0,\n",
       "       0, 0, 2, 1, 3, 0, 2, 1, 2, 2, 3, 1, 2, 0, 2, 0, 3, 2, 1, 2, 2, 3,\n",
       "       1, 1, 0, 2, 2, 1, 1, 3, 0, 1, 0, 0, 2, 3, 3, 0, 0, 1, 1, 2, 3, 3,\n",
       "       3, 0, 0, 2, 3, 1, 3, 1, 3, 3, 1, 0, 0, 0, 1, 0, 3, 1, 2, 2, 1, 1,\n",
       "       0, 1, 2, 2, 3, 3, 3, 1, 2, 3, 1, 0, 1, 3, 3, 3, 3, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 2, 2, 1, 3, 2, 2, 2, 1, 2, 1, 0, 2, 1, 2, 2, 0, 2,\n",
       "       0, 0, 0, 0, 2, 2, 2, 0, 1, 1, 3, 1, 2, 1, 2, 0, 1, 1, 1, 1, 0, 2,\n",
       "       3, 1, 1, 2, 3, 2, 0, 1, 3, 3, 1, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 2, 1, 0, 2, 0, 2, 0, 1, 0, 0, 3, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1,\n",
       "       1, 2, 1, 0, 3, 3, 0, 2, 1, 2, 1, 1, 3, 0, 0, 0, 3, 0, 1, 1, 0, 3,\n",
       "       2, 2, 1, 0, 0, 2, 0, 1, 0, 1, 1, 3, 2, 0, 2, 1, 1, 1, 2, 2, 0, 3,\n",
       "       0, 0, 0, 2, 2, 3, 0, 1, 3, 2, 2, 3, 3, 1, 1, 2, 0, 3, 0, 0, 2, 2,\n",
       "       3, 0, 0, 2, 3, 3, 0, 0, 2, 2, 3, 1, 1, 2, 2, 0, 2, 3, 1, 3, 0, 1,\n",
       "       3, 3, 1, 3, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] # this corresponding to the index value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of the categories\n",
    "df['target_names'] # this corresponding to the name value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category at index 0: Accounting_Finance\n",
      "Category at index 1: Engineering\n",
      "Category at index 2: Healthcare_Nursing\n",
      "Category at index 3: Sales\n"
     ]
    }
   ],
   "source": [
    "# loop through the index of the target_names and print the category name\n",
    "for i in range(len(df['target_names'])):\n",
    "    print(f'Category at index {i}: {df[\"target_names\"][i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Create temporary variable and assign a number for testing at that index**\n",
    "\n",
    "`test_index` is a number to test whether the attribute at that position matches the desired outputs. So we don't need to print to whole lengthly output each test and void memory problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/Healthcare_Nursing/Job_00491.txt', 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index = 20 # an example to test for the whole task outputs.\n",
    "\n",
    "df['filenames'][test_index], df['target'][test_index] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### --------------> OBSERVATION\n",
    "from the file path and the label we know that it's the correct label too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job description: b'Title: PERM Unit Mgr RGN Kid minster Flexi ****K due\\nWebindex: 71692209\\nDescription: Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home\\xe2\\x80\\x99s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home\\xe2\\x80\\x99s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home\\xe2\\x80\\x99s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home\\xe2\\x80\\x99s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user\\xe2\\x80\\x99s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user\\xe2\\x80\\x99s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home\\xe2\\x80\\x99s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home\\xe2\\x80\\x99s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home\\xe2\\x80\\x99s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user\\xe2\\x80\\x99s stay as comfortable and enjoyable as possible.'\n",
      "\n",
      "Corresponding to the label 2 inside the data/Healthcare_Nursing/Job_00491.txt directory\n"
     ]
    }
   ],
   "source": [
    "# assign variables\n",
    "full_description, category, directory = df.data, df.target, df.filenames\n",
    "\n",
    "# the test_index job advertisement description\n",
    "print(f'Job description: {full_description[test_index]}\\n\\nCorresponding to the label {category[test_index]} inside the {directory[test_index]} directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ------> OBSERVATION:\n",
    "As we can see the current description is in the **binary** form and read as a byte object (a `b` in front of each review text if you print it out). Therefore, we need to decode into normal string for further pre-processing\n",
    "\n",
    "However, the tokenizer cannot apply a string pattern on a bytes-like object. To resolve this, we decode each read `full_description` text using `utf-8` by writing a decode function\n",
    "\n",
    "### Decode the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: PERM Unit Mgr RGN Kid minster Flexi ****K due\\nWebindex: 71692209\\nDescription: Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home’s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home’s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home’s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home’s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user’s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user’s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home’s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home’s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home’s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user’s stay as comfortable and enjoyable as possible.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(l):\n",
    "    if isinstance(l, list):\n",
    "        return [decode(x) for x in l]\n",
    "    else:\n",
    "        return l.decode('utf-8')\n",
    "\n",
    "# decode the binary description into utf-8 form and save it to full_description\n",
    "full_description = decode(full_description)\n",
    "full_description[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ---------------> OBSERVATION:\n",
    "The current `full_description` contains these attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **MEANING**                                        |\n",
    "|--------------|----------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website |\n",
    "| Title        | Title of the advertised job position               |\n",
    "| Company      | Company (test_indexloyer) of the advertised job position  |\n",
    "| Description  | the description of each job advertisement          |\n",
    "\n",
    "I only want the description to perform text-preprocessing in task 1 and other attributes for further exploring different features of a job advertisement, e.g., the title in task 3, to test the accuracy. Therefore, I will extract each above attribute inside the `full_description`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.2 Pre-processing</strong></h3>\n",
    "\n",
    "1. Extract information from each job advertisement. Perform the following pre-processing steps to the description of each job advertisement;\n",
    "2. Tokenize each job advertisement description. The word tokenization must use the following regular expression, `r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"`;\n",
    "3. All the words must be converted into the lower case;\n",
    "4. Remove words with length less than 2.\n",
    "5. Remove stopwords using the provided stop words list (i.e, stopwords_en.txt). It is located inside the\n",
    "same downloaded folder.\n",
    "6. Remove the word that appears only once in the document collection, based on term frequency.\n",
    "7. Remove the top 50 most frequent words based on document frequency.\n",
    "8. Save all job advertisement text and information in txt file(s) (you have flexibility to choose what format\n",
    "you want to save the preprocessed job ads, and you will need to retrieve the pre-processed job ads\n",
    "text in Task 2 & 3);\n",
    "9. Build a vocabulary of the cleaned job advertisement descriptions, save it in a txt file (please refer to the\n",
    "required output);\n",
    "\n",
    "## 1.2.1 Extract information from each job advertisement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job description at index 20:\n",
      "Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home’s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home’s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home’s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home’s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user’s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user’s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home’s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home’s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home’s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user’s stay as comfortable and enjoyable as possible.\n",
      "\n",
      "Job title at index 20:\n",
      "PERM Unit Mgr RGN Kid minster Flexi ****K due\n",
      "\n",
      "Job webindex at index 20:\n",
      "71692209\n",
      "\n",
      "Job company at index 20:\n",
      "NA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract description, title, webindex,  from each job advertisement and test using test_index\n",
    "\n",
    "# Extract description\n",
    "def extract_description(full_description):\n",
    "    description = [re.search(r'\\nDescription: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return description\n",
    "description = extract_description(full_description)\n",
    "print(f'Job description at index {test_index}:\\n{description[test_index]}\\n')\n",
    "\n",
    "# Extract title\n",
    "def extract_title(full_description):\n",
    "    title = [re.search(r'Title: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return title\n",
    "title = extract_title(full_description)\n",
    "print(f'Job title at index {test_index}:\\n{title[test_index]}\\n')\n",
    "\n",
    "# Extract webindex\n",
    "def extract_webindex(full_description):\n",
    "    webindex = [re.search(r'Webindex: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return webindex\n",
    "webindex = extract_webindex(full_description)\n",
    "print(f'Job webindex at index {test_index}:\\n{webindex[test_index]}\\n')\n",
    "\n",
    "# Extract company\n",
    "def extract_company(company):\n",
    "    company = [re.search(r'Company: (.*)', str(i)).group(1) if re.search(r'Company: (.*)', str(i)) else \"NA\" for i in company]\n",
    "    return company\n",
    "company = extract_company(full_description)\n",
    "print(f'Job company at index {test_index}:\\n{company[test_index]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2.2 + 1.2.3 \n",
    "## Tokenize description using regular expression & lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw description:\n",
      " Job Title: Unit Manager Reporting to: Registered Manager Job Purpose: To manage in a professional manner the day to day running of the home’s administration, clinical policies and procedures, training and care planning. To implement working practices that monitors the health and welfare of the home’s service users and staff and their respective environments. To promote quality care within a warm friendly ambience. Key Result Areas Managing To work with the Directors to achieve the home’s financial targets. To manage the home in a manner which will not bring the home or service users into disrepute. To maintain confidentiality on all aspects of care and staff management. To ensure all the home’s policies and procedures are implemented and followed by all staff. To inform the Registered Manager immediately if a serious difficulty or event occurs. Managing Support To delegate responsibility effectively and within legal boundaries. To ensure through clinical standards and audits that good practice is maintained throughout the home. Nursing Duties To perform nursing duties that reflect current best practice and keep abreast of latest developments To inform the Registered Manager of any nursing and medical matters affecting the service user’s care. To assist service users with personal care which includes; using the bath / bed bath or shower, feet and nail care, hair care, shaving, mouth care, denture care, toileting needs, all daily activities as per individual care plans including tidying bed and room. To assist clients with their psychological needs, which includes; talking, listening, excursions, liaising with family, assisting with hobbies and recreation activities. To care for the service user as an individual and maintain a high level of care for their particular condition ensuring physical needs, comfort and dignity are met. To care for service user’s at their end of life in a respectful and dignified manner. To administer medication strictly in accordance with the Drug Administration policy and NMC guidelines. To assist the Registered Manager in ensuring adequate supplies of medication and correct storage as per the medication policy. To assist with serving meals and other domestic duties such as laundry, occasional cleaning etc. To deal with any internal or external communications from service users or third parties. To ensure all documentation is kept up to date and recorded accurately. To maintain records / care plans as required by the Care Quality Commission. Business Management To work both with the Directors and independently to generate enquiries and proactively deal with members of the public, and care / medical professionals, to ensure a high level of occupancy rates. To manage and have a working knowledge of the Home’s Policies and Procedures and ensure they are maintained at all times. To promote and act strictly in accordance with the Home’s Health and Safety Policy and ensure good Health and Safety practice within the working environment to comply with COSHH / RIDDOR / Environmental Health / Fire regulations and the Care Standards Act. To bring to the Directors immediate attention any item of Health and Safety importance that would be of concern and potentially needing prompt action. To have an in depth operational knowledge of all emergency procedures and ensure they are cascaded down through all the staff and service users. To attend staff meetings as arranged and to ensure other staff informed of items discussed and decisions made. To attend at least 21 hours per annum of compulsory training outlined by your personal development plan. To participate in regular reviews and an annual appraisal in order to develop your training needs in relation to skills and knowledge. To promote the good name of the Home at all times. To cover colleagues duties in times of sickness and holidays. To develop and share ideas for the improvement of practice within the home. To implement change sensitively but effectively. To replenish stocks and exercise cost / stock control. To operate within the guidelines of the Data Protection Act and ensure total confidentiality, especially with regards to staff and service users. Managing People To assist and supervise staff training. To be responsible for covering staff absences and managing disputes where appropriate. To ensure all staff contribute to the best of their ability to the efficient running of the home and that high standards are maintained. To ensure staff have the qualifications and training necessary for the duties they perform. To ensure all staff have two monthly reviews and an annual appraisal as per company policy. FURTHER DUTIES: In addition to the responsibilities listed above it may be necessary to perform other duties as required by the Home’s management or senior staff. Consideration will be given to your skills and status when given these duties. NOTE: It is expected that all duties carried out will be performed in a spirit of cooperation required from a dedicated efficient team whose prime aim is to make the service user’s stay as comfortable and enjoyable as possible. \n",
      "\n",
      "Tokenized description:\n",
      " ['job', 'title', 'unit', 'manager', 'reporting', 'to', 'registered', 'manager', 'job', 'purpose', 'to', 'manage', 'in', 'a', 'professional', 'manner', 'the', 'day', 'to', 'day', 'running', 'of', 'the', 'home', 's', 'administration', 'clinical', 'policies', 'and', 'procedures', 'training', 'and', 'care', 'planning', 'to', 'implement', 'working', 'practices', 'that', 'monitors', 'the', 'health', 'and', 'welfare', 'of', 'the', 'home', 's', 'service', 'users', 'and', 'staff', 'and', 'their', 'respective', 'environments', 'to', 'promote', 'quality', 'care', 'within', 'a', 'warm', 'friendly', 'ambience', 'key', 'result', 'areas', 'managing', 'to', 'work', 'with', 'the', 'directors', 'to', 'achieve', 'the', 'home', 's', 'financial', 'targets', 'to', 'manage', 'the', 'home', 'in', 'a', 'manner', 'which', 'will', 'not', 'bring', 'the', 'home', 'or', 'service', 'users', 'into', 'disrepute', 'to', 'maintain', 'confidentiality', 'on', 'all', 'aspects', 'of', 'care', 'and', 'staff', 'management', 'to', 'ensure', 'all', 'the', 'home', 's', 'policies', 'and', 'procedures', 'are', 'implemented', 'and', 'followed', 'by', 'all', 'staff', 'to', 'inform', 'the', 'registered', 'manager', 'immediately', 'if', 'a', 'serious', 'difficulty', 'or', 'event', 'occurs', 'managing', 'support', 'to', 'delegate', 'responsibility', 'effectively', 'and', 'within', 'legal', 'boundaries', 'to', 'ensure', 'through', 'clinical', 'standards', 'and', 'audits', 'that', 'good', 'practice', 'is', 'maintained', 'throughout', 'the', 'home', 'nursing', 'duties', 'to', 'perform', 'nursing', 'duties', 'that', 'reflect', 'current', 'best', 'practice', 'and', 'keep', 'abreast', 'of', 'latest', 'developments', 'to', 'inform', 'the', 'registered', 'manager', 'of', 'any', 'nursing', 'and', 'medical', 'matters', 'affecting', 'the', 'service', 'user', 's', 'care', 'to', 'assist', 'service', 'users', 'with', 'personal', 'care', 'which', 'includes', 'using', 'the', 'bath', 'bed', 'bath', 'or', 'shower', 'feet', 'and', 'nail', 'care', 'hair', 'care', 'shaving', 'mouth', 'care', 'denture', 'care', 'toileting', 'needs', 'all', 'daily', 'activities', 'as', 'per', 'individual', 'care', 'plans', 'including', 'tidying', 'bed', 'and', 'room', 'to', 'assist', 'clients', 'with', 'their', 'psychological', 'needs', 'which', 'includes', 'talking', 'listening', 'excursions', 'liaising', 'with', 'family', 'assisting', 'with', 'hobbies', 'and', 'recreation', 'activities', 'to', 'care', 'for', 'the', 'service', 'user', 'as', 'an', 'individual', 'and', 'maintain', 'a', 'high', 'level', 'of', 'care', 'for', 'their', 'particular', 'condition', 'ensuring', 'physical', 'needs', 'comfort', 'and', 'dignity', 'are', 'met', 'to', 'care', 'for', 'service', 'user', 's', 'at', 'their', 'end', 'of', 'life', 'in', 'a', 'respectful', 'and', 'dignified', 'manner', 'to', 'administer', 'medication', 'strictly', 'in', 'accordance', 'with', 'the', 'drug', 'administration', 'policy', 'and', 'nmc', 'guidelines', 'to', 'assist', 'the', 'registered', 'manager', 'in', 'ensuring', 'adequate', 'supplies', 'of', 'medication', 'and', 'correct', 'storage', 'as', 'per', 'the', 'medication', 'policy', 'to', 'assist', 'with', 'serving', 'meals', 'and', 'other', 'domestic', 'duties', 'such', 'as', 'laundry', 'occasional', 'cleaning', 'etc', 'to', 'deal', 'with', 'any', 'internal', 'or', 'external', 'communications', 'from', 'service', 'users', 'or', 'third', 'parties', 'to', 'ensure', 'all', 'documentation', 'is', 'kept', 'up', 'to', 'date', 'and', 'recorded', 'accurately', 'to', 'maintain', 'records', 'care', 'plans', 'as', 'required', 'by', 'the', 'care', 'quality', 'commission', 'business', 'management', 'to', 'work', 'both', 'with', 'the', 'directors', 'and', 'independently', 'to', 'generate', 'enquiries', 'and', 'proactively', 'deal', 'with', 'members', 'of', 'the', 'public', 'and', 'care', 'medical', 'professionals', 'to', 'ensure', 'a', 'high', 'level', 'of', 'occupancy', 'rates', 'to', 'manage', 'and', 'have', 'a', 'working', 'knowledge', 'of', 'the', 'home', 's', 'policies', 'and', 'procedures', 'and', 'ensure', 'they', 'are', 'maintained', 'at', 'all', 'times', 'to', 'promote', 'and', 'act', 'strictly', 'in', 'accordance', 'with', 'the', 'home', 's', 'health', 'and', 'safety', 'policy', 'and', 'ensure', 'good', 'health', 'and', 'safety', 'practice', 'within', 'the', 'working', 'environment', 'to', 'comply', 'with', 'coshh', 'riddor', 'environmental', 'health', 'fire', 'regulations', 'and', 'the', 'care', 'standards', 'act', 'to', 'bring', 'to', 'the', 'directors', 'immediate', 'attention', 'any', 'item', 'of', 'health', 'and', 'safety', 'importance', 'that', 'would', 'be', 'of', 'concern', 'and', 'potentially', 'needing', 'prompt', 'action', 'to', 'have', 'an', 'in', 'depth', 'operational', 'knowledge', 'of', 'all', 'emergency', 'procedures', 'and', 'ensure', 'they', 'are', 'cascaded', 'down', 'through', 'all', 'the', 'staff', 'and', 'service', 'users', 'to', 'attend', 'staff', 'meetings', 'as', 'arranged', 'and', 'to', 'ensure', 'other', 'staff', 'informed', 'of', 'items', 'discussed', 'and', 'decisions', 'made', 'to', 'attend', 'at', 'least', 'hours', 'per', 'annum', 'of', 'compulsory', 'training', 'outlined', 'by', 'your', 'personal', 'development', 'plan', 'to', 'participate', 'in', 'regular', 'reviews', 'and', 'an', 'annual', 'appraisal', 'in', 'order', 'to', 'develop', 'your', 'training', 'needs', 'in', 'relation', 'to', 'skills', 'and', 'knowledge', 'to', 'promote', 'the', 'good', 'name', 'of', 'the', 'home', 'at', 'all', 'times', 'to', 'cover', 'colleagues', 'duties', 'in', 'times', 'of', 'sickness', 'and', 'holidays', 'to', 'develop', 'and', 'share', 'ideas', 'for', 'the', 'improvement', 'of', 'practice', 'within', 'the', 'home', 'to', 'implement', 'change', 'sensitively', 'but', 'effectively', 'to', 'replenish', 'stocks', 'and', 'exercise', 'cost', 'stock', 'control', 'to', 'operate', 'within', 'the', 'guidelines', 'of', 'the', 'data', 'protection', 'act', 'and', 'ensure', 'total', 'confidentiality', 'especially', 'with', 'regards', 'to', 'staff', 'and', 'service', 'users', 'managing', 'people', 'to', 'assist', 'and', 'supervise', 'staff', 'training', 'to', 'be', 'responsible', 'for', 'covering', 'staff', 'absences', 'and', 'managing', 'disputes', 'where', 'appropriate', 'to', 'ensure', 'all', 'staff', 'contribute', 'to', 'the', 'best', 'of', 'their', 'ability', 'to', 'the', 'efficient', 'running', 'of', 'the', 'home', 'and', 'that', 'high', 'standards', 'are', 'maintained', 'to', 'ensure', 'staff', 'have', 'the', 'qualifications', 'and', 'training', 'necessary', 'for', 'the', 'duties', 'they', 'perform', 'to', 'ensure', 'all', 'staff', 'have', 'two', 'monthly', 'reviews', 'and', 'an', 'annual', 'appraisal', 'as', 'per', 'company', 'policy', 'further', 'duties', 'in', 'addition', 'to', 'the', 'responsibilities', 'listed', 'above', 'it', 'may', 'be', 'necessary', 'to', 'perform', 'other', 'duties', 'as', 'required', 'by', 'the', 'home', 's', 'management', 'or', 'senior', 'staff', 'consideration', 'will', 'be', 'given', 'to', 'your', 'skills', 'and', 'status', 'when', 'given', 'these', 'duties', 'note', 'it', 'is', 'expected', 'that', 'all', 'duties', 'carried', 'out', 'will', 'be', 'performed', 'in', 'a', 'spirit', 'of', 'cooperation', 'required', 'from', 'a', 'dedicated', 'efficient', 'team', 'whose', 'prime', 'aim', 'is', 'to', 'make', 'the', 'service', 'user', 's', 'stay', 'as', 'comfortable', 'and', 'enjoyable', 'as', 'possible'] \n",
      "\n",
      "\n",
      "The original number of Tokenized description tokens:  776\n"
     ]
    }
   ],
   "source": [
    "def tokenizeDescription(raw_description):\n",
    "    \"\"\"\n",
    "        This function first convert all words to lowercases,\n",
    "        it then segment the raw description into sentences and tokenize each sentences\n",
    "        and convert the description to a list of tokens.\n",
    "    \"\"\"\n",
    "    description = raw_description.lower() # convert all words to lowercase\n",
    "\n",
    "    # segment into sentences\n",
    "    sentences = sent_tokenize(description)\n",
    "\n",
    "    # tokenize each sentence\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "\n",
    "    # merge them into a list of tokens\n",
    "    tokenised_description = list(chain.from_iterable(token_lists))\n",
    "    return tokenised_description\n",
    "\n",
    "tk_description = [tokenizeDescription(r) for r in description]  # list comprehension, generate a list of tokenized articles\n",
    "\n",
    "print(\"Raw description:\\n\",description[test_index],'\\n')\n",
    "print(\"Tokenized description:\\n\",tk_description[test_index],'\\n\\n')\n",
    "print(\"The original number of Tokenized description tokens: \",len(tk_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### A Few Statistics Before Any Further Pre-processing\n",
    "\n",
    "In the following, we are interested to know a few statistics at this very begining stage, including:\n",
    "* The total number of tokens across the corpus\n",
    "* The total number of types across the corpus, i.e. the size of vocabulary \n",
    "* The so-called, [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity), referring to the ratio of different unique word stems (types) to the total number of words (tokens).  \n",
    "* The average, minimum and maximum number of token (i.e. document length) in the dataset.\n",
    "\n",
    "In the following, we wrap all these up as a function, since we will use this printing module later to compare these statistic values before and after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9834\n",
      "Total number of tokens:  186952\n",
      "Lexical diversity:  0.052601737344345076\n",
      "Total number of description: 776\n",
      "Average description length: 240.91752577319588\n",
      "Maximum description length: 815\n",
      "Minimum description length: 13\n",
      "Standard deviation of description length: 124.97750685071483\n"
     ]
    }
   ],
   "source": [
    "def stats_print(tk_description):\n",
    "    words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of description:\", len(tk_description))\n",
    "    lens = [len(article) for article in tk_description]\n",
    "    print(\"Average description length:\", np.mean(lens))\n",
    "    print(\"Maximum description length:\", np.max(lens))\n",
    "    print(\"Minimum description length:\", np.min(lens))\n",
    "    print(\"Standard deviation of description length:\", np.std(lens))\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.4 Remove words with length less than 2.\n",
    "remove any token that only contains a single character (a token that of length less than 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing, the number of words that appear with length less than 2: 4233\n",
      "After removing, the number of words that appear less than 2: 4231\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"Before removing, the number of words that appear with length less than 2:\", len([w for w in word_counts if word_counts[w] < 2]))\n",
    "\n",
    "# filter out single character tokens\n",
    "tk_description = [[w for w in description if len(w) >=2] \\\n",
    "                      for description in tk_description]\n",
    "\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"After removing, the number of words that appear less than 2:\", len([w for w in word_counts if word_counts[w] < 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.5 Remove stopwords using the provided stop words list\n",
    "> **NOTE**: as mentioned before, the purpose of this task is to pre-process the text reviews and later on we are going to use the pre-process text to build a sentiment analysis model. The stop word removal process requires careful consideration in this type of task.\n",
    "\n",
    "Remove the stop words from the tokenized text inside `stopwords_en.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stop words inside stopwords_en.txt is 571 including:\n",
      "\n",
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n"
     ]
    }
   ],
   "source": [
    "# remove the stop words inside `stopwords_en.txt` from the tokenized text\n",
    "stopwords_file = 'stopwords_en.txt'\n",
    "\n",
    "# read the stop words into a list\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    stop_words = f.read().splitlines() \n",
    "print(f'The number of stop words inside {stopwords_file} is {len(stop_words)} including:\\n\\n{stop_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### -----------> OBSERVATION:\n",
    "+ There 571 stopwords in total, which are often function words in English, like articles (e.g. \"the\", and \"an\"), pronouns (e.g. \"he\", \"him\", and \"they\"), particles (e.g., \"well\", \"however\" and \"thus\"), etc, and universal words in all job advertisement (e.g.'ask', 'asking', 'used', and 'useful')\n",
    "\n",
    "+ Note this this is just one of the lists and we test_indexhasize that there is no universal list of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['j', 'o', 'b'],\n",
       " ['t', 'i', 't', 'l', 'e'],\n",
       " ['u', 'n', 'i', 't'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'r'],\n",
       " ['r', 'e', 'p', 'o', 'r', 't', 'i', 'n', 'g'],\n",
       " ['t', 'o'],\n",
       " ['r', 'e', 'g', 'i', 's', 't', 'e', 'r', 'e', 'd'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'r'],\n",
       " ['j', 'o', 'b'],\n",
       " ['p', 'u', 'r', 'p', 'o', 's', 'e'],\n",
       " ['t', 'o'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e'],\n",
       " ['i', 'n'],\n",
       " ['p', 'r', 'o', 'f', 'e', 's', 's', 'i', 'o', 'n', 'a', 'l'],\n",
       " ['m', 'a', 'n', 'n', 'e', 'r'],\n",
       " ['t', 'h', 'e'],\n",
       " ['d', 'a', 'y'],\n",
       " ['t', 'o'],\n",
       " ['d', 'a', 'y'],\n",
       " ['r', 'u', 'n', 'n', 'i', 'n', 'g'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n'],\n",
       " ['c', 'l', 'i', 'n', 'i', 'c', 'a', 'l'],\n",
       " ['p', 'o', 'l', 'i', 'c', 'i', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['p', 'r', 'o', 'c', 'e', 'd', 'u', 'r', 'e', 's'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g'],\n",
       " ['a', 'n', 'd'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['p', 'l', 'a', 'n', 'n', 'i', 'n', 'g'],\n",
       " ['t', 'o'],\n",
       " ['i', 'm', 'p', 'l', 'e', 'm', 'e', 'n', 't'],\n",
       " ['w', 'o', 'r', 'k', 'i', 'n', 'g'],\n",
       " ['p', 'r', 'a', 'c', 't', 'i', 'c', 'e', 's'],\n",
       " ['t', 'h', 'a', 't'],\n",
       " ['m', 'o', 'n', 'i', 't', 'o', 'r', 's'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'e', 'a', 'l', 't', 'h'],\n",
       " ['a', 'n', 'd'],\n",
       " ['w', 'e', 'l', 'f', 'a', 'r', 'e'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['a', 'n', 'd'],\n",
       " ['t', 'h', 'e', 'i', 'r'],\n",
       " ['r', 'e', 's', 'p', 'e', 'c', 't', 'i', 'v', 'e'],\n",
       " ['e', 'n', 'v', 'i', 'r', 'o', 'n', 'm', 'e', 'n', 't', 's'],\n",
       " ['t', 'o'],\n",
       " ['p', 'r', 'o', 'm', 'o', 't', 'e'],\n",
       " ['q', 'u', 'a', 'l', 'i', 't', 'y'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['w', 'i', 't', 'h', 'i', 'n'],\n",
       " ['w', 'a', 'r', 'm'],\n",
       " ['f', 'r', 'i', 'e', 'n', 'd', 'l', 'y'],\n",
       " ['a', 'm', 'b', 'i', 'e', 'n', 'c', 'e'],\n",
       " ['k', 'e', 'y'],\n",
       " ['r', 'e', 's', 'u', 'l', 't'],\n",
       " ['a', 'r', 'e', 'a', 's'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'i', 'n', 'g'],\n",
       " ['t', 'o'],\n",
       " ['w', 'o', 'r', 'k'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['t', 'h', 'e'],\n",
       " ['d', 'i', 'r', 'e', 'c', 't', 'o', 'r', 's'],\n",
       " ['t', 'o'],\n",
       " ['a', 'c', 'h', 'i', 'e', 'v', 'e'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['f', 'i', 'n', 'a', 'n', 'c', 'i', 'a', 'l'],\n",
       " ['t', 'a', 'r', 'g', 'e', 't', 's'],\n",
       " ['t', 'o'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['i', 'n'],\n",
       " ['m', 'a', 'n', 'n', 'e', 'r'],\n",
       " ['w', 'h', 'i', 'c', 'h'],\n",
       " ['w', 'i', 'l', 'l'],\n",
       " ['n', 'o', 't'],\n",
       " ['b', 'r', 'i', 'n', 'g'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['o', 'r'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r', 's'],\n",
       " ['i', 'n', 't', 'o'],\n",
       " ['d', 'i', 's', 'r', 'e', 'p', 'u', 't', 'e'],\n",
       " ['t', 'o'],\n",
       " ['m', 'a', 'i', 'n', 't', 'a', 'i', 'n'],\n",
       " ['c', 'o', 'n', 'f', 'i', 'd', 'e', 'n', 't', 'i', 'a', 'l', 'i', 't', 'y'],\n",
       " ['o', 'n'],\n",
       " ['a', 'l', 'l'],\n",
       " ['a', 's', 'p', 'e', 'c', 't', 's'],\n",
       " ['o', 'f'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'm', 'e', 'n', 't'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['a', 'l', 'l'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['p', 'o', 'l', 'i', 'c', 'i', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['p', 'r', 'o', 'c', 'e', 'd', 'u', 'r', 'e', 's'],\n",
       " ['a', 'r', 'e'],\n",
       " ['i', 'm', 'p', 'l', 'e', 'm', 'e', 'n', 't', 'e', 'd'],\n",
       " ['a', 'n', 'd'],\n",
       " ['f', 'o', 'l', 'l', 'o', 'w', 'e', 'd'],\n",
       " ['b', 'y'],\n",
       " ['a', 'l', 'l'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['t', 'o'],\n",
       " ['i', 'n', 'f', 'o', 'r', 'm'],\n",
       " ['t', 'h', 'e'],\n",
       " ['r', 'e', 'g', 'i', 's', 't', 'e', 'r', 'e', 'd'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'r'],\n",
       " ['i', 'm', 'm', 'e', 'd', 'i', 'a', 't', 'e', 'l', 'y'],\n",
       " ['i', 'f'],\n",
       " ['s', 'e', 'r', 'i', 'o', 'u', 's'],\n",
       " ['d', 'i', 'f', 'f', 'i', 'c', 'u', 'l', 't', 'y'],\n",
       " ['o', 'r'],\n",
       " ['e', 'v', 'e', 'n', 't'],\n",
       " ['o', 'c', 'c', 'u', 'r', 's'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'i', 'n', 'g'],\n",
       " ['s', 'u', 'p', 'p', 'o', 'r', 't'],\n",
       " ['t', 'o'],\n",
       " ['d', 'e', 'l', 'e', 'g', 'a', 't', 'e'],\n",
       " ['r', 'e', 's', 'p', 'o', 'n', 's', 'i', 'b', 'i', 'l', 'i', 't', 'y'],\n",
       " ['e', 'f', 'f', 'e', 'c', 't', 'i', 'v', 'e', 'l', 'y'],\n",
       " ['a', 'n', 'd'],\n",
       " ['w', 'i', 't', 'h', 'i', 'n'],\n",
       " ['l', 'e', 'g', 'a', 'l'],\n",
       " ['b', 'o', 'u', 'n', 'd', 'a', 'r', 'i', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['t', 'h', 'r', 'o', 'u', 'g', 'h'],\n",
       " ['c', 'l', 'i', 'n', 'i', 'c', 'a', 'l'],\n",
       " ['s', 't', 'a', 'n', 'd', 'a', 'r', 'd', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['a', 'u', 'd', 'i', 't', 's'],\n",
       " ['t', 'h', 'a', 't'],\n",
       " ['g', 'o', 'o', 'd'],\n",
       " ['p', 'r', 'a', 'c', 't', 'i', 'c', 'e'],\n",
       " ['i', 's'],\n",
       " ['m', 'a', 'i', 'n', 't', 'a', 'i', 'n', 'e', 'd'],\n",
       " ['t', 'h', 'r', 'o', 'u', 'g', 'h', 'o', 'u', 't'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['n', 'u', 'r', 's', 'i', 'n', 'g'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['p', 'e', 'r', 'f', 'o', 'r', 'm'],\n",
       " ['n', 'u', 'r', 's', 'i', 'n', 'g'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['t', 'h', 'a', 't'],\n",
       " ['r', 'e', 'f', 'l', 'e', 'c', 't'],\n",
       " ['c', 'u', 'r', 'r', 'e', 'n', 't'],\n",
       " ['b', 'e', 's', 't'],\n",
       " ['p', 'r', 'a', 'c', 't', 'i', 'c', 'e'],\n",
       " ['a', 'n', 'd'],\n",
       " ['k', 'e', 'e', 'p'],\n",
       " ['a', 'b', 'r', 'e', 'a', 's', 't'],\n",
       " ['o', 'f'],\n",
       " ['l', 'a', 't', 'e', 's', 't'],\n",
       " ['d', 'e', 'v', 'e', 'l', 'o', 'p', 'm', 'e', 'n', 't', 's'],\n",
       " ['t', 'o'],\n",
       " ['i', 'n', 'f', 'o', 'r', 'm'],\n",
       " ['t', 'h', 'e'],\n",
       " ['r', 'e', 'g', 'i', 's', 't', 'e', 'r', 'e', 'd'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'r'],\n",
       " ['o', 'f'],\n",
       " ['a', 'n', 'y'],\n",
       " ['n', 'u', 'r', 's', 'i', 'n', 'g'],\n",
       " ['a', 'n', 'd'],\n",
       " ['m', 'e', 'd', 'i', 'c', 'a', 'l'],\n",
       " ['m', 'a', 't', 't', 'e', 'r', 's'],\n",
       " ['a', 'f', 'f', 'e', 'c', 't', 'i', 'n', 'g'],\n",
       " ['t', 'h', 'e'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['t', 'o'],\n",
       " ['a', 's', 's', 'i', 's', 't'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r', 's'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['p', 'e', 'r', 's', 'o', 'n', 'a', 'l'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['w', 'h', 'i', 'c', 'h'],\n",
       " ['i', 'n', 'c', 'l', 'u', 'd', 'e', 's'],\n",
       " ['u', 's', 'i', 'n', 'g'],\n",
       " ['t', 'h', 'e'],\n",
       " ['b', 'a', 't', 'h'],\n",
       " ['b', 'e', 'd'],\n",
       " ['b', 'a', 't', 'h'],\n",
       " ['o', 'r'],\n",
       " ['s', 'h', 'o', 'w', 'e', 'r'],\n",
       " ['f', 'e', 'e', 't'],\n",
       " ['a', 'n', 'd'],\n",
       " ['n', 'a', 'i', 'l'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['h', 'a', 'i', 'r'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['s', 'h', 'a', 'v', 'i', 'n', 'g'],\n",
       " ['m', 'o', 'u', 't', 'h'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['d', 'e', 'n', 't', 'u', 'r', 'e'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['t', 'o', 'i', 'l', 'e', 't', 'i', 'n', 'g'],\n",
       " ['n', 'e', 'e', 'd', 's'],\n",
       " ['a', 'l', 'l'],\n",
       " ['d', 'a', 'i', 'l', 'y'],\n",
       " ['a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's'],\n",
       " ['a', 's'],\n",
       " ['p', 'e', 'r'],\n",
       " ['i', 'n', 'd', 'i', 'v', 'i', 'd', 'u', 'a', 'l'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['p', 'l', 'a', 'n', 's'],\n",
       " ['i', 'n', 'c', 'l', 'u', 'd', 'i', 'n', 'g'],\n",
       " ['t', 'i', 'd', 'y', 'i', 'n', 'g'],\n",
       " ['b', 'e', 'd'],\n",
       " ['a', 'n', 'd'],\n",
       " ['r', 'o', 'o', 'm'],\n",
       " ['t', 'o'],\n",
       " ['a', 's', 's', 'i', 's', 't'],\n",
       " ['c', 'l', 'i', 'e', 'n', 't', 's'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['t', 'h', 'e', 'i', 'r'],\n",
       " ['p', 's', 'y', 'c', 'h', 'o', 'l', 'o', 'g', 'i', 'c', 'a', 'l'],\n",
       " ['n', 'e', 'e', 'd', 's'],\n",
       " ['w', 'h', 'i', 'c', 'h'],\n",
       " ['i', 'n', 'c', 'l', 'u', 'd', 'e', 's'],\n",
       " ['t', 'a', 'l', 'k', 'i', 'n', 'g'],\n",
       " ['l', 'i', 's', 't', 'e', 'n', 'i', 'n', 'g'],\n",
       " ['e', 'x', 'c', 'u', 'r', 's', 'i', 'o', 'n', 's'],\n",
       " ['l', 'i', 'a', 'i', 's', 'i', 'n', 'g'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['f', 'a', 'm', 'i', 'l', 'y'],\n",
       " ['a', 's', 's', 'i', 's', 't', 'i', 'n', 'g'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['h', 'o', 'b', 'b', 'i', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['r', 'e', 'c', 'r', 'e', 'a', 't', 'i', 'o', 'n'],\n",
       " ['a', 'c', 't', 'i', 'v', 'i', 't', 'i', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['f', 'o', 'r'],\n",
       " ['t', 'h', 'e'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r'],\n",
       " ['a', 's'],\n",
       " ['a', 'n'],\n",
       " ['i', 'n', 'd', 'i', 'v', 'i', 'd', 'u', 'a', 'l'],\n",
       " ['a', 'n', 'd'],\n",
       " ['m', 'a', 'i', 'n', 't', 'a', 'i', 'n'],\n",
       " ['h', 'i', 'g', 'h'],\n",
       " ['l', 'e', 'v', 'e', 'l'],\n",
       " ['o', 'f'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['f', 'o', 'r'],\n",
       " ['t', 'h', 'e', 'i', 'r'],\n",
       " ['p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r'],\n",
       " ['c', 'o', 'n', 'd', 'i', 't', 'i', 'o', 'n'],\n",
       " ['e', 'n', 's', 'u', 'r', 'i', 'n', 'g'],\n",
       " ['p', 'h', 'y', 's', 'i', 'c', 'a', 'l'],\n",
       " ['n', 'e', 'e', 'd', 's'],\n",
       " ['c', 'o', 'm', 'f', 'o', 'r', 't'],\n",
       " ['a', 'n', 'd'],\n",
       " ['d', 'i', 'g', 'n', 'i', 't', 'y'],\n",
       " ['a', 'r', 'e'],\n",
       " ['m', 'e', 't'],\n",
       " ['t', 'o'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['f', 'o', 'r'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r'],\n",
       " ['a', 't'],\n",
       " ['t', 'h', 'e', 'i', 'r'],\n",
       " ['e', 'n', 'd'],\n",
       " ['o', 'f'],\n",
       " ['l', 'i', 'f', 'e'],\n",
       " ['i', 'n'],\n",
       " ['r', 'e', 's', 'p', 'e', 'c', 't', 'f', 'u', 'l'],\n",
       " ['a', 'n', 'd'],\n",
       " ['d', 'i', 'g', 'n', 'i', 'f', 'i', 'e', 'd'],\n",
       " ['m', 'a', 'n', 'n', 'e', 'r'],\n",
       " ['t', 'o'],\n",
       " ['a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'e', 'r'],\n",
       " ['m', 'e', 'd', 'i', 'c', 'a', 't', 'i', 'o', 'n'],\n",
       " ['s', 't', 'r', 'i', 'c', 't', 'l', 'y'],\n",
       " ['i', 'n'],\n",
       " ['a', 'c', 'c', 'o', 'r', 'd', 'a', 'n', 'c', 'e'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['t', 'h', 'e'],\n",
       " ['d', 'r', 'u', 'g'],\n",
       " ['a', 'd', 'm', 'i', 'n', 'i', 's', 't', 'r', 'a', 't', 'i', 'o', 'n'],\n",
       " ['p', 'o', 'l', 'i', 'c', 'y'],\n",
       " ['a', 'n', 'd'],\n",
       " ['n', 'm', 'c'],\n",
       " ['g', 'u', 'i', 'd', 'e', 'l', 'i', 'n', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['a', 's', 's', 'i', 's', 't'],\n",
       " ['t', 'h', 'e'],\n",
       " ['r', 'e', 'g', 'i', 's', 't', 'e', 'r', 'e', 'd'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'r'],\n",
       " ['i', 'n'],\n",
       " ['e', 'n', 's', 'u', 'r', 'i', 'n', 'g'],\n",
       " ['a', 'd', 'e', 'q', 'u', 'a', 't', 'e'],\n",
       " ['s', 'u', 'p', 'p', 'l', 'i', 'e', 's'],\n",
       " ['o', 'f'],\n",
       " ['m', 'e', 'd', 'i', 'c', 'a', 't', 'i', 'o', 'n'],\n",
       " ['a', 'n', 'd'],\n",
       " ['c', 'o', 'r', 'r', 'e', 'c', 't'],\n",
       " ['s', 't', 'o', 'r', 'a', 'g', 'e'],\n",
       " ['a', 's'],\n",
       " ['p', 'e', 'r'],\n",
       " ['t', 'h', 'e'],\n",
       " ['m', 'e', 'd', 'i', 'c', 'a', 't', 'i', 'o', 'n'],\n",
       " ['p', 'o', 'l', 'i', 'c', 'y'],\n",
       " ['t', 'o'],\n",
       " ['a', 's', 's', 'i', 's', 't'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'n', 'g'],\n",
       " ['m', 'e', 'a', 'l', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['o', 't', 'h', 'e', 'r'],\n",
       " ['d', 'o', 'm', 'e', 's', 't', 'i', 'c'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['s', 'u', 'c', 'h'],\n",
       " ['a', 's'],\n",
       " ['l', 'a', 'u', 'n', 'd', 'r', 'y'],\n",
       " ['o', 'c', 'c', 'a', 's', 'i', 'o', 'n', 'a', 'l'],\n",
       " ['c', 'l', 'e', 'a', 'n', 'i', 'n', 'g'],\n",
       " ['e', 't', 'c'],\n",
       " ['t', 'o'],\n",
       " ['d', 'e', 'a', 'l'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['a', 'n', 'y'],\n",
       " ['i', 'n', 't', 'e', 'r', 'n', 'a', 'l'],\n",
       " ['o', 'r'],\n",
       " ['e', 'x', 't', 'e', 'r', 'n', 'a', 'l'],\n",
       " ['c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', 's'],\n",
       " ['f', 'r', 'o', 'm'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r', 's'],\n",
       " ['o', 'r'],\n",
       " ['t', 'h', 'i', 'r', 'd'],\n",
       " ['p', 'a', 'r', 't', 'i', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['a', 'l', 'l'],\n",
       " ['d', 'o', 'c', 'u', 'm', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n'],\n",
       " ['i', 's'],\n",
       " ['k', 'e', 'p', 't'],\n",
       " ['u', 'p'],\n",
       " ['t', 'o'],\n",
       " ['d', 'a', 't', 'e'],\n",
       " ['a', 'n', 'd'],\n",
       " ['r', 'e', 'c', 'o', 'r', 'd', 'e', 'd'],\n",
       " ['a', 'c', 'c', 'u', 'r', 'a', 't', 'e', 'l', 'y'],\n",
       " ['t', 'o'],\n",
       " ['m', 'a', 'i', 'n', 't', 'a', 'i', 'n'],\n",
       " ['r', 'e', 'c', 'o', 'r', 'd', 's'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['p', 'l', 'a', 'n', 's'],\n",
       " ['a', 's'],\n",
       " ['r', 'e', 'q', 'u', 'i', 'r', 'e', 'd'],\n",
       " ['b', 'y'],\n",
       " ['t', 'h', 'e'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['q', 'u', 'a', 'l', 'i', 't', 'y'],\n",
       " ['c', 'o', 'm', 'm', 'i', 's', 's', 'i', 'o', 'n'],\n",
       " ['b', 'u', 's', 'i', 'n', 'e', 's', 's'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'm', 'e', 'n', 't'],\n",
       " ['t', 'o'],\n",
       " ['w', 'o', 'r', 'k'],\n",
       " ['b', 'o', 't', 'h'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['t', 'h', 'e'],\n",
       " ['d', 'i', 'r', 'e', 'c', 't', 'o', 'r', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['i', 'n', 'd', 'e', 'p', 'e', 'n', 'd', 'e', 'n', 't', 'l', 'y'],\n",
       " ['t', 'o'],\n",
       " ['g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " ['e', 'n', 'q', 'u', 'i', 'r', 'i', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['p', 'r', 'o', 'a', 'c', 't', 'i', 'v', 'e', 'l', 'y'],\n",
       " ['d', 'e', 'a', 'l'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['m', 'e', 'm', 'b', 'e', 'r', 's'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e'],\n",
       " ['p', 'u', 'b', 'l', 'i', 'c'],\n",
       " ['a', 'n', 'd'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['m', 'e', 'd', 'i', 'c', 'a', 'l'],\n",
       " ['p', 'r', 'o', 'f', 'e', 's', 's', 'i', 'o', 'n', 'a', 'l', 's'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['h', 'i', 'g', 'h'],\n",
       " ['l', 'e', 'v', 'e', 'l'],\n",
       " ['o', 'f'],\n",
       " ['o', 'c', 'c', 'u', 'p', 'a', 'n', 'c', 'y'],\n",
       " ['r', 'a', 't', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e'],\n",
       " ['a', 'n', 'd'],\n",
       " ['h', 'a', 'v', 'e'],\n",
       " ['w', 'o', 'r', 'k', 'i', 'n', 'g'],\n",
       " ['k', 'n', 'o', 'w', 'l', 'e', 'd', 'g', 'e'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['p', 'o', 'l', 'i', 'c', 'i', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['p', 'r', 'o', 'c', 'e', 'd', 'u', 'r', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['t', 'h', 'e', 'y'],\n",
       " ['a', 'r', 'e'],\n",
       " ['m', 'a', 'i', 'n', 't', 'a', 'i', 'n', 'e', 'd'],\n",
       " ['a', 't'],\n",
       " ['a', 'l', 'l'],\n",
       " ['t', 'i', 'm', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['p', 'r', 'o', 'm', 'o', 't', 'e'],\n",
       " ['a', 'n', 'd'],\n",
       " ['a', 'c', 't'],\n",
       " ['s', 't', 'r', 'i', 'c', 't', 'l', 'y'],\n",
       " ['i', 'n'],\n",
       " ['a', 'c', 'c', 'o', 'r', 'd', 'a', 'n', 'c', 'e'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['h', 'e', 'a', 'l', 't', 'h'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 'a', 'f', 'e', 't', 'y'],\n",
       " ['p', 'o', 'l', 'i', 'c', 'y'],\n",
       " ['a', 'n', 'd'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['g', 'o', 'o', 'd'],\n",
       " ['h', 'e', 'a', 'l', 't', 'h'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 'a', 'f', 'e', 't', 'y'],\n",
       " ['p', 'r', 'a', 'c', 't', 'i', 'c', 'e'],\n",
       " ['w', 'i', 't', 'h', 'i', 'n'],\n",
       " ['t', 'h', 'e'],\n",
       " ['w', 'o', 'r', 'k', 'i', 'n', 'g'],\n",
       " ['e', 'n', 'v', 'i', 'r', 'o', 'n', 'm', 'e', 'n', 't'],\n",
       " ['t', 'o'],\n",
       " ['c', 'o', 'm', 'p', 'l', 'y'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['c', 'o', 's', 'h', 'h'],\n",
       " ['r', 'i', 'd', 'd', 'o', 'r'],\n",
       " ['e', 'n', 'v', 'i', 'r', 'o', 'n', 'm', 'e', 'n', 't', 'a', 'l'],\n",
       " ['h', 'e', 'a', 'l', 't', 'h'],\n",
       " ['f', 'i', 'r', 'e'],\n",
       " ['r', 'e', 'g', 'u', 'l', 'a', 't', 'i', 'o', 'n', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['t', 'h', 'e'],\n",
       " ['c', 'a', 'r', 'e'],\n",
       " ['s', 't', 'a', 'n', 'd', 'a', 'r', 'd', 's'],\n",
       " ['a', 'c', 't'],\n",
       " ['t', 'o'],\n",
       " ['b', 'r', 'i', 'n', 'g'],\n",
       " ['t', 'o'],\n",
       " ['t', 'h', 'e'],\n",
       " ['d', 'i', 'r', 'e', 'c', 't', 'o', 'r', 's'],\n",
       " ['i', 'm', 'm', 'e', 'd', 'i', 'a', 't', 'e'],\n",
       " ['a', 't', 't', 'e', 'n', 't', 'i', 'o', 'n'],\n",
       " ['a', 'n', 'y'],\n",
       " ['i', 't', 'e', 'm'],\n",
       " ['o', 'f'],\n",
       " ['h', 'e', 'a', 'l', 't', 'h'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 'a', 'f', 'e', 't', 'y'],\n",
       " ['i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 'c', 'e'],\n",
       " ['t', 'h', 'a', 't'],\n",
       " ['w', 'o', 'u', 'l', 'd'],\n",
       " ['b', 'e'],\n",
       " ['o', 'f'],\n",
       " ['c', 'o', 'n', 'c', 'e', 'r', 'n'],\n",
       " ['a', 'n', 'd'],\n",
       " ['p', 'o', 't', 'e', 'n', 't', 'i', 'a', 'l', 'l', 'y'],\n",
       " ['n', 'e', 'e', 'd', 'i', 'n', 'g'],\n",
       " ['p', 'r', 'o', 'm', 'p', 't'],\n",
       " ['a', 'c', 't', 'i', 'o', 'n'],\n",
       " ['t', 'o'],\n",
       " ['h', 'a', 'v', 'e'],\n",
       " ['a', 'n'],\n",
       " ['i', 'n'],\n",
       " ['d', 'e', 'p', 't', 'h'],\n",
       " ['o', 'p', 'e', 'r', 'a', 't', 'i', 'o', 'n', 'a', 'l'],\n",
       " ['k', 'n', 'o', 'w', 'l', 'e', 'd', 'g', 'e'],\n",
       " ['o', 'f'],\n",
       " ['a', 'l', 'l'],\n",
       " ['e', 'm', 'e', 'r', 'g', 'e', 'n', 'c', 'y'],\n",
       " ['p', 'r', 'o', 'c', 'e', 'd', 'u', 'r', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['t', 'h', 'e', 'y'],\n",
       " ['a', 'r', 'e'],\n",
       " ['c', 'a', 's', 'c', 'a', 'd', 'e', 'd'],\n",
       " ['d', 'o', 'w', 'n'],\n",
       " ['t', 'h', 'r', 'o', 'u', 'g', 'h'],\n",
       " ['a', 'l', 'l'],\n",
       " ['t', 'h', 'e'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r', 's'],\n",
       " ['t', 'o'],\n",
       " ['a', 't', 't', 'e', 'n', 'd'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['m', 'e', 'e', 't', 'i', 'n', 'g', 's'],\n",
       " ['a', 's'],\n",
       " ['a', 'r', 'r', 'a', 'n', 'g', 'e', 'd'],\n",
       " ['a', 'n', 'd'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['o', 't', 'h', 'e', 'r'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['i', 'n', 'f', 'o', 'r', 'm', 'e', 'd'],\n",
       " ['o', 'f'],\n",
       " ['i', 't', 'e', 'm', 's'],\n",
       " ['d', 'i', 's', 'c', 'u', 's', 's', 'e', 'd'],\n",
       " ['a', 'n', 'd'],\n",
       " ['d', 'e', 'c', 'i', 's', 'i', 'o', 'n', 's'],\n",
       " ['m', 'a', 'd', 'e'],\n",
       " ['t', 'o'],\n",
       " ['a', 't', 't', 'e', 'n', 'd'],\n",
       " ['a', 't'],\n",
       " ['l', 'e', 'a', 's', 't'],\n",
       " ['h', 'o', 'u', 'r', 's'],\n",
       " ['p', 'e', 'r'],\n",
       " ['a', 'n', 'n', 'u', 'm'],\n",
       " ['o', 'f'],\n",
       " ['c', 'o', 'm', 'p', 'u', 'l', 's', 'o', 'r', 'y'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g'],\n",
       " ['o', 'u', 't', 'l', 'i', 'n', 'e', 'd'],\n",
       " ['b', 'y'],\n",
       " ['y', 'o', 'u', 'r'],\n",
       " ['p', 'e', 'r', 's', 'o', 'n', 'a', 'l'],\n",
       " ['d', 'e', 'v', 'e', 'l', 'o', 'p', 'm', 'e', 'n', 't'],\n",
       " ['p', 'l', 'a', 'n'],\n",
       " ['t', 'o'],\n",
       " ['p', 'a', 'r', 't', 'i', 'c', 'i', 'p', 'a', 't', 'e'],\n",
       " ['i', 'n'],\n",
       " ['r', 'e', 'g', 'u', 'l', 'a', 'r'],\n",
       " ['r', 'e', 'v', 'i', 'e', 'w', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['a', 'n'],\n",
       " ['a', 'n', 'n', 'u', 'a', 'l'],\n",
       " ['a', 'p', 'p', 'r', 'a', 'i', 's', 'a', 'l'],\n",
       " ['i', 'n'],\n",
       " ['o', 'r', 'd', 'e', 'r'],\n",
       " ['t', 'o'],\n",
       " ['d', 'e', 'v', 'e', 'l', 'o', 'p'],\n",
       " ['y', 'o', 'u', 'r'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g'],\n",
       " ['n', 'e', 'e', 'd', 's'],\n",
       " ['i', 'n'],\n",
       " ['r', 'e', 'l', 'a', 't', 'i', 'o', 'n'],\n",
       " ['t', 'o'],\n",
       " ['s', 'k', 'i', 'l', 'l', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['k', 'n', 'o', 'w', 'l', 'e', 'd', 'g', 'e'],\n",
       " ['t', 'o'],\n",
       " ['p', 'r', 'o', 'm', 'o', 't', 'e'],\n",
       " ['t', 'h', 'e'],\n",
       " ['g', 'o', 'o', 'd'],\n",
       " ['n', 'a', 'm', 'e'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['a', 't'],\n",
       " ['a', 'l', 'l'],\n",
       " ['t', 'i', 'm', 'e', 's'],\n",
       " ['t', 'o'],\n",
       " ['c', 'o', 'v', 'e', 'r'],\n",
       " ['c', 'o', 'l', 'l', 'e', 'a', 'g', 'u', 'e', 's'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['i', 'n'],\n",
       " ['t', 'i', 'm', 'e', 's'],\n",
       " ['o', 'f'],\n",
       " ['s', 'i', 'c', 'k', 'n', 'e', 's', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['h', 'o', 'l', 'i', 'd', 'a', 'y', 's'],\n",
       " ['t', 'o'],\n",
       " ['d', 'e', 'v', 'e', 'l', 'o', 'p'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 'h', 'a', 'r', 'e'],\n",
       " ['i', 'd', 'e', 'a', 's'],\n",
       " ['f', 'o', 'r'],\n",
       " ['t', 'h', 'e'],\n",
       " ['i', 'm', 'p', 'r', 'o', 'v', 'e', 'm', 'e', 'n', 't'],\n",
       " ['o', 'f'],\n",
       " ['p', 'r', 'a', 'c', 't', 'i', 'c', 'e'],\n",
       " ['w', 'i', 't', 'h', 'i', 'n'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['t', 'o'],\n",
       " ['i', 'm', 'p', 'l', 'e', 'm', 'e', 'n', 't'],\n",
       " ['c', 'h', 'a', 'n', 'g', 'e'],\n",
       " ['s', 'e', 'n', 's', 'i', 't', 'i', 'v', 'e', 'l', 'y'],\n",
       " ['b', 'u', 't'],\n",
       " ['e', 'f', 'f', 'e', 'c', 't', 'i', 'v', 'e', 'l', 'y'],\n",
       " ['t', 'o'],\n",
       " ['r', 'e', 'p', 'l', 'e', 'n', 'i', 's', 'h'],\n",
       " ['s', 't', 'o', 'c', 'k', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['e', 'x', 'e', 'r', 'c', 'i', 's', 'e'],\n",
       " ['c', 'o', 's', 't'],\n",
       " ['s', 't', 'o', 'c', 'k'],\n",
       " ['c', 'o', 'n', 't', 'r', 'o', 'l'],\n",
       " ['t', 'o'],\n",
       " ['o', 'p', 'e', 'r', 'a', 't', 'e'],\n",
       " ['w', 'i', 't', 'h', 'i', 'n'],\n",
       " ['t', 'h', 'e'],\n",
       " ['g', 'u', 'i', 'd', 'e', 'l', 'i', 'n', 'e', 's'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e'],\n",
       " ['d', 'a', 't', 'a'],\n",
       " ['p', 'r', 'o', 't', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " ['a', 'c', 't'],\n",
       " ['a', 'n', 'd'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['t', 'o', 't', 'a', 'l'],\n",
       " ['c', 'o', 'n', 'f', 'i', 'd', 'e', 'n', 't', 'i', 'a', 'l', 'i', 't', 'y'],\n",
       " ['e', 's', 'p', 'e', 'c', 'i', 'a', 'l', 'l', 'y'],\n",
       " ['w', 'i', 't', 'h'],\n",
       " ['r', 'e', 'g', 'a', 'r', 'd', 's'],\n",
       " ['t', 'o'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r', 's'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'i', 'n', 'g'],\n",
       " ['p', 'e', 'o', 'p', 'l', 'e'],\n",
       " ['t', 'o'],\n",
       " ['a', 's', 's', 'i', 's', 't'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 'u', 'p', 'e', 'r', 'v', 'i', 's', 'e'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g'],\n",
       " ['t', 'o'],\n",
       " ['b', 'e'],\n",
       " ['r', 'e', 's', 'p', 'o', 'n', 's', 'i', 'b', 'l', 'e'],\n",
       " ['f', 'o', 'r'],\n",
       " ['c', 'o', 'v', 'e', 'r', 'i', 'n', 'g'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['a', 'b', 's', 'e', 'n', 'c', 'e', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'i', 'n', 'g'],\n",
       " ['d', 'i', 's', 'p', 'u', 't', 'e', 's'],\n",
       " ['w', 'h', 'e', 'r', 'e'],\n",
       " ['a', 'p', 'p', 'r', 'o', 'p', 'r', 'i', 'a', 't', 'e'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['a', 'l', 'l'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['c', 'o', 'n', 't', 'r', 'i', 'b', 'u', 't', 'e'],\n",
       " ['t', 'o'],\n",
       " ['t', 'h', 'e'],\n",
       " ['b', 'e', 's', 't'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e', 'i', 'r'],\n",
       " ['a', 'b', 'i', 'l', 'i', 't', 'y'],\n",
       " ['t', 'o'],\n",
       " ['t', 'h', 'e'],\n",
       " ['e', 'f', 'f', 'i', 'c', 'i', 'e', 'n', 't'],\n",
       " ['r', 'u', 'n', 'n', 'i', 'n', 'g'],\n",
       " ['o', 'f'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['a', 'n', 'd'],\n",
       " ['t', 'h', 'a', 't'],\n",
       " ['h', 'i', 'g', 'h'],\n",
       " ['s', 't', 'a', 'n', 'd', 'a', 'r', 'd', 's'],\n",
       " ['a', 'r', 'e'],\n",
       " ['m', 'a', 'i', 'n', 't', 'a', 'i', 'n', 'e', 'd'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['h', 'a', 'v', 'e'],\n",
       " ['t', 'h', 'e'],\n",
       " ['q', 'u', 'a', 'l', 'i', 'f', 'i', 'c', 'a', 't', 'i', 'o', 'n', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g'],\n",
       " ['n', 'e', 'c', 'e', 's', 's', 'a', 'r', 'y'],\n",
       " ['f', 'o', 'r'],\n",
       " ['t', 'h', 'e'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['t', 'h', 'e', 'y'],\n",
       " ['p', 'e', 'r', 'f', 'o', 'r', 'm'],\n",
       " ['t', 'o'],\n",
       " ['e', 'n', 's', 'u', 'r', 'e'],\n",
       " ['a', 'l', 'l'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['h', 'a', 'v', 'e'],\n",
       " ['t', 'w', 'o'],\n",
       " ['m', 'o', 'n', 't', 'h', 'l', 'y'],\n",
       " ['r', 'e', 'v', 'i', 'e', 'w', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['a', 'n'],\n",
       " ['a', 'n', 'n', 'u', 'a', 'l'],\n",
       " ['a', 'p', 'p', 'r', 'a', 'i', 's', 'a', 'l'],\n",
       " ['a', 's'],\n",
       " ['p', 'e', 'r'],\n",
       " ['c', 'o', 'm', 'p', 'a', 'n', 'y'],\n",
       " ['p', 'o', 'l', 'i', 'c', 'y'],\n",
       " ['f', 'u', 'r', 't', 'h', 'e', 'r'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['i', 'n'],\n",
       " ['a', 'd', 'd', 'i', 't', 'i', 'o', 'n'],\n",
       " ['t', 'o'],\n",
       " ['t', 'h', 'e'],\n",
       " ['r',\n",
       "  'e',\n",
       "  's',\n",
       "  'p',\n",
       "  'o',\n",
       "  'n',\n",
       "  's',\n",
       "  'i',\n",
       "  'b',\n",
       "  'i',\n",
       "  'l',\n",
       "  'i',\n",
       "  't',\n",
       "  'i',\n",
       "  'e',\n",
       "  's'],\n",
       " ['l', 'i', 's', 't', 'e', 'd'],\n",
       " ['a', 'b', 'o', 'v', 'e'],\n",
       " ['i', 't'],\n",
       " ['m', 'a', 'y'],\n",
       " ['b', 'e'],\n",
       " ['n', 'e', 'c', 'e', 's', 's', 'a', 'r', 'y'],\n",
       " ['t', 'o'],\n",
       " ['p', 'e', 'r', 'f', 'o', 'r', 'm'],\n",
       " ['o', 't', 'h', 'e', 'r'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['a', 's'],\n",
       " ['r', 'e', 'q', 'u', 'i', 'r', 'e', 'd'],\n",
       " ['b', 'y'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'o', 'm', 'e'],\n",
       " ['m', 'a', 'n', 'a', 'g', 'e', 'm', 'e', 'n', 't'],\n",
       " ['o', 'r'],\n",
       " ['s', 'e', 'n', 'i', 'o', 'r'],\n",
       " ['s', 't', 'a', 'f', 'f'],\n",
       " ['c', 'o', 'n', 's', 'i', 'd', 'e', 'r', 'a', 't', 'i', 'o', 'n'],\n",
       " ['w', 'i', 'l', 'l'],\n",
       " ['b', 'e'],\n",
       " ['g', 'i', 'v', 'e', 'n'],\n",
       " ['t', 'o'],\n",
       " ['y', 'o', 'u', 'r'],\n",
       " ['s', 'k', 'i', 'l', 'l', 's'],\n",
       " ['a', 'n', 'd'],\n",
       " ['s', 't', 'a', 't', 'u', 's'],\n",
       " ['w', 'h', 'e', 'n'],\n",
       " ['g', 'i', 'v', 'e', 'n'],\n",
       " ['t', 'h', 'e', 's', 'e'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['n', 'o', 't', 'e'],\n",
       " ['i', 't'],\n",
       " ['i', 's'],\n",
       " ['e', 'x', 'p', 'e', 'c', 't', 'e', 'd'],\n",
       " ['t', 'h', 'a', 't'],\n",
       " ['a', 'l', 'l'],\n",
       " ['d', 'u', 't', 'i', 'e', 's'],\n",
       " ['c', 'a', 'r', 'r', 'i', 'e', 'd'],\n",
       " ['o', 'u', 't'],\n",
       " ['w', 'i', 'l', 'l'],\n",
       " ['b', 'e'],\n",
       " ['p', 'e', 'r', 'f', 'o', 'r', 'm', 'e', 'd'],\n",
       " ['i', 'n'],\n",
       " ['s', 'p', 'i', 'r', 'i', 't'],\n",
       " ['o', 'f'],\n",
       " ['c', 'o', 'o', 'p', 'e', 'r', 'a', 't', 'i', 'o', 'n'],\n",
       " ['r', 'e', 'q', 'u', 'i', 'r', 'e', 'd'],\n",
       " ['f', 'r', 'o', 'm'],\n",
       " ['d', 'e', 'd', 'i', 'c', 'a', 't', 'e', 'd'],\n",
       " ['e', 'f', 'f', 'i', 'c', 'i', 'e', 'n', 't'],\n",
       " ['t', 'e', 'a', 'm'],\n",
       " ['w', 'h', 'o', 's', 'e'],\n",
       " ['p', 'r', 'i', 'm', 'e'],\n",
       " ['a', 'i', 'm'],\n",
       " ['i', 's'],\n",
       " ['t', 'o'],\n",
       " ['m', 'a', 'k', 'e'],\n",
       " ['t', 'h', 'e'],\n",
       " ['s', 'e', 'r', 'v', 'i', 'c', 'e'],\n",
       " ['u', 's', 'e', 'r'],\n",
       " ['s', 't', 'a', 'y'],\n",
       " ['a', 's'],\n",
       " ['c', 'o', 'm', 'f', 'o', 'r', 't', 'a', 'b', 'l', 'e'],\n",
       " ['a', 'n', 'd'],\n",
       " ['e', 'n', 'j', 'o', 'y', 'a', 'b', 'l', 'e'],\n",
       " ['a', 's'],\n",
       " ['p', 'o', 's', 's', 'i', 'b', 'l', 'e']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which words inside the description also inside inside `stopwords_en.txt`\n",
    "filtered_tokens_test_index = [[token for token in description if token in stop_words] for description in tk_description[test_index]]\n",
    "filtered_tokens_test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens in the test_index index description BEFORE removing stop words: 795\n",
      "The number of tokens in the test_index index description AFTER removing stop words: 440\n"
     ]
    }
   ],
   "source": [
    "# flitering stop words\n",
    "\n",
    "# print the test_index index description length before removing stop words\n",
    "print(\"The number of tokens in the test_index index description BEFORE removing stop words:\",len(tk_description[test_index]))\n",
    "\n",
    "# filter stop words in each document for the whole tokenized description\n",
    "tk_description = [[token for token in description if token not in stop_words] for description in tk_description]\n",
    "\n",
    "# print the test_index index description length after removing stop words\n",
    "print(\"The number of tokens in the test_index index description AFTER removing stop words:\",len(tk_description[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"ain't\",\n",
       " 'another',\n",
       " \"aren't\",\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " \"couldn't\",\n",
       " \"didn't\",\n",
       " \"doesn't\",\n",
       " \"don't\",\n",
       " 'enough',\n",
       " \"hadn't\",\n",
       " \"hasn't\",\n",
       " \"haven't\",\n",
       " 'ignored',\n",
       " \"isn't\",\n",
       " 'know',\n",
       " 'knows',\n",
       " 'known',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " \"shouldn't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"won't\",\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should I filter this?\n",
    "[w for w in stop_words if (\"not\" in w or \"n't\" in w or \"no\" in w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.6 Remove the word that appears only once in the document collection, based on term frequency\n",
    "\n",
    "* find out the list of words that appear only once in the **entire corpus**\n",
    "* remove these less frequent words from each tokenized description text\n",
    "\n",
    "We first need to find out the set of less frequent words by using the `hapaxes` function applied on the **term frequency** dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that appear only once in the entire corpus 4186\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'uncappedsales',\n",
       " 'apparellocation',\n",
       " 'navigation',\n",
       " 'label',\n",
       " 'mattunitystoke',\n",
       " 'londoncare',\n",
       " 'voted',\n",
       " 'ovens',\n",
       " 'conversational',\n",
       " 'flexitime',\n",
       " 'vulnerabilities',\n",
       " 'boasted',\n",
       " 'nele',\n",
       " 'complicated',\n",
       " 'uphold',\n",
       " 'bowling',\n",
       " 'geosciences',\n",
       " 'competitivesalary',\n",
       " 'personthis',\n",
       " 'greener',\n",
       " 'differing',\n",
       " 'journalists',\n",
       " 'redecoration',\n",
       " 'fta',\n",
       " \"industry's\",\n",
       " 'replying',\n",
       " 'sighted',\n",
       " 'prescreen',\n",
       " 'levelexcellent',\n",
       " 'presentationability',\n",
       " 'prefer',\n",
       " 'pe',\n",
       " 'rigorous',\n",
       " 'legislative',\n",
       " 'legend',\n",
       " 'busienss',\n",
       " 'usual',\n",
       " 'sapa',\n",
       " 'bobl',\n",
       " 'sterile',\n",
       " 'shilley',\n",
       " 'encountered',\n",
       " 'stays',\n",
       " 'supportaccountant',\n",
       " 'horley',\n",
       " 'rimellpenguinrecruitment',\n",
       " 'flwyddyn',\n",
       " 'comparison',\n",
       " 'modernisation',\n",
       " 'populus',\n",
       " 'savage',\n",
       " 'cardiovascular',\n",
       " 'corporatetaxaccounantcoventry',\n",
       " 'equipping',\n",
       " 'blaenoriaethu',\n",
       " 'sudbury',\n",
       " 'permanently',\n",
       " 'offhighway',\n",
       " 'stan',\n",
       " 'grips',\n",
       " 'netley',\n",
       " 'caliber',\n",
       " 'introduce',\n",
       " 'kingswodd',\n",
       " 'highenergy',\n",
       " 'aelodau',\n",
       " 'bullexcellent',\n",
       " 'stop',\n",
       " 'estimators',\n",
       " 'enterpriserecruitment',\n",
       " 'cavity',\n",
       " 'static',\n",
       " 'treasurer',\n",
       " 'stretford',\n",
       " 'opportunties',\n",
       " 'apqp',\n",
       " 'visibility',\n",
       " 'sortec',\n",
       " 'highreliability',\n",
       " 'sessional',\n",
       " 'possession',\n",
       " 'ethicexcellent',\n",
       " 'qr',\n",
       " 'groundbreaking',\n",
       " 'autoclaves',\n",
       " 'score',\n",
       " 'recruitmentbranchmanagerbirmingham',\n",
       " 'testability',\n",
       " 'recreational',\n",
       " 'venepuncture',\n",
       " 'expereince',\n",
       " 'grows',\n",
       " 'expereinced',\n",
       " 'littlehampton',\n",
       " 'remits',\n",
       " 'racheal',\n",
       " 'ljs',\n",
       " 'financialaccountant',\n",
       " 'nice',\n",
       " 'goaloriented',\n",
       " 'bishop',\n",
       " 'exploit',\n",
       " 'die',\n",
       " 'areathe',\n",
       " 'toulouse',\n",
       " \"yw'ch\",\n",
       " 'franchise',\n",
       " 'occassionaly',\n",
       " 'squash',\n",
       " 'compilation',\n",
       " 'careful',\n",
       " 'rotahours',\n",
       " 'onetoone',\n",
       " 'correspondent',\n",
       " 'iwerddon',\n",
       " 'defeat',\n",
       " 'ctu',\n",
       " 'payrolls',\n",
       " \"gyda'r\",\n",
       " 'unconscious',\n",
       " 'serviceengineerhighpressuresystems',\n",
       " 'eyre',\n",
       " 'gateway',\n",
       " 'shortlist',\n",
       " 'sut',\n",
       " 'robinsonappointgroup',\n",
       " 'ota',\n",
       " 'iosh',\n",
       " 'practises',\n",
       " 'assure',\n",
       " 'dysphagia',\n",
       " 'infopath',\n",
       " 'payrollmanager',\n",
       " 'ukbased',\n",
       " 'psychology',\n",
       " 'multinationals',\n",
       " 'megabrands',\n",
       " 'uksalary',\n",
       " 'netherlands',\n",
       " 'staines',\n",
       " 'crafts',\n",
       " 'facilitys',\n",
       " 'nonconformance',\n",
       " 'recruited',\n",
       " 'lng',\n",
       " 'caledonian',\n",
       " 'citations',\n",
       " 'disruption',\n",
       " 'accurancy',\n",
       " 'bradley',\n",
       " 'supermarket',\n",
       " 'smithhclpermanent',\n",
       " 'philosophy',\n",
       " 'laiddown',\n",
       " 'sort',\n",
       " 'closeknit',\n",
       " 'chriscavendishmaine',\n",
       " 'forces',\n",
       " 'crushers',\n",
       " 'lin',\n",
       " 'electrification',\n",
       " 'overlook',\n",
       " 'suffered',\n",
       " \"nation's\",\n",
       " 'bri',\n",
       " 'elimination',\n",
       " 'academies',\n",
       " 'distract',\n",
       " 'carolineprestonpagepersonnel',\n",
       " 'gauge',\n",
       " 'ranked',\n",
       " 'germanspeakingtelemarketer',\n",
       " 'financialadministratorandsupportfleetdept',\n",
       " 'experimentation',\n",
       " \"mae'r\",\n",
       " 'requesting',\n",
       " 'stroud',\n",
       " 'manylebau',\n",
       " 'interconnects',\n",
       " 'partaking',\n",
       " 'neurology',\n",
       " 'hawthorn',\n",
       " 'consumerfinanceadministrator',\n",
       " 'russell',\n",
       " 'msm',\n",
       " 'inherent',\n",
       " 'packageread',\n",
       " 'relayed',\n",
       " 'mitigation',\n",
       " 'essence',\n",
       " 'blow',\n",
       " 'nonaggressive',\n",
       " 'avonmouth',\n",
       " 'optimistic',\n",
       " 'learnt',\n",
       " 'bmssales',\n",
       " 'backselling',\n",
       " 'vss',\n",
       " 'twilight',\n",
       " 'mortgageprocessor',\n",
       " 'lynch',\n",
       " 'pi',\n",
       " 'complications',\n",
       " 'multipurpose',\n",
       " 'kate',\n",
       " 'uplift',\n",
       " 'chaps',\n",
       " 'offaly',\n",
       " 'charges',\n",
       " 'lapsed',\n",
       " 'quqlification',\n",
       " 'onpatch',\n",
       " 'currency',\n",
       " 'wagstaff',\n",
       " 'formally',\n",
       " 'hughes',\n",
       " 'pretender',\n",
       " 'attendances',\n",
       " 'ise',\n",
       " 'mpgw',\n",
       " 'microcephaly',\n",
       " 'aldershot',\n",
       " 'alectorecruit',\n",
       " 'oohs',\n",
       " 'recklessly',\n",
       " 'liquids',\n",
       " 'phoneas',\n",
       " 'residency',\n",
       " 'mno',\n",
       " 'radiographic',\n",
       " \"year's\",\n",
       " 'ox',\n",
       " 'borrows',\n",
       " 'scousinscompassltd',\n",
       " 'upselling',\n",
       " 'balcance',\n",
       " 'profound',\n",
       " 'endless',\n",
       " 'corby',\n",
       " 'aviation',\n",
       " 'walking',\n",
       " 'subsided',\n",
       " 'polishing',\n",
       " 'flavour',\n",
       " 'amazon',\n",
       " 'attraction',\n",
       " 'sicr',\n",
       " 'oddeutu',\n",
       " 'ciob',\n",
       " 'kyc',\n",
       " 'ofeurope',\n",
       " 'huntress',\n",
       " 'electricalcontrolandinstrumentationengineer',\n",
       " 'conveyers',\n",
       " 'legally',\n",
       " 'pritchardinteractionrecruitment',\n",
       " 'mortgagebroker',\n",
       " 'homeis',\n",
       " 'fetching',\n",
       " 'thoughts',\n",
       " 'ionformation',\n",
       " 'perceptions',\n",
       " 'formulate',\n",
       " \"upp's\",\n",
       " 'accidents',\n",
       " 'kilfoyleselectiongroup',\n",
       " 'cofrestru',\n",
       " 'hfss',\n",
       " 'anaesthesia',\n",
       " 'argue',\n",
       " 'responsibilitiesassess',\n",
       " 'hong',\n",
       " 'orally',\n",
       " 'concluding',\n",
       " 'schemethornbury',\n",
       " 'dialux',\n",
       " 'lte',\n",
       " 'kay',\n",
       " 'udp',\n",
       " 'ccst',\n",
       " 'barnstaple',\n",
       " 'recognises',\n",
       " 'creates',\n",
       " 'riskbased',\n",
       " 'fmmedia',\n",
       " 'tradition',\n",
       " 'accessories',\n",
       " 'accountsseniorportfoliomanager',\n",
       " 'petersfield',\n",
       " 'taflenni',\n",
       " 'allocate',\n",
       " 'installs',\n",
       " 'domain',\n",
       " 'earthworks',\n",
       " 'selfmotived',\n",
       " 'afraid',\n",
       " 'showerhead',\n",
       " 'graduateschemeexecutiverecruitment',\n",
       " 'referees',\n",
       " 'proficiency',\n",
       " 'reaching',\n",
       " 'lawned',\n",
       " 'martin',\n",
       " 'headquarter',\n",
       " 'faro',\n",
       " 'baileynorthstaffs',\n",
       " 'irish',\n",
       " 'cath',\n",
       " 'equate',\n",
       " 'weighing',\n",
       " 'catalogues',\n",
       " 'associateconsultantlondon',\n",
       " 'moss',\n",
       " 'accountmanagerinsurances',\n",
       " 'allegro',\n",
       " 'evangelize',\n",
       " 'earliest',\n",
       " 'super',\n",
       " 'pensioner',\n",
       " 'dct',\n",
       " 'bolster',\n",
       " 'hackney',\n",
       " 'strip',\n",
       " 'hightech',\n",
       " 'motability',\n",
       " 'salesminded',\n",
       " 'usage',\n",
       " 'jobshcone',\n",
       " 'republic',\n",
       " 'inverness',\n",
       " 'abstracts',\n",
       " 'estimations',\n",
       " 'crude',\n",
       " 'kpderbykareplus',\n",
       " 'exacting',\n",
       " 'academics',\n",
       " \"cv's\",\n",
       " 'musee',\n",
       " 'dismissal',\n",
       " 'negotiations',\n",
       " 'browse',\n",
       " 'dewrance',\n",
       " 'enforcing',\n",
       " 'math',\n",
       " 'vulnerability',\n",
       " 'nadine',\n",
       " 'uncover',\n",
       " 'preservation',\n",
       " 'modularizing',\n",
       " 'embolization',\n",
       " 'academia',\n",
       " 'stowmarket',\n",
       " 'keith',\n",
       " 'teller',\n",
       " 'unlike',\n",
       " 'prereg',\n",
       " 'midsize',\n",
       " 'shirebrook',\n",
       " 'careersflamecare',\n",
       " 'ajc',\n",
       " 'barnt',\n",
       " 'winchfield',\n",
       " 'ryan',\n",
       " 'applicationsdesignengineer',\n",
       " 'illustration',\n",
       " 'juniorsalesassistantimmediatestart',\n",
       " 'frics',\n",
       " 'rpo',\n",
       " 'cyfweliadau',\n",
       " 'quantitysurveyorseniorquantitysurveyorcivils',\n",
       " 'etcfull',\n",
       " 'roadmaps',\n",
       " 'validity',\n",
       " 'readiness',\n",
       " 'gilltimothyjamesconsulting',\n",
       " 'society',\n",
       " 'tagfeydd',\n",
       " 'bedroom',\n",
       " 'particpate',\n",
       " 'expensive',\n",
       " 'underpins',\n",
       " 'connells',\n",
       " 'stick',\n",
       " 'paxton',\n",
       " 'yourprimary',\n",
       " 'computers',\n",
       " 'msk',\n",
       " 'felt',\n",
       " 'geo',\n",
       " 'uniques',\n",
       " 'leveraging',\n",
       " 'enduser',\n",
       " 'obi',\n",
       " 'tenanted',\n",
       " 'databus',\n",
       " 'mlse',\n",
       " 'losses',\n",
       " 'ukas',\n",
       " 'biomedical',\n",
       " 'radar',\n",
       " 'bojang',\n",
       " 'repeatable',\n",
       " 'sean',\n",
       " 'estio',\n",
       " 'sacrifices',\n",
       " 'bulllegal',\n",
       " 'hitech',\n",
       " 'detox',\n",
       " 'oeic',\n",
       " 'wow',\n",
       " 'cso',\n",
       " 'extraordinary',\n",
       " 'independantly',\n",
       " 'alphanumeric',\n",
       " 'cafes',\n",
       " 'ntms',\n",
       " 'oho',\n",
       " 'elearning',\n",
       " 'humanitarian',\n",
       " 'injured',\n",
       " 'deeper',\n",
       " 'austell',\n",
       " 'exploiting',\n",
       " 'genres',\n",
       " 'surgeries',\n",
       " 'stamping',\n",
       " 'lunsars',\n",
       " 'pbo',\n",
       " 'pitches',\n",
       " 'btec',\n",
       " 'referafriend',\n",
       " 'comanage',\n",
       " 'seakeeping',\n",
       " 'purposely',\n",
       " 'frps',\n",
       " 'calmly',\n",
       " 'recruitmentsalesexecutive',\n",
       " 'holsbenefits',\n",
       " 'entailing',\n",
       " 'ddyfynnwyd',\n",
       " 'trailers',\n",
       " 'brosiectau',\n",
       " 'removal',\n",
       " 'mode',\n",
       " 'perkz',\n",
       " 'carryout',\n",
       " 'staffnet',\n",
       " 'elevators',\n",
       " 'tape',\n",
       " 'financeanalyst',\n",
       " 'diarising',\n",
       " 'airconditioningengineer',\n",
       " 'faced',\n",
       " 'actioning',\n",
       " 'count',\n",
       " 'stb',\n",
       " 'gds',\n",
       " 'observe',\n",
       " 'slots',\n",
       " 'advertisements',\n",
       " 'chelsea',\n",
       " 'sidmouth',\n",
       " 'mpgj',\n",
       " 'educate',\n",
       " 'eec',\n",
       " 'governments',\n",
       " 'honours',\n",
       " 'nhbc',\n",
       " 'burelf',\n",
       " \"manager's\",\n",
       " 'scratch',\n",
       " 'frankfurt',\n",
       " 'lyme',\n",
       " 'gains',\n",
       " 'fibre',\n",
       " 'mdonnellycobaltrecruitment',\n",
       " 'xslt',\n",
       " 'economically',\n",
       " 'disney',\n",
       " 'hsqe',\n",
       " 'painting',\n",
       " 'solet',\n",
       " 'safehands',\n",
       " 'salesmanagerwithdigitaladvertisingexperience',\n",
       " 'timesto',\n",
       " 'boy',\n",
       " 'nfosterencoretechnical',\n",
       " 'sows',\n",
       " 'tba',\n",
       " 'retrieve',\n",
       " 'cavelle',\n",
       " 'ce',\n",
       " 'brlocation',\n",
       " 'angeles',\n",
       " 'msirecruitment',\n",
       " 'bmw',\n",
       " 'execute',\n",
       " 'shrewsbury',\n",
       " 'accrued',\n",
       " 'stats',\n",
       " 'dirty',\n",
       " 'estateagencyseniorsalesnegotiator',\n",
       " 'wilmslow',\n",
       " 'indispensable',\n",
       " 'matched',\n",
       " 'herts',\n",
       " 'biotechnology',\n",
       " 'commercialaccountant',\n",
       " 'safetycritical',\n",
       " 'tollbooths',\n",
       " 'customerbased',\n",
       " 'seniormanufacturingengineer',\n",
       " 'leadgenerator',\n",
       " 'strike',\n",
       " 'tgargettredlinegroup',\n",
       " 'subsidized',\n",
       " 'villages',\n",
       " 'classen',\n",
       " 'brworking',\n",
       " 'psychiatrists',\n",
       " 'boards',\n",
       " 'entertaining',\n",
       " 'grad',\n",
       " 'wifi',\n",
       " 'ddefnyddio',\n",
       " 'aberdenshire',\n",
       " 'temps',\n",
       " 'experienceyou',\n",
       " 'kgv',\n",
       " 'aspiration',\n",
       " 'smb',\n",
       " 'excellentorganisation',\n",
       " 'str',\n",
       " 'parcel',\n",
       " 'brwdfrydedd',\n",
       " 'internships',\n",
       " 'barskydomusrecruitment',\n",
       " 'instances',\n",
       " 'shona',\n",
       " 'rated',\n",
       " 'reservoirdriven',\n",
       " 'cysylltwch',\n",
       " 'serviceengineercommercialcateringlaundryequipmentnorthamptonshire',\n",
       " 'competitively',\n",
       " 'staffnurse',\n",
       " 'rheoli',\n",
       " 'appendage',\n",
       " 'crawfordablyresources',\n",
       " 'mindset',\n",
       " 'bag',\n",
       " 'mds',\n",
       " 'graduatesalesexecutivetraineefieldsales',\n",
       " 'graphics',\n",
       " 'sanctuarygroup',\n",
       " 'healthcarewith',\n",
       " 'dvds',\n",
       " 'kuka',\n",
       " 'instruction',\n",
       " 'peirianneg',\n",
       " 'agilent',\n",
       " 'weeklocation',\n",
       " 'architecting',\n",
       " 'wimax',\n",
       " 'acces',\n",
       " 'dyddiad',\n",
       " 'resigning',\n",
       " 'nick',\n",
       " 'manned',\n",
       " 'nonconforming',\n",
       " 'motorsport',\n",
       " 'hco',\n",
       " 'pertaining',\n",
       " 'whls',\n",
       " 'farms',\n",
       " 'collaborate',\n",
       " 'fieldsalesexecutivederby',\n",
       " 'committees',\n",
       " 'plowe',\n",
       " 'witness',\n",
       " 'signed',\n",
       " 'entertainers',\n",
       " 'rpaynegenesisassociates',\n",
       " 'prestigiouslondoncontracts',\n",
       " 'ofdm',\n",
       " 'koteuncapp',\n",
       " 'liverpoolsalesassistant',\n",
       " 'bordon',\n",
       " 'alliance',\n",
       " 'hcr',\n",
       " 'sequence',\n",
       " 'hurt',\n",
       " 'congress',\n",
       " 'pub',\n",
       " 'wholesaler',\n",
       " 'oxygen',\n",
       " 'scarborough',\n",
       " 'willow',\n",
       " 'nmcresponsible',\n",
       " 'winches',\n",
       " 'universal',\n",
       " 'stokeontrent',\n",
       " 'timelimited',\n",
       " 'shepton',\n",
       " 'gatwick',\n",
       " 'workingforus',\n",
       " 'dollars',\n",
       " 'poultry',\n",
       " 'wrexham',\n",
       " 'attercliffe',\n",
       " 'introduces',\n",
       " 'waverley',\n",
       " 'excursions',\n",
       " 'businessdevelopmentexecutivefieldsalesdartford',\n",
       " 'preempt',\n",
       " 'magnificent',\n",
       " 'chorley',\n",
       " 'minuets',\n",
       " 'copenhagen',\n",
       " 'toptable',\n",
       " 'lgapiuenpcvwow',\n",
       " 'breath',\n",
       " 'compared',\n",
       " 'authoritative',\n",
       " 'meade',\n",
       " 'beneficialdegree',\n",
       " 'realstaffing',\n",
       " 'projectmanager',\n",
       " 'solutionsales',\n",
       " 'multiyear',\n",
       " 'dinnington',\n",
       " 'antibiotics',\n",
       " 'boasting',\n",
       " 'mnos',\n",
       " 'nx',\n",
       " 'dividendscorporateactionsadministrator',\n",
       " 'ofmanufacturing',\n",
       " 'vc',\n",
       " 'unethical',\n",
       " 'dietary',\n",
       " 'churches',\n",
       " 'ilford',\n",
       " 'wembley',\n",
       " 'challenged',\n",
       " 'currencies',\n",
       " 'lunchtime',\n",
       " \"bank's\",\n",
       " 'commercialdirectoronline',\n",
       " 'pabx',\n",
       " 'simms',\n",
       " 'envolve',\n",
       " 'trouble',\n",
       " 'bran',\n",
       " 'berks',\n",
       " 'similarly',\n",
       " 'bridgwater',\n",
       " 'resistance',\n",
       " 'considerate',\n",
       " 'tuning',\n",
       " 'transverse',\n",
       " 'cfranceeclypserecruitment',\n",
       " 'clearable',\n",
       " 'mailing',\n",
       " 'linda',\n",
       " 'deficient',\n",
       " 'unsocial',\n",
       " 'riskmanager',\n",
       " 'mechanicalbiasshiftmanager',\n",
       " 'caealpaed',\n",
       " 'campaigning',\n",
       " 'seminars',\n",
       " 'creditcontrollerparttime',\n",
       " 'ado',\n",
       " 'greeneking',\n",
       " 'hants',\n",
       " 'accustomed',\n",
       " 'producer',\n",
       " 'storrington',\n",
       " 'stocking',\n",
       " 'unprecedented',\n",
       " 'pf',\n",
       " 'adeiladu',\n",
       " 'sarahclinicalprofessionals',\n",
       " 'leadleyinteractionrecruitment',\n",
       " 'originating',\n",
       " 'citizenrecruit',\n",
       " 'cherry',\n",
       " 'crg',\n",
       " 'performancebased',\n",
       " 'inventors',\n",
       " 'spots',\n",
       " 'spares',\n",
       " 'dr',\n",
       " 'shoker',\n",
       " 'purchases',\n",
       " 'eh',\n",
       " 'winners',\n",
       " 'deypenguinrecruitment',\n",
       " 'harlow',\n",
       " 'draughting',\n",
       " 'learner',\n",
       " 'edwardspearce',\n",
       " 'overnights',\n",
       " 'industryinternational',\n",
       " 'policyassess',\n",
       " 'morley',\n",
       " 'analytically',\n",
       " 'evident',\n",
       " 'bundles',\n",
       " 'zone',\n",
       " 'oq',\n",
       " 'literateexperience',\n",
       " 'dws',\n",
       " 'messages',\n",
       " 'leverage',\n",
       " 'barriers',\n",
       " 'producers',\n",
       " 'chp',\n",
       " 'subscriptions',\n",
       " 'viable',\n",
       " 'datatransfer',\n",
       " 'morrisrisetechnical',\n",
       " 'shipley',\n",
       " 'agility',\n",
       " 'intrastat',\n",
       " 'investable',\n",
       " 'seeker',\n",
       " 'midwife',\n",
       " 'savvy',\n",
       " 'chatting',\n",
       " 'barnet',\n",
       " 'edk',\n",
       " 'hudson',\n",
       " 'cassual',\n",
       " 'rapportthe',\n",
       " 'bourton',\n",
       " 'depreciation',\n",
       " 'precious',\n",
       " 'citizensadvice',\n",
       " 'mechanic',\n",
       " 'tray',\n",
       " 'refinement',\n",
       " 'catalogue',\n",
       " \"engineer's\",\n",
       " 'seasoned',\n",
       " 'buses',\n",
       " 'resterilising',\n",
       " 'pfis',\n",
       " 'victims',\n",
       " 'postcloses',\n",
       " 'existence',\n",
       " 'benefitsmanchester',\n",
       " 'showering',\n",
       " 'ceremonies',\n",
       " 'diagnostics',\n",
       " 'servicesaround',\n",
       " 'payne',\n",
       " 'beneferedenbrown',\n",
       " 'financebusinesspartner',\n",
       " 'ts',\n",
       " 'cgmp',\n",
       " 'interdisciplinary',\n",
       " 'gradually',\n",
       " 'talents',\n",
       " 'wheelchairs',\n",
       " 'climates',\n",
       " 'homec',\n",
       " 'observed',\n",
       " 'dcdc',\n",
       " 'failed',\n",
       " 'stratford',\n",
       " 'denture',\n",
       " 'tables',\n",
       " 'efthis',\n",
       " 'toilet',\n",
       " 'timber',\n",
       " 'dispense',\n",
       " 'rails',\n",
       " 'powered',\n",
       " 'potentials',\n",
       " 'wwf',\n",
       " 'huntington',\n",
       " 'laois',\n",
       " 'buttress',\n",
       " 'therapeutics',\n",
       " 'veterinary',\n",
       " 'offset',\n",
       " 'massage',\n",
       " 'cro',\n",
       " 'suspicious',\n",
       " 'motherwell',\n",
       " 'insert',\n",
       " 'ntd',\n",
       " 'uncappedwell',\n",
       " 'exmouth',\n",
       " 'travels',\n",
       " 'showcasing',\n",
       " 'principally',\n",
       " 'hcsw',\n",
       " 'productive',\n",
       " 'clinet',\n",
       " 'bprs',\n",
       " 'gaming',\n",
       " 'mansion',\n",
       " 'psychiatry',\n",
       " 'kob',\n",
       " 'numberprevious',\n",
       " 'risklead',\n",
       " 'phy',\n",
       " 'arket',\n",
       " 'sas',\n",
       " 'motortradejobs',\n",
       " 'highcalibre',\n",
       " 'inyear',\n",
       " 'sarahrjanelewis',\n",
       " 'harder',\n",
       " 'acquire',\n",
       " 'survival',\n",
       " 'holds',\n",
       " 'hcv',\n",
       " 'fod',\n",
       " 'coleraine',\n",
       " 'rp',\n",
       " 'conscientious',\n",
       " 'jedwardscitizenrecruitment',\n",
       " 'postqualification',\n",
       " 'weektype',\n",
       " 'commences',\n",
       " 'sporting',\n",
       " 'collaborative',\n",
       " 'selfconfidence',\n",
       " 'feasibility',\n",
       " 'spectacles',\n",
       " 'humour',\n",
       " 'exposed',\n",
       " 'advertisers',\n",
       " 'solar',\n",
       " 'excessive',\n",
       " 'rachel',\n",
       " 'regulatorypolicymanager',\n",
       " 'chymryd',\n",
       " 'reinvest',\n",
       " \"hospital's\",\n",
       " 'gofyn',\n",
       " 'manageryou',\n",
       " 'sewage',\n",
       " 'medicalrecruitmentaapct',\n",
       " 'ballance',\n",
       " 'prm',\n",
       " 'initiating',\n",
       " 'eb',\n",
       " 'cameron',\n",
       " 'barring',\n",
       " 'pdr',\n",
       " 'gyfle',\n",
       " 'declinature',\n",
       " 'verticals',\n",
       " 'discharges',\n",
       " 'pmisalesexecutive',\n",
       " 'datacenters',\n",
       " 'kpmg',\n",
       " 'scade',\n",
       " 'awconsultingltd',\n",
       " 'borehamwood',\n",
       " 'ohand',\n",
       " 'uncappedpensioncarmobilelaptop',\n",
       " 'nhsggcrecruitmentnhs',\n",
       " 'adeptness',\n",
       " 'trader',\n",
       " 'insa',\n",
       " 'vitae',\n",
       " 'stationary',\n",
       " 'arrive',\n",
       " 'packaged',\n",
       " 'fifteen',\n",
       " 'fluctuations',\n",
       " 'atthhis',\n",
       " 'fodlon',\n",
       " 'bsuiness',\n",
       " 'rabaiotti',\n",
       " 'redditch',\n",
       " 'couriers',\n",
       " 'archived',\n",
       " 'percentage',\n",
       " 'tl',\n",
       " 'bitton',\n",
       " 'mcc',\n",
       " 'profi',\n",
       " 'matthewssearch',\n",
       " 'insure',\n",
       " 'outturn',\n",
       " 'safest',\n",
       " 'megane',\n",
       " 'flm',\n",
       " 'chwilio',\n",
       " 'qcf',\n",
       " 'gravitas',\n",
       " 'steering',\n",
       " 'lounge',\n",
       " 'gaines',\n",
       " 'badenochandclark',\n",
       " 'reed',\n",
       " 'marshfield',\n",
       " 'appliances',\n",
       " 'kalamazooreynolds',\n",
       " 'oscilloscopes',\n",
       " 'burton',\n",
       " 'populusconsultants',\n",
       " 'difficulty',\n",
       " 'broadcasters',\n",
       " 'brentwood',\n",
       " 'btelesalesadvisor',\n",
       " 'manageraims',\n",
       " 'oubound',\n",
       " 'block',\n",
       " 'lights',\n",
       " 'ne',\n",
       " 'shortly',\n",
       " 'gawley',\n",
       " 'towcester',\n",
       " \"leaver's\",\n",
       " 'xpath',\n",
       " 'hoist',\n",
       " 'headofcompliance',\n",
       " 'uninterrupted',\n",
       " 'leatherhead',\n",
       " 'tints',\n",
       " 'geoff',\n",
       " 'interpretations',\n",
       " 'worthington',\n",
       " 'ricoh',\n",
       " 'admincharterhouserecruitment',\n",
       " 'arada',\n",
       " 'bioscience',\n",
       " 'peg',\n",
       " 'telecomm',\n",
       " 'developmentongoing',\n",
       " 'dark',\n",
       " 'scan',\n",
       " 'recruitmentrevolution',\n",
       " 'undoubted',\n",
       " 'jack',\n",
       " 'attachment',\n",
       " 'transactsql',\n",
       " 'theukwithout',\n",
       " 'selfregulated',\n",
       " 'prisons',\n",
       " 'multidiscipline',\n",
       " 'combustion',\n",
       " 'derricklawesrecruit',\n",
       " 'borner',\n",
       " 'converged',\n",
       " 'logue',\n",
       " 'smalikchase',\n",
       " 'highlymotivated',\n",
       " 'peterlee',\n",
       " 'longwell',\n",
       " 'insulation',\n",
       " 'modifies',\n",
       " 'commercialunderwritingmotorteamleader',\n",
       " 'corfforaethol',\n",
       " 'attendant',\n",
       " 'washroom',\n",
       " 'newhall',\n",
       " 'rapportyou',\n",
       " 'conclusions',\n",
       " 'bannerappointgroup',\n",
       " 'cumulative',\n",
       " 'discrimination',\n",
       " 'yearfor',\n",
       " 'logic',\n",
       " 'organiser',\n",
       " 'analytic',\n",
       " 'gynae',\n",
       " 'finishers',\n",
       " 'aptitudes',\n",
       " 'fostered',\n",
       " 'akton',\n",
       " 'unincorporated',\n",
       " 'pair',\n",
       " 'overachiever',\n",
       " 'applytodaystarttomorrownewsalesfor',\n",
       " 'cascaded',\n",
       " 'destamat',\n",
       " 'qualityrewarded',\n",
       " 'tvs',\n",
       " 'servicesthis',\n",
       " 'pending',\n",
       " 'hydrodynamic',\n",
       " 'thunderhclplc',\n",
       " 'earns',\n",
       " 'hsmp',\n",
       " 'strgroup',\n",
       " 'commmercial',\n",
       " 'opportunityto',\n",
       " 'scenery',\n",
       " 'korea',\n",
       " 'searchrecruitmentconsultantmediaandtechnologybrighton',\n",
       " 'crossfunctional',\n",
       " 'outsource',\n",
       " 'pipefitting',\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import *\n",
    "from itertools import chain\n",
    "\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "term_fd = FreqDist(words) # compute term frequency for each unique word/type\n",
    "\n",
    "# Using hapaxes() to see less frequent words in term frequency\n",
    "lessFreqWords = set(term_fd.hapaxes())\n",
    "print(f'The number of words that appear only once in the entire corpus {len(lessFreqWords)}\\n')\n",
    "lessFreqWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5218\n",
      "Total number of tokens:  102975\n",
      "Lexical diversity:  0.05067249332362224\n",
      "Total number of description: 776\n",
      "Average description length: 132.69974226804123\n",
      "Maximum description length: 471\n",
      "Minimum description length: 12\n",
      "Standard deviation of description length: 70.3782402519735\n"
     ]
    }
   ],
   "source": [
    "def removeLessFreqWords(description):\n",
    "    return [w for w in description if w not in lessFreqWords]\n",
    "\n",
    "tk_description = [removeLessFreqWords(description) for description in tk_description]\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.7 Remove the top 50 most frequent words based on document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experience', 586),\n",
       " ('role', 499),\n",
       " ('work', 453),\n",
       " ('team', 431),\n",
       " ('working', 407),\n",
       " ('skills', 366),\n",
       " ('client', 358),\n",
       " ('job', 348),\n",
       " ('company', 343),\n",
       " ('business', 342),\n",
       " ('uk', 316),\n",
       " ('excellent', 309),\n",
       " ('management', 301),\n",
       " ('based', 287),\n",
       " ('apply', 286),\n",
       " ('opportunity', 280),\n",
       " ('salary', 270),\n",
       " ('required', 269),\n",
       " ('successful', 267),\n",
       " ('support', 261),\n",
       " ('join', 252),\n",
       " ('candidate', 248),\n",
       " ('service', 242),\n",
       " ('knowledge', 241),\n",
       " ('development', 235),\n",
       " ('leading', 234),\n",
       " ('high', 224),\n",
       " ('cv', 223),\n",
       " ('manager', 220),\n",
       " ('www', 220),\n",
       " ('training', 214),\n",
       " ('sales', 211),\n",
       " ('strong', 211),\n",
       " ('provide', 209),\n",
       " ('including', 209),\n",
       " ('services', 208),\n",
       " ('ability', 201),\n",
       " ('contact', 200),\n",
       " ('position', 199),\n",
       " ('recruitment', 196),\n",
       " ('full', 194),\n",
       " ('benefits', 193),\n",
       " ('posted', 192),\n",
       " ('jobseeking', 191),\n",
       " ('originally', 191),\n",
       " ('clients', 187),\n",
       " ('include', 187),\n",
       " ('good', 187),\n",
       " ('essential', 186),\n",
       " ('information', 184)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(chain.from_iterable([set(description) for description in tk_description]))\n",
    "doc_fd = FreqDist(words)  # compute document frequency for each unique word/type\n",
    "top50MostFreqWords = doc_fd.most_common(50)\n",
    "top50MostFreqWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5218\n",
      "Total number of tokens:  102975\n",
      "Lexical diversity:  0.05067249332362224\n",
      "Total number of description: 776\n",
      "Average description length: 132.69974226804123\n",
      "Maximum description length: 471\n",
      "Minimum description length: 12\n",
      "Standard deviation of description length: 70.3782402519735\n"
     ]
    }
   ],
   "source": [
    "def removeMostFreqWords(description):\n",
    "    return [w for w in description if w not in top50MostFreqWords]\n",
    "\n",
    "tk_description = [removeMostFreqWords(description) for description in tk_description]\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Updated Statistics\n",
    "In the above, we have done all required pre-processed steps, now let's have a look at the statistics again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final statistic after pre-processing:\n",
      "\n",
      "Vocabulary size:  5218\n",
      "Total number of tokens:  102975\n",
      "Lexical diversity:  0.05067249332362224\n",
      "Total number of description: 776\n",
      "Average description length: 132.69974226804123\n",
      "Maximum description length: 471\n",
      "Minimum description length: 12\n",
      "Standard deviation of description length: 70.3782402519735\n"
     ]
    }
   ],
   "source": [
    "# # specify\n",
    "# ignored_words = [w for w in stop_words if not (\"not\" in w or \"n't\" in w or \"no\" in w)]\n",
    "\n",
    "# # filter out stop words\n",
    "# tk_description = [[w for w in description if w not in ignored_words] \\\n",
    "#                       for description in tk_description]\n",
    "\n",
    "\n",
    "print(f'The final statistic description after pre-processing:\\n')\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Recall, from the beginning, we have the following:**  \n",
    "_____________________________________________\n",
    "\n",
    "Vocabulary size:  9834\n",
    "\n",
    "Total number of tokens:  186952\n",
    "\n",
    "Lexical diversity:  0.052601737344345076\n",
    "\n",
    "Total number of description: 776\n",
    "\n",
    "Average description length: 240.91752577319588\n",
    "\n",
    "Maximun description length: 815\n",
    "\n",
    "Minimun description length: 13\n",
    "\n",
    "Standard deviation of description length: 124.97750685071483\n",
    "_____________________________________________\n",
    "\n",
    "We've shrunk more than 40% of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.8 Save all job advertisement text and information in `.txt` files \n",
    "+ we will retrieve them. in task 2 and 3\n",
    "+ Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "+ We are going to store all the preprocessed description texts and its corresponding labels into files for task 2.\n",
    "* all the tokenized description are stored in a .txt file named `description.txt`\n",
    "    * each line is a description text, which contained all the tokens of the description text, separated by a space ' '\n",
    "* all the corresponding labels are store in a .txt file named `category.txt`\n",
    "    * each line is a label (one of these 4 values: 0,1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved description into description.txt\n",
      "Successfully saved category into category.txt\n",
      "Successfully saved title into title.txt\n"
     ]
    }
   ],
   "source": [
    "# save description text\n",
    "def save_description(descriptionFilename,tk_description):\n",
    "    out_file = open(descriptionFilename, 'w') # creates a txt file and open to save the descriptions\n",
    "    string = \"\\n\".join([\" \".join(description) for description in tk_description])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the category corresponding with the description text\n",
    "def save_category(categoryFilename,category):\n",
    "    out_file = open(categoryFilename, 'w') # creates a txt file and open to save category\n",
    "    string = \"\\n\".join([str(s) for s in category])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the title corresponding with the description text\n",
    "def save_title(titleFilename,title):\n",
    "    out_file = open(titleFilename, 'w') # creates a txt file and open to save title\n",
    "    string = \"\\n\".join([str(s) for s in title])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save description into txt file\n",
    "descriptionFilename = \"description.txt\"\n",
    "save_description(descriptionFilename,tk_description)\n",
    "print(f'Successfully saved description into {descriptionFilename}')\n",
    "\n",
    "# save category into txt file\n",
    "categoryFilename = \"category.txt\"\n",
    "save_category(categoryFilename,category)\n",
    "print(f'Successfully saved category into {categoryFilename}')\n",
    "\n",
    "# save title into txt file\n",
    "titleFilename = \"title.txt\"\n",
    "save_title(titleFilename,title)\n",
    "print(f'Successfully saved title into {titleFilename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.9 Build a vocabulary of the cleaned job advertisement descriptions\n",
    "\n",
    "`vocab.txt`\n",
    "\n",
    "This file contains the unigram vocabulary, one each line, in the following format: word_string:word_integer_index. Very importantly, words in the vocabulary must be sorted in alphabetical order, and the index value starts from 0. This file is the key to interpret the sparse encoding. For instance, in the following example, the word aaron is the test_index word (the corresponding integer_index as 19) in the vocabulary (note that the index values and words in the following image are artificial and used to demonstrate the required format only, it doesn't reflect the values of the actual expected output).\n",
    "\n",
    "In the following, we also specify the format that we want the information to be displayed by specifying the formatting string:\n",
    "\n",
    "`'%(word_string):%(word_integer_index)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aap', 'aaron', 'aat', 'abb', 'abenefit', 'aberdeen', 'abi', 'abilities', 'ability', 'abreast']\n"
     ]
    }
   ],
   "source": [
    "def write_vocab(vocab, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            f.write(word + ':' + str(i) + '\\n')\n",
    "            \n",
    "# convert tokenized description into a alphabetically sorted list\n",
    "vocab = sorted(list(set(chain.from_iterable(tk_description))))\n",
    "\n",
    "# save the sorted vocabulary list into a file according to the required format\n",
    "write_vocab(vocab, 'vocab.txt')\n",
    "\n",
    "# print out the first 10 words in the vocabulary to test\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category at index 0: Accounting_Finance\n",
      "Category at index 1: Engineering\n",
      "Category at index 2: Healthcare_Nursing\n",
      "Category at index 3: Sales\n"
     ]
    }
   ],
   "source": [
    "# loop through the index of the target_names and print the category name\n",
    "for i in range(len(df['target_names'])):\n",
    "    print(f'Category at index {i}: {df[\"target_names\"][i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 776 entries, 0 to 775\n",
      "Data columns (total 6 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Title                  776 non-null    object\n",
      " 1   Webindex               776 non-null    int64 \n",
      " 2   Company                776 non-null    object\n",
      " 3   Description            776 non-null    object\n",
      " 4   Tokenized Description  776 non-null    object\n",
      " 5   Category               776 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 36.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Webindex</th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>Tokenized Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finance / Accounts Asst Bromley to ****k</td>\n",
       "      <td>68997528</td>\n",
       "      <td>First Recruitment Services</td>\n",
       "      <td>Accountant (partqualified) to **** p.a. South ...</td>\n",
       "      <td>accountant partqualified south east london cli...</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fund Accountant  Hedge Fund</td>\n",
       "      <td>68063513</td>\n",
       "      <td>Austin Andrew Ltd</td>\n",
       "      <td>One of the leading Hedge Funds in London is cu...</td>\n",
       "      <td>leading hedge funds london recruiting fund acc...</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deputy Home Manager</td>\n",
       "      <td>68700336</td>\n",
       "      <td>Caritas</td>\n",
       "      <td>An exciting opportunity has arisen to join an ...</td>\n",
       "      <td>exciting opportunity arisen join establish pro...</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  Webindex  \\\n",
       "0  Finance / Accounts Asst Bromley to ****k  68997528   \n",
       "1               Fund Accountant  Hedge Fund  68063513   \n",
       "2                       Deputy Home Manager  68700336   \n",
       "\n",
       "                      Company  \\\n",
       "0  First Recruitment Services   \n",
       "1           Austin Andrew Ltd   \n",
       "2                     Caritas   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Accountant (partqualified) to **** p.a. South ...   \n",
       "1  One of the leading Hedge Funds in London is cu...   \n",
       "2  An exciting opportunity has arisen to join an ...   \n",
       "\n",
       "                               Tokenized Description            Category  \n",
       "0  accountant partqualified south east london cli...  Accounting_Finance  \n",
       "1  leading hedge funds london recruiting fund acc...  Accounting_Finance  \n",
       "2  exciting opportunity arisen join establish pro...  Healthcare_Nursing  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert job ad to a dataframe\n",
    "job_ad = pd.DataFrame({'Title': title, 'Webindex': webindex, 'Company': company, 'Description': description,'Tokenized Description': tk_description, 'Category': category})\n",
    "\n",
    "# change Tokenized Description to string separated by space\n",
    "job_ad['Tokenized Description'] = job_ad['Tokenized Description'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "\n",
    "# replace the value in Category column\n",
    "job_ad['Category'] = job_ad['Category'].replace([0,1,2,3],['Accounting_Finance','Engineering','Healthcare_Nursing','Sales'])\n",
    "\n",
    "# Cast Webindex to int\n",
    "job_ad['Webindex'] = job_ad['Webindex'].astype(int)\n",
    "\n",
    "# save job ad to csv file\n",
    "job_ad.to_csv('job_ad.csv', index=False)\n",
    "\n",
    "# print basic info about the job_ad data frame\n",
    "print(job_ad.info())\n",
    "\n",
    "# print first 3 rows\n",
    "job_ad.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Convert all `.ipynb` notebooks in the same directory into `.py` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook task1.ipynb to python\n",
      "[NbConvertApp] Writing 28834 bytes to task1.py\n",
      "[NbConvertApp] Converting notebook task2_3.ipynb to python\n",
      "[NbConvertApp] Writing 14959 bytes to task2_3.py\n"
     ]
    }
   ],
   "source": [
    "# The .py format of the jupyter notebook\n",
    "for fname in os.listdir():\n",
    "    if fname.endswith('ipynb'):\n",
    "        os.system(f'jupyter nbconvert {fname} --to python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.3 Summary</strong></h3>\n",
    "\n",
    "We have demonstrated the basic text pre-processing steps of sentence segmentation and tokenization. \n",
    "There are a couple of things that we should keep in mind:\n",
    "\n",
    "* we have covered the fundamentals of text pre-processing steps of Case Normalization, Stop Word Removing, Stemming and Lemmatization. \n",
    "\n",
    "* As mentioned before, though these steps are doing very different things to the text we have, however, one common effect among them, is the reduction on the size of the vocabulary (the list of distinct words contained in the corpus). \n",
    "\n",
    "* How we should process the text depends on the downstream analysis. Before we do any pre-processing, we should decide on the scope of the text to be used in the downstream analysis task. For instance, should we use an entire document? Or should we break the document down into sections, paragraphs, or sentences. Take another example. If we are analysing emails, should we keep the headers information? or should we focus on the email body? Choosing the proper scope depends on the goals of the analysis task. For example, you might choose to use an entire document in document classification and clustering tasks while one might choose smaller units like paragraphs or sentences in document summarization and information retrieval tasks. The scope chosen will have an  impact on the steps needed in the pre-processing process.\n",
    "\n",
    "* In this activity, we have shown you multiple ways to do tokenization. However, there is no single right way to do tokenization.  It completely depends on the corpus and the text analysis task you are going to perform. The major question of the tokenization phase is what counts as a token. In some of the text analysis task. Although word tokenization is relatively easy compared with other NLP or text mining task, errors made in this phase will propagate into later analysis and cause problems.\n",
    "\n",
    "* Case Normalization is a very simple process to do, though it is indeed very effective. \n",
    "The above is a very simple example (consisting of a very short paragraph) and it might not show much reduction on the vocabulary size. Imagine if you have a large corpus, doing case normalization will significantly reduce the vocabulary size, and thus helps the analysis algorithms to focus on different meaning of tokens rather than its cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> # Discussion\n",
    ">>In some of the text analysis tasks, we have to be mindful in the process of stopword removal. \n",
    "In some scenarios, stop words removal can wipe out relevant information and modify the context in a given sentence. \n",
    "For example, if we are performing a sentiment analysis, the word 'not', although is a stop word, it carries critical information, i.e. 'like' and 'not like' obviously are carrying completely reversed meaning.\n",
    "We might fool out our algorithm off track if we remove a stop word like “not”. \n",
    "You should always carefully consider these conditions, design the list of \"stop words\" that are to removed based on your specific objectives.\n",
    "\n",
    ">> we have build logistic regression models based our generated text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.4 References</strong></h3>\n",
    "\n",
    "\n",
    "+ [1] Sentence boundary disambiguation. https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation\n",
    "+ [2] Tokenization. https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html  \n",
    "+ [3] Your Guide to Natural Language Processing (NLP). https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1  \n",
    "+ [4] Introduction to Natural Language Processing for Text. https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63  \n",
    "+ [5] [Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book/ch02.html): Chapter 2 of \"Natural Language Processing with Python\" By Steven Bird, Ewan Kelin & Edward Loper.  \n",
    "+ [6]. [Corpus Readers](http://www.nltk.org/howto/corpus.html#tagged-corpora): An NLTK tutorial on accessing the contents of a diverse set of corpora.\n",
    "\n",
    "+ [1] Stop words. https://en.wikipedia.org/wiki/Stop_word  \n",
    "+ [3] Bird, Steven, Edward Loper and Ewan Klein (2009), [Natural Language Processing with Python](http://www.nltk.org/book/). O’Reilly Media Inc.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}