{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "\n",
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Task 1. Basic Text Pre-processing</strong></h3>\n",
    "\n",
    "#### Student Name: Tran Ngoc Anh Thu\n",
    "#### Student ID: s3879312\n",
    "\n",
    "Date: \"October 2, 2022\"\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* sklearn\n",
    "* collections\n",
    "* re\n",
    "* numpy\n",
    "* nltk\n",
    "* itertools\n",
    "* pandas\n",
    "* os\n",
    "\n",
    "## Steps\n",
    "1. Load data\n",
    "2. Text Pre-processing\n",
    "    * Sentence Segmentation\n",
    "    * Word Tokenization\n",
    "    * Removing Single Character Tokens\n",
    "    * Removing Stop words\n",
    "3. Saving the Pre-processing Reviews\n",
    "\n",
    "## Introduction\n",
    "Nowadays there are many job hunting websites including seek.com.au and au.indeed.com. These job hunting sites all manage a job search system, where job hunters could search for relevant jobs based on keywords, salary, and categories. In previous years, the category of an advertised job was often manually entered by the advertiser (e.g., the employer). There were mistakes made for category assignment. As a result, the jobs in the wrong class did not get enough exposure to relevant candidate groups.\n",
    "With advances in text analysis, automated job classification has become feasible; and sensible suggestions for job categories can then be made to potential advertisers. This can help reduce human data entry error, increase the job exposure to relevant candidates, and also improve the user experience of the job hunting site. In order to do so, we need an automated job ads classification system that helps to predict the categories of newly entered job advertisements.\n",
    "\n",
    "In this **task1** notebook, we are going to explore a job advertisement data set, and focus on pre-processing the description only.\n",
    "In the next task **task2_3**, we will then use the pre-processed text reviews to generate data features and build classification models to predict label of the description.\n",
    "\n",
    "## Dataset\n",
    "+ A small collection of job advertisement documents (around 776 jobs) inside the `data` folder.\n",
    "+ Inside the data folder, there are four different sub-folders: Accounting_Finance, Engineering, Healthcare_Nursing, and Sales, representing a job category.\n",
    "+ The job advertisement text documents of a particular category are in the corresponding sub-folder.\n",
    "+ Each job advertisement document is a txt file named `Job_<ID>.txt`. It contains the title, the webindex (some will also have information on the company name, some might not), and the full description of the job advertisement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "from collections import Counter\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the version of the main packages\n",
    "print(\"Numpy version: \", np.__version__)\n",
    "print(\"Pandas version: \",pd.__version__)\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.1 Examining and loading data</strong></h3>\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n",
    "\n",
    "\n",
    "\n",
    "Before doing any pre-processing, we need to load the data into a proper format. \n",
    "To load the data, you have to explore the data folder. Inside the `data` folder:\n",
    "+ Inside the data folder you will see 4 different subfolders, namely: `Accounting_Finance`, `Engineering`,`Healthcare_Nursing`, and `Sales`, each folder name is a job category.\n",
    "+ The job advertisement text documents of a particular category are located in the corresponding subfolder.\n",
    "+ Each job advertisement document is a txt file, named as \"Job_<ID>.txt\". It contains the title, the webindex,(some will also have information on the company name, some might not), and the full description of the job advertisement. \n",
    "\n",
    "In this case, providing that the dataset is given in a very well organised way, I would use a super handy API [`load_files`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html) from `sklearn.datasets`. \n",
    "    \n",
    "**import the function by:**\n",
    "```python\n",
    "from sklearn.datasets import load_files  \n",
    "```\n",
    "\n",
    "Then you can use the function to directly load the data and labels, for example:\n",
    "```python\n",
    "df = load_files(r\"data\")  \n",
    "```\n",
    "\n",
    "The loaded `movie_data` is then a dictionary, with the following attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **DESCRIPTION**                                           |\n",
    "|--------------|---------------------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website            |\n",
    "| Title        | Title of the advertised job position                          |\n",
    "| Company      | Company (employer) of the advertised job position             |\n",
    "| Description  | the description of each job advertisement                     |\n",
    "\n",
    "\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load each folder and file inside the data folder\n",
    "df = load_files(r\"data\")\n",
    "# type of the loaded file\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['target'] # this corresponding to the index value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Name of the categories\n",
    "df['target_names'] # this corresponding to the name value of the 4 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Category at index 0: {df[\"target_names\"][0]}')\n",
    "print(f'Category at index 1: {df[\"target_names\"][1]}')\n",
    "print(f'Category at index 2: {df[\"target_names\"][2]}')\n",
    "print(f'Category at index 3: {df[\"target_names\"][3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test whether it matches, just in case\n",
    "emp = 20 # an example, note we will use this example to test for the whole task outputs.\n",
    "\n",
    "df['filenames'][emp], df['target'][emp] # from the file path we know that it's the correct label too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# assign variables\n",
    "full_description, category, directory = df.data, df.target, df.filenames\n",
    "\n",
    "# the 20th job advertisement description\n",
    "print(f'Job description: {full_description[emp]}\\n\\nCorresponding to the label {category[emp]} inside the {directory[emp]} directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ------> OBSERVATION:\n",
    "As we can see the current description is in the **binary** form and read as a byte object (a `b` in front of each review text if you print it out). Therefore, we need to decode into normal string for further pre-processing\n",
    "\n",
    "However, the tokenizer cannot apply a string pattern on a bytes-like object. To resolve this, we decode each read `full_description` text using `utf-8` by writing a decode function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def decode(l):\n",
    "    if isinstance(l, list):\n",
    "        return [decode(x) for x in l]\n",
    "    else:\n",
    "        return l.decode('utf-8')\n",
    "\n",
    "# decode the binary description into utf-8 form and save it to full_description\n",
    "full_description = decode(full_description)\n",
    "full_description[emp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ---------------> OBSERVATION:\n",
    "The current `description` contains these attributes:\n",
    "\n",
    "| **ATTRIBUTES**   | **MEANING**                                        |\n",
    "|--------------|----------------------------------------------------|\n",
    "| Webindex     | 8 digit Id of the job advertisement on the website |\n",
    "| Title        | Title of the advertised job position               |\n",
    "| Company      | Company (employer) of the advertised job position  |\n",
    "| Description  | the description of each job advertisement          |\n",
    "\n",
    "and I only want the description itself to perform text-preprocessing and NLP on `description`. Therefore, I will perform the following pre-processing steps to the description of each job advertisement;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.2 Pre-processing</strong></h3>\n",
    "\n",
    "## 1.2.1 Extract information from each job advertisement\n",
    "1. Extract information from each job advertisement. Perform the following pre-processing steps to the description of each job advertisement;\n",
    "2. Tokenize each job advertisement description. The word tokenization must use the following regular expression, `r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"`;\n",
    "3. All the words must be converted into the lower case;\n",
    "4. Remove words with length less than 2.\n",
    "5. Remove stopwords using the provided stop words list (i.e, stopwords_en.txt). It is located inside the\n",
    "same downloaded folder.\n",
    "6. Remove the word that appears only once in the document collection, based on term frequency.\n",
    "7. Remove the top 50 most frequent words based on document frequency.\n",
    "8. Save all job advertisement text and information in txt file(s) (you have flexibility to choose what format\n",
    "you want to save the preprocessed job ads, and you will need to retrieve the pre-processed job ads\n",
    "text in Task 2 & 3);\n",
    "9. Build a vocabulary of the cleaned job advertisement descriptions, save it in a txt file (please refer to the\n",
    "required output);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extract description, title, webindex,  from each job advertisement and test using emp\n",
    "\n",
    "# Extract description\n",
    "def extract_description(full_description):\n",
    "    description = [re.search(r'\\nDescription: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return description\n",
    "description = extract_description(full_description)\n",
    "print(f'Job description at index {emp}:\\n{description[emp]}\\n')\n",
    "\n",
    "# Extract title\n",
    "def extract_title(full_description):\n",
    "    title = [re.search(r'Title: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return title\n",
    "title = extract_title(full_description)\n",
    "print(f'Job title at index {emp}:\\n{title[emp]}\\n')\n",
    "\n",
    "# Extract webindex\n",
    "def extract_webindex(full_description):\n",
    "    webindex = [re.search(r'Webindex: (.*)', str(i)).group(1) for i in full_description]\n",
    "    return webindex\n",
    "webindex = extract_webindex(full_description)\n",
    "print(f'Job webindex at index {emp}:\\n{webindex[emp]}\\n')\n",
    "\n",
    "# Extract company\n",
    "def extract_company(company):\n",
    "    company = [re.search(r'Company: (.*)', str(i)).group(1) if re.search(r'Company: (.*)', str(i)) else \"NA\" for i in company]\n",
    "    return company\n",
    "company = extract_company(full_description)\n",
    "print(f'Job company at index {emp}:\\n{company[emp]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 + 1.2.3 Tokenize each job advertisement description lowing regular expression & lowercase all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizeDescription(raw_description):\n",
    "    \"\"\"\n",
    "        This function first convert all words to lowercases,\n",
    "        it then segment the raw description into sentences and tokenize each sentences\n",
    "        and convert the description to a list of tokens.\n",
    "    \"\"\"\n",
    "    description = raw_description.lower() # convert all words to lowercase\n",
    "\n",
    "    # segment into sentences\n",
    "    sentences = sent_tokenize(description)\n",
    "\n",
    "    # tokenize each sentence\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "\n",
    "    # merge them into a list of tokens\n",
    "    tokenised_description = list(chain.from_iterable(token_lists))\n",
    "    return tokenised_description\n",
    "\n",
    "tk_description = [tokenizeDescription(r) for r in description]  # list comprehension, generate a list of tokenized articles\n",
    "\n",
    "print(\"Raw description:\\n\",description[emp],'\\n')\n",
    "print(\"Tokenized description:\\n\",tk_description[emp],'\\n\\n')\n",
    "print(\"The original number of Tokenized description tokens: \",len(tk_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### A Few Statistics Before Any Further Pre-processing\n",
    "\n",
    "In the following, we are interested to know a few statistics at this very begining stage, including:\n",
    "* The total number of tokens across the corpus\n",
    "* The total number of types across the corpus, i.e. the size of vocabulary \n",
    "* The so-called, [lexical diversity](https://en.wikipedia.org/wiki/Lexical_diversity), referring to the ratio of different unique word stems (types) to the total number of words (tokens).  \n",
    "* The average, minimum and maximum number of token (i.e. document length) in the dataset.\n",
    "\n",
    "In the following, we wrap all these up as a function, since we will use this printing module later to compare these statistic values before and after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stats_print(tk_description):\n",
    "    words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of description:\", len(tk_description))\n",
    "    lens = [len(article) for article in tk_description]\n",
    "    print(\"Average description length:\", np.mean(lens))\n",
    "    print(\"Maximum description length:\", np.max(lens))\n",
    "    print(\"Minimum description length:\", np.min(lens))\n",
    "    print(\"Standard deviation of description length:\", np.std(lens))\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.4 Remove words with length less than 2.\n",
    "+ We have 2 types of most frequent words is that in terms either term frequency or document frequency\n",
    "+ In this sub-task, you are required to remove any token that only contains a single character (a token that of length less than 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"Before removing, the number of words that appear with length less than 2:\", len([w for w in word_counts if word_counts[w] < 2]))\n",
    "\n",
    "# filter out single character tokens\n",
    "tk_description = [[w for w in description if len(w) >=2] \\\n",
    "                      for description in tk_description]\n",
    "\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "print(\"After removing, the number of words that appear less than 2:\", len([w for w in word_counts if word_counts[w] < 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Remove the top 50 most frequent words based on document frequency.\n",
    "+ We have 2 types of most frequent words is that in terms either term frequency or document frequency\n",
    "\n",
    "Explore the most frequent words in the pre-processed tokenized review text corpus:\n",
    "* explore the most frequent words (top 50) based on term frequency and document frequency, respectively\n",
    "* compare the results using different frequency measurements, which words are extracted based on both frequency measurements?\n",
    "* think and decide on whether or not you would remove some of the most frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the top 50 most frequent words\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "word_counts = Counter(words) # count the number of times each word appears in the corpus\n",
    "top50 = word_counts.most_common(50) # get the top 50 most frequent words\n",
    "print(\"Top 50 most frequent words:\\n\",top50, \"\\n\\n\")\n",
    "\n",
    "tk_description = [[w for w in tk_description if w not in top50] for description in tk_description]\n",
    "top50 = word_counts.most_common(50) # get the top 50 most frequent words\n",
    "print(\"Top 50 most frequent words after removing:\\n\",top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable([set(description) for description in tk_description]))\n",
    "doc_fd = FreqDist(words)  # compute document frequency for each unique word/type\n",
    "doc_fd.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tk_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.5 Remove stopwords using the provided stop words list (i.e, stopwords_en.txt)\n",
    "> **Be CAREFUL**: as mentioned before, the purpose of this task is to pre-process the text reviews and later on we are going to use the pre-process text to build a sentiment analysis model. The stop word removal process requires careful consideration in this type of task.\n",
    "\n",
    "Remove the stop words from the tokenized text inside `stopwords_en.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove the stop words inside `stopwords_en.txt` from the tokenized text\n",
    "stopwords_file = 'stopwords_en.txt'\n",
    "\n",
    "# read the stop words into a list\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    stop_words = f.read().splitlines() \n",
    "print(f'The number of stop words inside {stopwords_file} is {len(stop_words)} including:\\n\\n{stop_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------> OBSERVATION:\n",
    "+ There 571 stopwords in total, which are often function words in English, like articles (e.g. \"the\", and \"an\"), pronouns (e.g. \"he\", \"him\", and \"they\"), particles (e.g., \"well\", \"however\" and \"thus\"), etc, and universal words in all job advertisement (e.g.'ask', 'asking', 'used', and 'useful')\n",
    "\n",
    "+ Note this this is just one of the lists and we emphasize that there is no universal list of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flitering stop words\n",
    "print(\"Before stopword removal:\",len(tk_description),\" tokens\")\n",
    "tk_description = [token for token in tk_description if token not in stop_words]\n",
    "print(\"After stopword removal:\",len(tk_description),\" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = [token for token in tk_description if token in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[w for w in stop_words if (\"not\" in w or \"n't\" in w or \"no\" in w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2.6 Remove the word that appears only once in the document collection, based on term frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2.7 Remove the top 50 most frequent words based on document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The Updated Statistics\n",
    "\n",
    "In the above, we have done a few pre-processed steps, now let's have a look at the statistics again:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# specify\n",
    "ignored_words = [w for w in stop_words if not (\"not\" in w or \"n't\" in w or \"no\" in w)]\n",
    "\n",
    "# filter out stop words\n",
    "tk_description = [[w for w in description if w not in ignored_words] \\\n",
    "                      for description in tk_description]\n",
    "\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall, from the beginning, we have the following:  \n",
    "_____________________________________________\n",
    "\n",
    "Vocabulary size:  9834\n",
    "\n",
    "Total number of tokens:  186952\n",
    "\n",
    "Lexical diversity:  0.052601737344345076\n",
    "\n",
    "Total number of description: 776\n",
    "\n",
    "Average description length: 240.91752577319588\n",
    "\n",
    "Maximun description length: 815\n",
    "\n",
    "Minimun description length: 13\n",
    "\n",
    "Standard deviation of description length: 124.97750685071483\n",
    "_____________________________________________\n",
    "\n",
    "We've shrunk more than 40% of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.8 Save all job advertisement text and information in txt files (we will retrieve them. in task 2 and 3)\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "\n",
    "We are going to store all the preprocessed description texts and its corresponding labels into files for task 2.\n",
    "* all the tokenized description are stored in a .txt file named `description.txt`\n",
    "    * each line is a description text, which contained all the tokens of the description text, separated by a space ' '\n",
    "* all the corresponding labels are store in a .txt file named `category.txt`\n",
    "    * each line is a label (one of these 4 values: 0,1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save description text\n",
    "def save_description(descriptionFilename,tk_description):\n",
    "    out_file = open(descriptionFilename, 'w') # creates a txt file and open to save the descriptions\n",
    "    string = \"\\n\".join([\" \".join(description) for description in tk_description])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the category corresponding with the description text\n",
    "def save_category(categoryFilename,category):\n",
    "    out_file = open(categoryFilename, 'w') # creates a txt file and open to save category\n",
    "    string = \"\\n\".join([str(s) for s in category])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save the title corresponding with the description text\n",
    "def save_title(titleFilename,title):\n",
    "    out_file = open(titleFilename, 'w') # creates a txt file and open to save title\n",
    "    string = \"\\n\".join([str(s) for s in title])\n",
    "    out_file.write(string)\n",
    "    out_file.close() # close the file\n",
    "\n",
    "# save description into txt file\n",
    "descriptionFilename = \"description.txt\"\n",
    "save_description(descriptionFilename,tk_description)\n",
    "print(f'Successfully saved description into {descriptionFilename}')\n",
    "\n",
    "# save category into txt file\n",
    "categoryFilename = \"category.txt\"\n",
    "save_category(categoryFilename,category)\n",
    "print(f'Successfully saved category into {categoryFilename}')\n",
    "\n",
    "# save title into txt file\n",
    "titleFilename = \"title.txt\"\n",
    "save_title(titleFilename,title)\n",
    "print(f'Successfully saved title into {titleFilename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2.9 Build a vocabulary of the cleaned job advertisement descriptions\n",
    "\n",
    "`vocab.txt`\n",
    "This file contains the unigram vocabulary, one each line, in the following format: word_string:word_integer_index. Very importantly, words in the vocabulary must be sorted in alphabetical order, and the index value starts from 0. This file is the key to interpret the sparse encoding. For instance, in the following example, the word aaron is the 20th word (the corresponding integer_index as 19) in the vocabulary (note that the index values and words in the following image are artificial and used to demonstrate the required format only, it doesn't reflect the values of the actual expected output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save all job advertisement text and information in txt file\n",
    "with open('job_ad.txt', 'w') as f:\n",
    "    f.write(\"Category: \" + str(category) + \"\\n\")\n",
    "    for i in range(len(tk_description)):\n",
    "        f.write(full_description[i] + \"\\n\")\n",
    "        f.write(\"Tokenized Description: \" + str(tk_description[i]) + \"\\n\")\n",
    "        f.write(\"Category: \" + str(df['target'][i]) + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    print(\"Successfully write job advertisement with the tokenized description in txt file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_vocab(vocab, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            f.write(word + ':' + str(i) + '\\n')\n",
    "# convert tokenized description into a alphabetically sorted list\n",
    "vocab = sorted(list(set(chain.from_iterable(tk_description))))\n",
    "write_vocab(vocab, 'vocab.txt')\n",
    "# print out the first 10 words in the vocabulary\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Category at index 0: {df[\"target_names\"][0]}')\n",
    "print(f'Category at index 1: {df[\"target_names\"][1]}')\n",
    "print(f'Category at index 2: {df[\"target_names\"][2]}')\n",
    "print(f'Category at index 3: {df[\"target_names\"][3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert job ad to a dataframe\n",
    "job_ad = pd.DataFrame({'Title': title, 'Webindex': webindex, 'Company': company, 'Description': description,'Tokenized Description': tk_description, 'Category': category})\n",
    "\n",
    "# change Tokenized Description to string separated by space\n",
    "job_ad['Tokenized Description'] = job_ad['Tokenized Description'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "\n",
    "\n",
    "# replace the value in Category column\n",
    "# Category at index 0: Accounting_Finance\n",
    "# Category at index 1: Engineering\n",
    "# Category at index 2: Healthcare_Nursing\n",
    "# Category at index 3: Sales\n",
    "job_ad['Category'] = job_ad['Category'].replace([0,1,2,3],['Accounting_Finance','Engineering','Healthcare_Nursing','Sales'])\n",
    "\n",
    "# Cast Webindex to int\n",
    "job_ad['Webindex'] = job_ad['Webindex'].astype(int)\n",
    "\n",
    "# save job ad to csv file\n",
    "job_ad.to_csv('job_ad.csv', index=False)\n",
    "\n",
    "print(job_ad.info())\n",
    "# print first 3 rows\n",
    "job_ad.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The .py format of the jupyter notebook\n",
    "for fname in os.listdir():\n",
    "    if fname.endswith('ipynb'):\n",
    "        os.system(f'jupyter nbconvert {fname} --to python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.3 Summary</strong></h3>\n",
    "\n",
    "In this activity, we have demonstrated the basic text pre-processing steps of sentence segmentation and tokenization. \n",
    "There are a couple of things that we should keep in mind:\n",
    "\n",
    "* we have covered the fundamentals of text pre-processing steps of Case Normalization, Stop Word Removing, Stemming and Lemmatization. \n",
    "\n",
    "* As mentioned before, though these steps are doing very different things to the text we have, however, one common effect among them, is the reduction on the size of the vocabulary (the list of distinct words contained in the corpus). \n",
    "\n",
    "* How we should process the text depends on the downstream analysis. Before we do any pre-processing, we should decide on the scope of the text to be used in the downstream analysis task. For instance, should we use an entire document? Or should we break the document down into sections, paragraphs, or sentences. Take another example. If we are analysing emails, should we keep the headers information? or should we focus on the email body? Choosing the proper scope depends on the goals of the analysis task. For example, you might choose to use an entire document in document classification and clustering tasks while one might choose smaller units like paragraphs or sentences in document summarization and information retrieval tasks. The scope chosen will have an  impact on the steps needed in the pre-processing process.\n",
    "\n",
    "* In this activity, we have shown you multiple ways to do tokenization. However, there is no single right way to do tokenization.  It completely depends on the corpus and the text analysis task you are going to perform. The major question of the tokenization phase is what counts as a token. In some of the text analysis task. Although word tokenization is relatively easy compared with other NLP or text mining task, errors made in this phase will propagate into later analysis and cause problems.\n",
    "\n",
    "* Case Normalization is a very simple process to do, though it is indeed very effective. \n",
    "The above is a very simple example (consisting of a very short paragraph) and it might not show much reduction on the vocabulary size. Imagine if you have a large corpus, doing case normalization will significantly reduce the vocabulary size, and thus helps the analysis algorithms to focus on different meaning of tokens rather than its cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of distinct words BEFORE further pre-processing:\",len(set(description)))\n",
    "print(\"Number of distinct words AFTER further pre-processing:\",len(set(tk_description)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Discussion\n",
    ">>In some of the text analysis tasks, we have to be mindful in the process of stopword removal. \n",
    "In some scenarios, stop words removal can wipe out relevant information and modify the context in a given sentence. \n",
    "For example, if we are performing a sentiment analysis, the word 'not', although is a stop word, it carries critical information, i.e. 'like' and 'not like' obviously are carrying completely reversed meaning.\n",
    "We might fool out our algorithm off track if we remove a stop word like “not”. \n",
    "You should always carefully consider these conditions, design the list of \"stop words\" that are to removed based on your specific objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>1.4 References</strong></h3>\n",
    "\n",
    "\n",
    "+ [1] Sentence boundary disambiguation. https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation\n",
    "+ [2] Tokenization. https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html  \n",
    "+ [3] Your Guide to Natural Language Processing (NLP). https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1  \n",
    "+ [4] Introduction to Natural Language Processing for Text. https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63  \n",
    "+ [5] [Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book/ch02.html): Chapter 2 of \"Natural Language Processing with Python\" By Steven Bird, Ewan Kelin & Edward Loper.  \n",
    "+ [6]. [Corpus Readers](http://www.nltk.org/howto/corpus.html#tagged-corpora): An NLTK tutorial on accessing the contents of a diverse set of corpora.\n",
    "\n",
    "+ [1] Stop words. https://en.wikipedia.org/wiki/Stop_word  \n",
    "+ [3] Bird, Steven, Edward Loper and Ewan Klein (2009), [Natural Language Processing with Python](http://www.nltk.org/book/). O’Reilly Media Inc.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
