{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzgpHVFP06Vc"
   },
   "source": [
    "# Fun with Word Embeddings\n",
    "\n",
    "In this activity, we will explore a few pre-trained word embeddings, and also will learn to train Word2Vec and FastText word embeddings. \n",
    "\n",
    "We will use Gensim implementation of Word2Vec. \n",
    "[Gensim](https://radimrehurek.com/gensim/) is an open source Python library for natural language processing, with a focus on topic modeling. \n",
    "To prepare for this activity, you need to install Gensim. \n",
    "You can do this by going to your terminal, and run the following command:\n",
    "```\n",
    "pip install --upgrade gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1622523314879,
     "user": {
      "displayName": "Huong Ha",
      "photoUrl": "",
      "userId": "18400067400744811817"
     },
     "user_tz": -600
    },
    "id": "U5SBmRUR06Vk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/tnathu-ai/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim  # lazy installation :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWGZvzM506Vm"
   },
   "source": [
    "In this notebook, there will be lots of things happening behind the scene &#128556; \n",
    "We can track events and display information through basic [Logging](https://docs.python.org/3/howto/logging.html).\n",
    "\n",
    "In the following, we also specify the format that we want the information to be displayed by specifying the formatting string `'%(asctime)s : %(levelname)s : %(message)s'`. \n",
    "\n",
    "For a full set of things that can appear in format strings, you can refer to the documentation for [LogRecord](https://docs.python.org/3/library/logging.html#logrecord-attributes) attributes, but for simple usage, you just need the `levelname` (severity), `message` (event description, including variable data) and perhaps to display when the event occurred with `asctime`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1622523477051,
     "user": {
      "displayName": "Huong Ha",
      "photoUrl": "",
      "userId": "18400067400744811817"
     },
     "user_tz": -600
    },
    "id": "6RYnY3KG06Vo"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pprint import pprint as print\n",
    "\n",
    "# Tracking events and display information through Logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X96WKrLz06WZ"
   },
   "source": [
    "## 3. FastText\n",
    "\n",
    "As mentioned before, `word2vec` model does not accommodate words that do not appear in the training corpus. \n",
    "\n",
    "Here, weâ€™ll learn to work with the fastText library for training word-embedding models, and performing similarity operations & vector lookups analogous to Word2Vec. \n",
    "\n",
    "In the following block of code, we import the `FastText` model form Gensim library, then:\n",
    "1. We set the path to the corpus file. Similar as above, we use the Bbc News article as the training corpus;\n",
    "2. Initialise the `FastText` model, similar as before, we use 100 dimention vectors;\n",
    "3. Then we build the vocabulary from the copurs;\n",
    "4. Finally, we train the fasttext model based on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ff96yIuR06Wa",
    "outputId": "ace4cabd-bab4-4186-a86b-290c31d00e11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 21:36:05,999 : INFO : FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=100, alpha=0.025>', 'datetime': '2022-10-01T21:36:05.999746', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "2022-10-01 21:36:06,000 : INFO : collecting all words and their counts\n",
      "2022-10-01 21:36:06,001 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-10-01 21:36:06,104 : INFO : collected 18345 word types from a corpus of 426118 raw words and 2225 sentences\n",
      "2022-10-01 21:36:06,104 : INFO : Creating a fresh vocabulary\n",
      "2022-10-01 21:36:06,137 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 retains 10485 unique words (57.15% of original 18345, drops 7860)', 'datetime': '2022-10-01T21:36:06.137930', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-10-01 21:36:06,138 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 leaves 404893 word corpus (95.02% of original 426118, drops 21225)', 'datetime': '2022-10-01T21:36:06.138527', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-10-01 21:36:06,187 : INFO : deleting the raw counts dictionary of 18345 items\n",
      "2022-10-01 21:36:06,188 : INFO : sample=0.001 downsamples 7 most-common words\n",
      "2022-10-01 21:36:06,188 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 396150.67942971224 word corpus (97.8%% of prior 404893)', 'datetime': '2022-10-01T21:36:06.188664', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-10-01 21:36:06,361 : INFO : estimated required memory for 10485 words, 2000000 buckets and 100 dimensions: 815621984 bytes\n",
      "2022-10-01 21:36:06,362 : INFO : resetting layer weights\n",
      "2022-10-01 21:36:08,133 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-10-01T21:36:08.133444', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-10-01 21:36:08,134 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 10485 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-10-01T21:36:08.134587', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-10-01 21:36:09,920 : INFO : EPOCH 0 - PROGRESS: at 25.89% examples, 73310 words/s, in_qsize -1, out_qsize 1\n",
      "2022-10-01 21:36:10,026 : INFO : EPOCH 0: training on 443497 raw words (412350 effective words) took 1.9s, 219259 effective words/s\n",
      "2022-10-01 21:36:11,836 : INFO : EPOCH 1 - PROGRESS: at 25.89% examples, 72315 words/s, in_qsize -1, out_qsize 1\n",
      "2022-10-01 21:36:11,943 : INFO : EPOCH 1: training on 443497 raw words (412364 effective words) took 1.9s, 216393 effective words/s\n",
      "2022-10-01 21:36:13,715 : INFO : EPOCH 2 - PROGRESS: at 25.89% examples, 73805 words/s, in_qsize -1, out_qsize 1\n",
      "2022-10-01 21:36:13,826 : INFO : EPOCH 2: training on 443497 raw words (412234 effective words) took 1.9s, 220073 effective words/s\n",
      "2022-10-01 21:36:15,586 : INFO : EPOCH 3 - PROGRESS: at 25.89% examples, 74379 words/s, in_qsize -1, out_qsize 1\n",
      "2022-10-01 21:36:15,695 : INFO : EPOCH 3: training on 443497 raw words (412435 effective words) took 1.9s, 221952 effective words/s\n",
      "2022-10-01 21:36:17,464 : INFO : EPOCH 4 - PROGRESS: at 25.89% examples, 73972 words/s, in_qsize -1, out_qsize 1\n",
      "2022-10-01 21:36:17,580 : INFO : EPOCH 4: training on 443497 raw words (412296 effective words) took 1.9s, 220058 effective words/s\n",
      "2022-10-01 21:36:17,581 : INFO : FastText lifecycle event {'msg': 'training on 2217485 raw words (2061679 effective words) took 9.4s, 218243 effective words/s', 'datetime': '2022-10-01T21:36:17.581687', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x7fcf00e417c0>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# 1. Set the corpus file names/path\n",
    "corpus_file = './bbcNews.txt'\n",
    "\n",
    "# 2. Initialise the Fast Text model\n",
    "bbcFT = FastText(vector_size=100) \n",
    "\n",
    "# 3. build the vocabulary\n",
    "bbcFT.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# 4. train the model\n",
    "bbcFT.train(\n",
    "    corpus_file=corpus_file, epochs=bbcFT.epochs,\n",
    "    total_examples=bbcFT.corpus_count, total_words=bbcFT.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(bbcFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hl50GrJ06Wa"
   },
   "source": [
    "We can retrieve the KeyedVectors from the model as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VHc0Wdf506Wb",
    "outputId": "9330e3b0-a9e0-4929-97e6-8c07f56139dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastTextKeyedVectors object at 0x7fcf00e410d0>\n"
     ]
    }
   ],
   "source": [
    "bbcFT_wv = bbcFT.wv\n",
    "print(bbcFT_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNEQkPbj06Wc"
   },
   "source": [
    "And see the vector of `king`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fs8ug13x06Wc",
    "outputId": "2d4bb135-8253-4552-97f1-56e88680bcbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25515214, -0.15216236, -0.91329324, -0.1548347 ,  0.25573623,\n",
       "        0.6096976 , -0.02978956,  0.4597498 ,  0.5173004 ,  0.21684076,\n",
       "       -0.8100364 ,  0.45270357, -0.76255137,  0.03837309,  0.29864702,\n",
       "        0.3910782 , -0.09769247, -0.6677153 ,  0.19720758, -0.6545355 ,\n",
       "       -0.6917739 ,  0.2979505 ,  0.5610619 ,  0.06513355, -0.35859495,\n",
       "       -0.55487525, -0.45098144,  0.02784753, -0.14543949,  0.6603091 ,\n",
       "       -0.60943335,  0.5215059 ,  0.2755244 , -0.13164893,  0.08852942,\n",
       "       -0.03730462,  1.2275649 ,  0.90195924, -1.1044537 ,  0.04230173,\n",
       "       -0.20181541,  0.22362003, -0.28896233, -0.48851252,  0.01154309,\n",
       "       -0.12411773, -0.5494902 , -0.3262416 ,  0.9721059 ,  0.09267493,\n",
       "        0.5495732 ,  0.8440249 ,  0.50472564,  0.12608802,  0.43247995,\n",
       "       -0.11829105, -1.2220495 , -0.33981144, -0.51818687, -0.01814928,\n",
       "       -0.5242976 , -0.94597554, -0.8754663 , -0.164685  , -0.91298574,\n",
       "        0.87168354, -0.59012735,  0.0038243 , -0.7854779 ,  1.0371838 ,\n",
       "        0.3059239 , -0.1683271 ,  0.6893739 , -0.34496242, -0.02724472,\n",
       "       -0.23117636,  0.52167505, -0.904656  ,  1.1196722 ,  0.15517478,\n",
       "        0.31754503,  0.07202518, -1.3765123 , -0.46616432, -0.6820944 ,\n",
       "       -0.23597361, -0.1692188 ,  0.24544148, -0.17338829, -0.6763998 ,\n",
       "       -1.3013135 , -0.49202585, -0.04881959, -1.0533633 ,  0.35943985,\n",
       "        0.90518755, -0.6832739 ,  0.3830676 , -0.38992864, -0.06441137],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbcFT_wv['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9cRhdpQ06Wk"
   },
   "source": [
    "### Save the model\n",
    "\n",
    "Similar as we do with the trained Word2Vec, we can also save our trained FastText model using the standard gensim methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d67lf31706Wl",
    "outputId": "907d5c67-13dc-4a03-847b-6037d3d1d2e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 21:36:18,007 : INFO : FastText lifecycle event {'fname_or_handle': 'bbcFT.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-10-01T21:36:18.007755', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-10-01 21:36:18,008 : INFO : storing np array 'vectors_ngrams' to bbcFT.model.wv.vectors_ngrams.npy\n",
      "2022-10-01 21:36:19,445 : INFO : not storing attribute buckets_word\n",
      "2022-10-01 21:36:19,446 : INFO : not storing attribute vectors\n",
      "2022-10-01 21:36:19,446 : INFO : not storing attribute cum_table\n",
      "2022-10-01 21:36:19,454 : INFO : saved bbcFT.model\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "bbcFT.save(\"bbcFT.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bYAvH9U06Ws"
   },
   "source": [
    "## 4. Summary\n",
    "\n",
    "In this activity, we have played with a few pretrained word embeddings including `Word2Vec` trained on the Google news dataset, and GloVe. \n",
    "We have also learnt to trained our `Word2Vec` and `FastText` models using our BBC News dataset. \n",
    "I hope you have a lot of fun in this activity. \n",
    "\n",
    "The semantic embeddings seem amazing, though so far, we have not yet really explore the actual usage of them. \n",
    "In the next activity, we will try to use this word embeddings for text classification task. Get ready! ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3NmNSQ06Ws"
   },
   "source": [
    "## 5. Exercise\n",
    "* There are multiple pre-trained models in Gensim, see Section **Pretrained models** in https://radimrehurek.com/gensim/models/word2vec.html. Indeed, Gensim also includes the GloVe implementation. In this activity, we didn't use this Gensim implementation, instead, we had demonstated how to load the pre-trained GloVe word embeddings from the original source. You can explore other pretrained models in Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tIPAg0Y06Ws"
   },
   "source": [
    "## Reference:\n",
    "[1] [Word Embeddingsâ€” Fun with Word2Vec and Game of Thrones](https://medium.com/@khulasaandh/word-embeddings-fun-with-word2vec-and-game-of-thrones-ea4c24fcf1b8)  \n",
    "[2] [Gensim Word2Vec Tutorial â€“ Full Working Example](http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.YJzBmmYza3f)  \n",
    "[3] [Word2vec Tutorial](https://rare-technologies.com/word2vec-tutorial/)  \n",
    "[4] [Word2Vec Model -- gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)   \n",
    "[5] [Using pre-trained word embeddings](https://nlp.stanford.edu/projects/glove/) \n",
    "[6] [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "[7] [Using pre-trained word embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-bYAvH9U06Ws",
    "TV3NmNSQ06Ws"
   ],
   "name": "Act 5_Fun with word embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
