{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Intro\n",
    "\n",
    "Once the text data is cleaned and tokenized it is ready for NLP analysis. Vectorization of the tokens allows data scientist to mathematically represent text as vectors. There are numerous ways to create these vectors.\n",
    "\n",
    "\n",
    "+ compare between ML models with different feature representation (count vector, weigthed and unweighted embedding models)\n",
    "\n",
    "+ single model (e.g., logistic regression with count vector representation), and compare the performance with different amount of info. \n",
    "\n",
    "+ Unweighted, you just sum the word embeddings of an job ad as the representation of an job ad. \n",
    "\n",
    "+ Weighted sum, ou can do a weighted sum of word embeddings as representation instead of just sum. \n",
    "\n",
    "You can do that for any embedding models and generate two version of doc representation\n",
    "\n",
    "## bulid:\n",
    "\n",
    "+ count vector representation\n",
    "+ weigthed embedding representation (use TF-IDF as the weight when you do weighted sum of word embeddings)\n",
    "+ unweigted embedding representation. \n",
    "\n",
    "## Question \n",
    "+ load pretrained model or train it from stractch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "\n",
    "<h3 style=\"color:#ffc0cb;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Task 2&3.<br>Feature Representation & Classification</strong></h3>\n",
    "\n",
    "#### Student Name: Tran Ngoc Anh Thu\n",
    "#### Student ID: s3879312\n",
    "\n",
    "Date: \"October 2, 2022\"\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used (please go to `requirements.txt` file for further details)\n",
    "* sklearn\n",
    "* collections\n",
    "* re\n",
    "* numpy\n",
    "* nltk\n",
    "* itertools\n",
    "* pandas\n",
    "* os\n",
    "* pylab\n",
    "* collections\n",
    "\n",
    "\n",
    "## Steps\n",
    "+ 1.1. Examining and loading data\n",
    "\n",
    "+ 1.2. Basic Text Pre-processing\n",
    "    * 1.2.1. Extract information from each job advertisement. Perform the following pre-processing steps to the description of each job advertisement;\n",
    "    * 1.2.2. Tokenize each job advertisement description. The word tokenization must use the following regular expression:\n",
    "    ```python\n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\" \n",
    "    ```\n",
    "    * 1.2.3. All the words must be converted into the lower case;\n",
    "    * 1.2.4. Remove words with length less than 2.\n",
    "    * 1.2.5. Remove stopwords using the provided stop words list (i.e, stopwords_en.txt). It is located inside the same downloaded folder.\n",
    "    * 1.2.6. Remove the word that appears only once in the document collection, based on term frequency.\n",
    "    * 1.2.7. Remove the top 50 most frequent words based on document frequency.\n",
    "    * 1.2.8. Save all job advertisement text and information in txt file(s) \n",
    "    * 1.2.9. Build a vocabulary of the cleaned job advertisement descriptions, save it in a txt file (please refer to the required output)\n",
    "    \n",
    "+ 1.3. Summary\n",
    "> * Discussion\n",
    "      \n",
    "+ 1.4. References\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Machine and Algorithm can not understand lossing categorical data. Therefore, we should encode those text into numerical values using feature representation\n",
    "We have pre-processed data in Task1. Once the text data is cleaned and tokenized it is ready for NLP analysis. Vectorization of the tokens allows us to mathematically represent text as vectors. There are numerous ways to create these vectors.\n",
    "\n",
    "\n",
    "+ compare between ML models with different feature representation (count vector, weigthed and unweighted embedding models)\n",
    "\n",
    "+ single model (e.g., logistic regression with count vector representation), and compare the performance with different amount of info. \n",
    "\n",
    "+ Unweighted, you just sum the word embeddings of an job ad as the representation of an job ad. \n",
    "\n",
    "+ Weighted sum, ou can do a weighted sum of word embeddings as representation instead of just sum.\n",
    "\n",
    "## Dataset\n",
    "+ A small collection of job advertisement documents (around 776 jobs) inside the `data` folder.\n",
    "+ Inside the data folder, there are four different sub-folders: Accounting_Finance, Engineering, Healthcare_Nursing, and Sales, representing a job category.\n",
    "+ The job advertisement text documents of a particular category are in the corresponding sub-folder.\n",
    "+ Each job advertisement document is a txt file named `Job_<ID>.txt`. It contains the title, the webindex (some will also have information on the company name, some might not), and the full description of the job advertisement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "\n",
    "# import code as a function\n",
    "from src.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ignore warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "# set desired matplotlib gloabal figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Webindex</th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>Tokenized Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Finance / Accounts Asst Bromley to ****k</td>\n",
       "      <td>68997528</td>\n",
       "      <td>First Recruitment Services</td>\n",
       "      <td>Accountant (partqualified) to **** p.a. South ...</td>\n",
       "      <td>['accountant', 'partqualified', 'south', 'east...</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fund Accountant  Hedge Fund</td>\n",
       "      <td>68063513</td>\n",
       "      <td>Austin Andrew Ltd</td>\n",
       "      <td>One of the leading Hedge Funds in London is cu...</td>\n",
       "      <td>['leading', 'hedge', 'funds', 'london', 'recru...</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deputy Home Manager</td>\n",
       "      <td>68700336</td>\n",
       "      <td>Caritas</td>\n",
       "      <td>An exciting opportunity has arisen to join an ...</td>\n",
       "      <td>['exciting', 'opportunity', 'arisen', 'join', ...</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Title  Webindex  \\\n",
       "0  Finance / Accounts Asst Bromley to ****k  68997528   \n",
       "1               Fund Accountant  Hedge Fund  68063513   \n",
       "2                       Deputy Home Manager  68700336   \n",
       "\n",
       "                      Company  \\\n",
       "0  First Recruitment Services   \n",
       "1           Austin Andrew Ltd   \n",
       "2                     Caritas   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Accountant (partqualified) to **** p.a. South ...   \n",
       "1  One of the leading Hedge Funds in London is cu...   \n",
       "2  An exciting opportunity has arisen to join an ...   \n",
       "\n",
       "                               Tokenized Description            Category  \n",
       "0  ['accountant', 'partqualified', 'south', 'east...  Accounting_Finance  \n",
       "1  ['leading', 'hedge', 'funds', 'london', 'recru...  Accounting_Finance  \n",
       "2  ['exciting', 'opportunity', 'arisen', 'join', ...  Healthcare_Nursing  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read job_ad.csv\n",
    "job_ad = pd.read_csv('job_ad.csv')\n",
    "\n",
    "# # get the description of the job ad\n",
    "# description = job_ad['Description']\n",
    "# # get the tokenized description of the job ad\n",
    "# tk_description = job_ad['Tokenized Description']\n",
    "webindex = job_ad['Webindex']\n",
    "\n",
    "\n",
    "# print first 3 rows\n",
    "job_ad.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptionFile = './description.txt'\n",
    "with open(descriptionFile) as f:\n",
    "    tk_description = f.read().splitlines() # read all the descriptions into a list\n",
    "    \n",
    "print(len(tk_description))\n",
    "type(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Converting each description text string into list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5218\n",
      "Total number of tokens:  102975\n",
      "Lexical diversity:  0.05067249332362224\n",
      "Total number of description: 776\n",
      "Average description length: 132.69974226804123\n",
      "Maximum description length: 471\n",
      "Minimum description length: 12\n",
      "Standard deviation of description length: 70.3782402519735\n"
     ]
    }
   ],
   "source": [
    "tk_description = [description.split(\" \") for description in tk_description] # note that we have to revert the join string into\n",
    "\n",
    "# Explore the current statistics\n",
    "stats_print(tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Reading the corresponding category labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the category of the job ad\n",
    "categoryFile = './category.txt'\n",
    "with open(categoryFile) as f:\n",
    "    category = f.read().splitlines() # read all the category into a list\n",
    "    \n",
    "print(len(category))\n",
    "type(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Making sure we done it right\n",
    "Take an example, e.g., the 10th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the category: 776\n",
      "The number of the description: 776\n",
      "The number of category of category and description are the same and corresponding to each other\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of the category: {len(category)}')\n",
    "print(f'The number of the description: {len(tk_description)}')\n",
    "if len(category) == len(tk_description):\n",
    "    print(f'The number of category of category and description are the same and corresponding to each other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create temporary variable and assign a number for testing at that index**\n",
    "\n",
    "`test_index` is a number to test whether the attribute at that position matches the desired outputs. So we don't need to print to whole lengthly output each test and void memory problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_index = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ----------------> OBSERVATION\n",
    "\n",
    "We can see the length of the description text and corresponding labels are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Webindex</th>\n",
       "      <th>Company</th>\n",
       "      <th>Description</th>\n",
       "      <th>Tokenized Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PERM Unit Mgr RGN Kid minster Flexi ****K due</td>\n",
       "      <td>71692209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Job Title: Unit Manager Reporting to: Register...</td>\n",
       "      <td>['job', 'title', 'unit', 'manager', 'reporting...</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Title  Webindex Company  \\\n",
       "20  PERM Unit Mgr RGN Kid minster Flexi ****K due  71692209     NaN   \n",
       "\n",
       "                                          Description  \\\n",
       "20  Job Title: Unit Manager Reporting to: Register...   \n",
       "\n",
       "                                Tokenized Description            Category  \n",
       "20  ['job', 'title', 'unit', 'manager', 'reporting...  Healthcare_Nursing  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = job_ad[job_ad['Webindex'] == 71692209]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    Job Title: Unit Manager Reporting to: Register...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert the loaded category labels to integers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Constructing the Vocabulary\n",
    "\n",
    "Now, we complete all the basic pre-process step and we are ready to move to feature generation! &#129321;\n",
    "Before we start, in this task, you are required to construct the final vocabulary, e.g., `vocab`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5218"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating the vocabulary\n",
    "\n",
    "words = list(chain.from_iterable(tk_description)) # we put all the tokens in the corpus in a single list\n",
    "vocab = sorted(list(set(words))) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "\n",
    "# total number of the vocabulary\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text cleaning function for gensim fastText word embeddings in python\n",
    "# def process_text(document):\n",
    "     \n",
    "#             # Remove extra white space from text\n",
    "#         document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "         \n",
    "#         # Remove all the special characters from text\n",
    "#         document = re.sub(r'\\W', ' ', str(document))\n",
    " \n",
    "#         # Remove all single characters from text\n",
    "#         document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    " \n",
    "#         # Converting to Lowercase\n",
    "#         document = document.lower()\n",
    " \n",
    "#         # Word tokenization       \n",
    "#         tokens = document.split()\n",
    "#         # Lemmatization using NLTK\n",
    "#         lemma_txt = [stemmer.lemmatize(word) for word in tokens]\n",
    "#         # Remove stop words\n",
    "#         lemma_no_stop_txt = [word for word in lemma_txt if word not in en_stop]\n",
    "#         # Drop words \n",
    "#         tokens = [word for word in tokens if len(word) > 3]\n",
    "                 \n",
    "#         clean_txt = ' '.join(lemma_no_stop_txt)\n",
    " \n",
    "#         return clean_txt\n",
    "\n",
    "\n",
    "\n",
    "# clean_corpus = [process_text(sentence) for sentence in tqdm(some_sent) if sentence.strip() !='']\n",
    " \n",
    "# word_tokenizer = nltk.WordPunctTokenizer()\n",
    "# word_tokens = [word_tokenizer.tokenize(sent) for sent in tqdm(clean_corpus)]\n",
    "# word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Task 2. Generating Feature Representations</strong></h3>\n",
    "\n",
    "Generate different types of feature representations for the collection of the job advertisements. Note that in this task, we will only consider the description of the job advertisement. The feature representation that you need to generate includes the following:\n",
    "\n",
    "* Bag-of-words model: Generate the Count vector representation for each job advertisement description, and save them into a file (please refer to the required output). Note, the generated Count vector representation must be based on the generated vocabulary in Task 1 (as saved in `vocab.txt`).\n",
    "\n",
    "* Models based on word embeddings: You are required to generate feature representation of job advertisement description based on the following language models, respectively:\n",
    ">> * Choose 1 embedding language model FastText\n",
    ">> * Build the weighted (use TF-IDF as the weight when you do weighted sum of word embeddings) \n",
    ">> * Build the unweighted vector representation for each job advertisement description using the chosen language model.\n",
    "\n",
    "So let's say we do binary feature representation but with 3 types of data, the title, the description, and title+description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\"\n",
    "Bag-of-words model:\n",
    "Generate the Count vector representation for each job advertisement description, and save\n",
    "them into a file (please refer to the required output). Note, the generated Count vector\n",
    "representation must be based on the generated vocabulary in Task 1 (as saved in vocab.txt).\n",
    "\"\"\"\n",
    "# bag of words model\n",
    "def bag_of_words(description, vocab):\n",
    "    # create a list of 0s with the same length as the vocab\n",
    "    bow = [0] * len(vocab)\n",
    "    # count the number of times each word appears in the description\n",
    "    word_counts = Counter(description)\n",
    "    # update the bow list with the word counts\n",
    "    for word, count in word_counts.items():\n",
    "        bow[vocab.index(word)] = count\n",
    "    return bow\n",
    "\n",
    "# Generate the Count vector representation for each job advertisement description\n",
    "bow = [bag_of_words(description, vocab) for description in tk_description]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 Saving outputs\n",
    "Save the count vector representation as per spectification.\n",
    "- `count_vectors.txt`\n",
    "\n",
    "`count_vectors.txt` stores the sparse count vector representation of job advertisement descriptions in the following format. Each line of this file corresponds to one advertisement. It starts with a â€˜#â€™ key followed by the webindex of the job advertisement, and a comma â€˜,â€™. The rest of the line is the sparse representation of the corresponding description in the form of word_integer_index:word_freq separated by comma. Following is an example of the file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully write count vector representation of job advertisement descriptions into count_vectors.txt file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save count vector representation of job advertisement descriptions\n",
    "with open('count_vectors.txt', 'w') as f:\n",
    "    for i, description in enumerate(tk_description):\n",
    "        f.write('#' + str(webindex[i]) + ',')\n",
    "        for word in description:\n",
    "            f.write(str(vocab.index(word)) + ':' + str(bow[i][vocab.index(word)]) + ',')\n",
    "        f.write('\\n')\n",
    "    print(\"Successfully write count vector representation of job advertisement descriptions into count_vectors.txt file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Vector Representation\n",
    "\n",
    "After text pre-processing has been completed, each individual document needs to be transformed into \n",
    "some kind of numeric representation that can be input into most NLP and text mining algorithms.\n",
    "For example, classification algorithms, such as Support Vector Machine, can only take data in a \n",
    "structured and numerical form. They do not accept free language text.\n",
    "A popular structured representation of text is the vector-space model, which represents each text/article\n",
    "as a vector where the elements of the vector indicate the occurence of words within the text.\n",
    "\n",
    "The vector-space model makes an implicit assumption that \n",
    "the order of words in a text document are not as\n",
    "important as words themselves, and thus disregarded.\n",
    "This assumpiton is called [**Bag-of-words**](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "Given a set of documents and a pre-defined list of words appearing \n",
    "in those documents (i.e. a vocabulary), we can compute a vector representation for each document.\n",
    "This vector representation can take one of the following three forms:\n",
    "* a binary representation, each entry is either `word:0` (the word does not appear in the document; or `word:1` (the word appears in the document). We call this **binary vector representation**. \n",
    "* an integer count, each entry is `word:count`, telling how many times a word appear in a document. We call this **count vector representation**. \n",
    "* and a float-valued weighted vector, each entry is `word:weight`, telling a **weighted representative importance** of a word to a document. One of the most common weighted vectors used in natural language processing is called the *tfidf* vector. \n",
    "\n",
    "Given the cleaned up BBC News articles, how can we generate those vectors for each document? \n",
    "\n",
    "Unfortunately, NLTK does not implement methods that directly produce those vectors.\n",
    "Therefore, we will either write our own code to compute them or appeal to other data analysis libraries.\n",
    "\n",
    "Here we are going to use [scikit-learn](http://scikit-learn.org/stable/index.html), an open source machine \n",
    "learning library for Python.\n",
    "If you use Anaconda, you should already have scikit-learn installed, otherwise you will need to \n",
    "[install it](http://scikit-learn.org/stable/install.html) by following the instruction on its official website.\n",
    "\n",
    "Although scikit-learn features various classification, regression and clustering algorithms\n",
    "we are particularly interested in its feature extraction module, [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction).\n",
    "This module is often used to \"extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\" Please refer to its documentation on text feature extraction,\n",
    "Section 6.2.3 of [Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). \n",
    "\n",
    "In the following, we will demonstrate the usage of the following two classes:\n",
    "* [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer): It converts a collection of text documents to a matrix of token counts. \n",
    "* [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "It converts a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "\n",
    "### 5.3 Generating TF-IDF Vectors\n",
    "\n",
    "Finally, we will generate the TF-IDF Vector to represent each of the document.\n",
    "\n",
    "Similar to the use of `CountVector`, we first initialise a `TfidfVectorizer` object by only specifying the value of \"analyzer\" and the vocabulary, and then covert the BBC News articles into a list of strings, each of which corresponds\n",
    "to a BBC News article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5218)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab) # initialised the TfidfVectorizer\n",
    "tfidf_features = tVectorizer.fit_transform([' '.join(article) for article in tk_description]) # generate the tfidf vector representation for all articles\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<776x5218 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 75446 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print out the weighted vector for the example document.\n",
    "# validator(tfidf_features,vocab,test_ind, article_ids,article_txts,tk_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Saving the Vector Representation\n",
    "\n",
    "Given the vocabulary, each document can be represented as a sequence of entries that correspond to the tokens, or in the following sparse form:\n",
    "```\n",
    "    word_index:word_count\n",
    "```\n",
    "where the `word_count` is either 0 or 1 or binary vector, and an integer value for count vector; or \n",
    "```\n",
    "    word_index:weight\n",
    "```\n",
    "for the TF-IDF vector. \n",
    "In the following, we save the binary, count and tfidf vector representation of the documents, respectively. \n",
    "Each of the following block of codes loops through each document, and each token appears in the document (retrived by the index `f_ind`. \n",
    "As the data features is of the dimension of the size of the vocab, but there are only a limited number of words appear in a document, we retrieve the index of the non-zero entry of the data features by calling the `nonzero()` function:\n",
    "* the [`nonzero()`](https://numpy.org/doc/stable/reference/generated/numpy.nonzero.html) function return the indices of the elements that are non-zero. These indices are returned as a tuple of arrays, one for each dimension of the matrix, containing the indices of the non-zero elements in that dimension. \n",
    "\n",
    "Note that here `data_features[a_ind]` returns a (1 x vocabSzie) sparse matrix of type '<class 'numpy.int64'>'. \n",
    "Therefore, the return from \n",
    "```python\n",
    "binary_features[a_ind].nonzero()\n",
    "```\n",
    "is a tuple of two arrays, the first array (indexed 0) is the indices of non-zero elements in the row dimension, and the second array (indexed 1) is the indicies of the non-zero elements in the column dimension. \n",
    "Here we need to take the column dimension (indexed of a word in the vocabulary that appear in the document), therefore we take the 2nd array (retrieved by index 1). \n",
    "Note also that the element of the 1st array is always 0, as mentioned before, the dimension of the matrix is 1 times the size of vocabulary. \n",
    "\n",
    "For each word index that has a non-zero entry, one could then retrieve the feature value by indexing, for example:\n",
    "```python\n",
    "count_features[a_ind][0,f_ind]\n",
    "```\n",
    "this retrieves the frequency count of word (indexed `f_ind`) in the document `count_features[a_ind]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved ./jobAd_tVector.txt into the directory\n"
     ]
    }
   ],
   "source": [
    "# Saving the Vector Representation\n",
    "def write_vectorFile(data_features,filename):\n",
    "    num = data_features.shape[0] # the number of document\n",
    "    out_file = open(filename, 'w') # creates a txt file and open to save the vector representation\n",
    "    for a_ind in range(0, num): # loop through each article by index\n",
    "        for f_ind in data_features[a_ind].nonzero()[1]: # for each word index that has non-zero entry in the data_feature\n",
    "            value = data_features[a_ind][0,f_ind] # retrieve the value of the entry from data_features\n",
    "            out_file.write(\"{}:{} \".format(f_ind,value)) # write the entry to the file in the format of word_index:value\n",
    "        out_file.write('\\n') # start a new line after each article\n",
    "    out_file.close() # close the file\n",
    "    \n",
    "tVector_file = \"./jobAd_tVector.txt\" # file name of the tfidf vector\n",
    "\n",
    "write_vectorFile(tfidf_features,tVector_file) # write the tfidf vector to file\n",
    "print(f'Successfully saved {tVector_file} into the directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Saving the Article IDs\n",
    "\n",
    "Oh sorry, one last thing..... we should also save the article IDs accordingly, so that we can easily retrieve the labels of the article in downstream analysis. \n",
    "\n",
    "A very important note that the Article ID is different from the article index here. \n",
    "The Article ID is given from the original dataset, as an identifier of an article. \n",
    "The article index we were talking about in a couple of places in this Jupyter Notebook is related to how we read and preprocess the data. \n",
    "Remember, at the beginning, we have read all the articles into a list `article_txts`, so the index here refers to the index of the article in the list  `article_txts`. \n",
    "\n",
    "Indeed, in this example, since the article IDs are integers, we should have make this more consistent (just in case...). \n",
    "This could be simply done by setting an order when we read the articles. \n",
    "The following code that we use before to loop through each of the article .txt file\n",
    "```python\n",
    "for filename in os.listdir(dir_path):\n",
    "``` \n",
    "can be changed to: \n",
    "```python\n",
    "for filename in sorted(os.listdir(dir_path)):\n",
    "``` \n",
    "\n",
    "as such, we visit the article txt files in a sorted order according to their filename. &#128578; We will leave this for you to try and experience the difference. \n",
    "\n",
    "Though, in many other context, we might not have that much luck with the file naming :) Therefore, we should always learn to save the indexing we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_path = \"./articles\"\n",
    "# article_ids = [] # list to store the article ID\n",
    "# article_txts = [] # list to store the raw text\n",
    "\n",
    "out_file = open(\"./webindex.txt\", 'w') # creates a txt file named 'bbcNews_articleIDs.txt' to save the tfidf vector\n",
    "for a_ind in range(0, len(tk_description)):\n",
    "    out_file.write(\"{}\\n\".format(webindex[a_ind])) # write the article ID of each article\n",
    "out_file.close() # close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Task 2. Generating Feature Representations</strong></h3>\n",
    "\n",
    "So let's say we do binary feature representation but with 3 types of data, the title, the description, and title+description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 5. Generating Feature Vectors\n",
    "\n",
    "In this task, we are going to generate feature vectors from tokenized review text. We are going to explore different feature vectors, including binary, count, and tf-idf vectors.\n",
    "\n",
    "Binary, Count, TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5.1 Generating Binary Vectors\n",
    "In this subtask, let's start with generating the binary vector representation for each review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to first import the `CountVectorizer` and initialise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# binding the words together for each review\n",
    "joined_description = [' '.join(review) for review in tk_description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bVectorizer = CountVectorizer(analyzer = \"word\",binary = True,vocabulary = vocab) # initialise the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5218)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_features = bVectorizer.fit_transform(joined_description)\n",
    "binary_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved models/BinaryVectors/jobAd_bVector.txt into the directory\n"
     ]
    }
   ],
   "source": [
    "bVector_file = \"models/BinaryVectors/jobAd_bVector.txt\" # file name of the tfidf vector\n",
    "\n",
    "write_vectorFile(binary_features,bVector_file) # write the tfidf vector to file\n",
    "print(f'Successfully saved {bVector_file} into the directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generating Count Vectors\n",
    "\n",
    "Each word coresponding to a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5218)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cVectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab) # initialised the CountVectorizer\n",
    "count_features = cVectorizer.fit_transform(joined_description)\n",
    "count_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aat</th>\n",
       "      <th>abb</th>\n",
       "      <th>abenefit</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abi</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>abreast</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>yeovil</th>\n",
       "      <th>yn</th>\n",
       "      <th>york</th>\n",
       "      <th>yorkshire</th>\n",
       "      <th>youmust</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 5218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aap  aaron  aat  abb  abenefit  aberdeen  abi  abilities  ability  \\\n",
       "61     0      0    0    0         0         0    0          0        0   \n",
       "769    0      0    0    0         0         0    0          0        0   \n",
       "673    0      0    0    0         0         0    0          0        0   \n",
       "\n",
       "     abreast  ...  years  yeovil  yn  york  yorkshire  youmust  young  \\\n",
       "61         0  ...      0       0   0     0          0        0      0   \n",
       "769        0  ...      1       0   0     0          0        0      0   \n",
       "673        0  ...      0       0   0     0          0        0      0   \n",
       "\n",
       "     younger  yrs  zest  \n",
       "61         0    0     0  \n",
       "769        0    0     0  \n",
       "673        0    0     0  \n",
       "\n",
       "[3 rows x 5218 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_array = count_features.toarray()\n",
    "\n",
    "# get_feature_names_out return the vocabulary of unique words\n",
    "df = pd.DataFrame(data=count_array,columns = cVectorizer.get_feature_names_out())\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved models/CountVectors/jobAd_cVector.txt into the directory\n"
     ]
    }
   ],
   "source": [
    "cVector_file = \"models/CountVectors/jobAd_cVector.txt\" # file name of the tfidf vector\n",
    "\n",
    "write_vectorFile(count_features,cVector_file) # write the tfidf vector to file\n",
    "print(f'Successfully saved {cVector_file} into the directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5.3 Generating TF-IDF Vectors\n",
    "\n",
    "![](media/images/td-idf-graphic.png)\n",
    "Source: http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html\n",
    "\n",
    "In this subtasks, you are required to generate the count vector features of review texts.\n",
    "\n",
    "\n",
    "\n",
    "TF-IDF also gives larger values for less frequent words and is high when both IDF and TF values are high i.e the word is rare in all the documents combined but frequent in a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5218)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab) # initialised the TfidfVectorizer\n",
    "tfidf_features = tVectorizer.fit_transform(joined_description) # generate the tfidf vector representation for all articles\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved models/TfidfVectors/jobAd_tVector.txt into the directory\n"
     ]
    }
   ],
   "source": [
    "tVector_file = \"models/TfidfVectors/jobAd_tVector.txt\" # file name of the tfidf vector\n",
    "\n",
    "write_vectorFile(tfidf_features,tVector_file) # write the tfidf vector to file\n",
    "print(f'Successfully saved {tVector_file} into the directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating TF-IDF weighted document vectors\n",
    "\n",
    "Ok, I hope you have lots of fun building document embeddings based on varoius word embedding models. \n",
    "Previously, when we generate the document embeddings, we just sum up the embeddings vector of each tokenized word in the article, a bit simplicity ðŸ¤”\n",
    "\n",
    "In this section, let's make it a bit more challenging, we are going to build the tf-idf document embeddings. \n",
    "What does that mean? ðŸ¤¨\n",
    "Hmm~~ it's not magic, we just do a weigthed sum of the word embedding vectors, however, the weight here, refers to the tf-idf weight of the word. \n",
    "\n",
    "If you already forgot about what is `tf-idf`, please refer to Activity 3 Pre-processing Text and Generating Features. \n",
    "Otherwise, move on!\n",
    "So we've generated the tf-idf vector representation of documents in Activity 3 and saved in a txt file called `bbcNews_tVector.txt`. The format of this file is:\n",
    "- each line represents an article;\n",
    "- each line is of the format 'w_index:weight w_index:weight ......' \n",
    "\n",
    "Oh, but we don't have that word index `w_index` here in this activity, what should we do? ðŸ¤”\n",
    "ah ha, we also saved the vocabulary in a file `bbcNews_voc`, in which each line is a word, and of the format `index,word`. \n",
    "Theresore, based on these two files, we can create a word:weight mapping for each tokenized word in a document!\n",
    "\n",
    "Ok, in the following couple block of codes, this is exactly what we are trying to do, step by step. \n",
    "- the `gen_vocIndex` function reads the the vocabulary file, and create an w_index:word dictionary\n",
    "\n",
    "\n",
    "Term frequency (TF): number of times a term has appeared in a document.\n",
    "\n",
    "The term frequency is a measure of how frequently or how common a word is for a given sentence.\n",
    "\n",
    "Inverse Document Frequency (IDF):\n",
    "\n",
    "The inverse document frequency (IDF ) is a measure of how rare a word is in a document. Words like â€œtheâ€,â€ aâ€ show up in all the documents but rare words will not occur in all the documents of the corpus.\n",
    "\n",
    "If a word appears in almost every document means itâ€™s not significant for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(vocab_file):\n",
    "    vocab = {}\n",
    "    with open(vocab_file) as f:\n",
    "        for line in f:\n",
    "            (word, index) = line.split(':')\n",
    "            vocab[word.strip()] = int(index)\n",
    "    return vocab\n",
    "\n",
    "# Generates the w_index:word dictionary\n",
    "voc_fname = 'vocab.txt'\n",
    "voc_dict = read_vocab(voc_fname)\n",
    "voc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------> OBSERVATION\n",
    "\n",
    "- the `doc_wordweights` function takes the tfidf document vector file, as well as the `word_string:word_integer_index` dictionary, creates the mapping between word_integer_index and the actual word, and creates a dictionary of word:weight or each unique word appear in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_wordweights(fName_tVectors, voc_dict):\n",
    "    tfidf_weights = [] # a list to store the  word:weight dictionaries of documents\n",
    "    \n",
    "    with open(fName_tVectors) as tVecf: \n",
    "        tVectors = tVecf.read().splitlines() # each line is a tfidf vector representation of a document in string format 'word_index:weight word_index:weight .......'\n",
    "    for tv in tVectors: # for each tfidf document vector\n",
    "        tv = tv.strip()\n",
    "        weights = tv.split(' ') # list of 'word_index:weight' entries\n",
    "        weights = [w.split(':') for w in weights] # change the format of weight to a list of '[word_index,weight]' entries\n",
    "        wordweight_dict = {voc_dict[int(w[0])]:w[1] for w in weights} # construct the weight dictionary, where each entry is 'word:weight'\n",
    "        tfidf_weights.append(wordweight_dict) \n",
    "    return tfidf_weights\n",
    "\n",
    "fName_tVectors = 'jobAd_tVector.txt'\n",
    "tfidf_weights = doc_wordweights(fName_tVectors, voc_dict)\n",
    "\n",
    "# take a look at the tfidf word weights dictionary of the first document\n",
    "tfidf_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ------------------> OBSERVATION:\n",
    "\n",
    "Ok, once we have the word:weight dictionary of each document, now we can construct the tf-idf weighted document embeddings. \n",
    "* the following `gen_docVecs` function is an revision/extension of the previous written function, that takes the word embeddings dictionary, the tokenized text of articles, and the tfidf weights (list of word:weight dictionaries, one for each article) as arguments, and generates the document embeddings:\n",
    " 1. creates an empty dataframe `docs_vectors` to store the document embeddings of articles\n",
    "  2. it loop through every tokenized text:\n",
    "    - creates an empty dataframe `temp` to store all the word embeddings of the article\n",
    "    - for each word that exists in the word embeddings dictionary/keyedvectors, \n",
    "        - if the argument `tfidf` weights are empty `[]`, it sets the weight of the word as 1\n",
    "        - otherwise, retrieve the weight of the word from the corresponding word:weight dictionary of the article from  `tfidf`\n",
    "    - row bind the weighted word embedding to `temp`\n",
    "    - takes the sum of each column to create the document vector, i.e., the embedding of an article\n",
    "    - append the created document vector to the list of document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated the weighted version of the document embedding vectors first\n",
    "weighted_bbcFT_dvs = gen_docVecs(bbcFT_wv,df['tk_description'],tfidf_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do very much the same thing as what we do before for other models. \n",
    "Here, we will do this as loops, for each model:\n",
    "- we plot out the feature vectors  projected in a 2-dimensional space,then \n",
    "- we build the logistic regression model for document classfication and report the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "seed = 0\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = [weighted_preTW2v_dvs,weighted_preTGloVe_dvs,weighted_bbcW2v_dvs,weighted_bbcFT_dvs]\n",
    "model_names = [\"Weighted Pretrained Word2Vec\", \"Weighted Pretrained GloVe\", \"Weighted In-house Word2Vec\",\"Weighted In-house FastText\"]\n",
    "for i in range(0,len(models)): #loop through each model\n",
    "    dv = models[i]\n",
    "    name = model_names[i]\n",
    "    features = dv.to_numpy() # convert the dataframe stored features to an numpy array\n",
    "    print(name + \": tSNE 2 dimensional projected Feature space\")\n",
    "    plotTSNE(df['Category'],features)\n",
    "    \n",
    "    # creating training and test split\n",
    "    X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(dv, df['Category'], list(range(0,len(df))),test_size=0.33, random_state=seed)\n",
    "\n",
    "    model = LogisticRegression(max_iter = 2000,random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Accuracy: \", model.score(X_test, y_test))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR\n",
    "\n",
    "#### Note: Creating tfidf weighted document embeddings using Gensim\n",
    "\n",
    "In the previous sections, we tried very hard to create the tfidf weighted document embeddings using the generated tf-idf weights save in previous activity. \n",
    "Indeed, we can using Genism to do this direction, and it's indeed, a bit less effor required ðŸ˜‘ Will show you below. \n",
    "We will use the in-house build Word2Vec model as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ad.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# we have two vocabularies here, one from the in-house bulit Word2Vec, the other from the articles \n",
    "# note that althought the Word2Vec is built on the same dataset, but they might have done further \n",
    "# pre-processing during model build (e.g., setting min_count), and thus, might create mismatch in the two vocabularis. \n",
    "# therefore, we remove tokenized words that doesn't exist in the keyedvectors in the Word2Vec keyedvectors\n",
    "processed_text = [[w for w in t if w in bbcFT_wv.index_to_key] for t in job_ad['Tokenized Description']] \n",
    "\n",
    "# use the Gensim package to create a dictionary that encapsulates the mapping between normalized words and their integer ids.\n",
    "docs_dict = Dictionary(processed_text) # creates a dictionary from the text\n",
    "docs_dict.filter_extremes(no_below=5) # filtering words that appear less than 5 times\n",
    "docs_dict.compactify() # assign new word ids to all words, shrinking any gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what are the words that been get rid off when we do the fliter\n",
    "[w for w in bbcFT_wv.index_to_key if w not in docs_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.matutils import sparse2full\n",
    "\n",
    "docs_corpus = [docs_dict.doc2bow(doc) for doc in job_ad['Tokenized Description']] # convert corpus to Bag of Word format\n",
    "model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict) # fit the tfidf model\n",
    "# apply model to the list of corpus document, \n",
    "# so each document is a list of tuples, (word_index, weight) for each word appears in the document\n",
    "docs_tfidf  = model_tfidf[docs_corpus]\n",
    "\n",
    "# see for example, the tfidf weights of the words in the 2nd document\n",
    "docs_tfidf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted sum, ou can do a weighted sum of word embeddings as representation instead of just sum.\n",
    "def create_weighted_embedding_models(X):\n",
    "    global weighted_features\n",
    "    weighted_features = []\n",
    "    for i in range(len(X)):\n",
    "        weighted_features.append(np.average(embedding_matrix[X[i]], axis=0, weights=weights[i]))\n",
    "\n",
    "\n",
    "# create logistic regression with unweighted embedding representation\n",
    "def create_unweighted_embedding_models(X):\n",
    "    global unweighted_features\n",
    "    unweighted_features = []\n",
    "    for i in range(len(X)):\n",
    "        unweighted_features.append(np.sum(embedding_matrix[X[i]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended version of the `gen_docVecs` function\n",
    "def gen_docVecs(wv,tk_txts,tfidf = []): # generate vector representation for documents\n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    #stopwords = nltk.corpus.stopwords.words('english') # removing stop words\n",
    "\n",
    "    for i in range(0,len(tk_txts)):\n",
    "        tokens = list(set(tk_txts[i])) # get the list of distinct words of the document\n",
    "\n",
    "        temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        for w_ind in range(0, len(tokens)): # looping through each word of a single document and spliting through space\n",
    "            try:\n",
    "                word = tokens[w_ind]\n",
    "                word_vec = wv[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                \n",
    "                if tfidf != []:\n",
    "                    word_weight = float(tfidf[i][word])\n",
    "                else:\n",
    "                    word_weight = 1\n",
    "                temp = temp.concat(pd.Series(word_vec*word_weight), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                pass\n",
    "        doc_vector = temp.sum() # take the sum of each column(w0, w1, w2,........w300)\n",
    "        docs_vectors = docs_vectors.concat(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "    return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save count vector representation of job advertisement descriptions\n",
    "with open('count_vectors.txt', 'w') as f:\n",
    "    for i, description in enumerate(tk_description):\n",
    "        f.write('#' + str(webindex[i]) + ',')\n",
    "        for word in description:\n",
    "            f.write(str(vocab.index(word)) + ':' + str(bow[i][vocab.index(word)]) + ',')\n",
    "        f.write('\\n')\n",
    "    print(\"Successfully write count vector representation of job advertisement descriptions into count_vectors.txt file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 6. Training Logistic Regression Models for label Classification\n",
    "\n",
    "In this final task, you are required to bulid logistic regression models based on different feature vectors (binary, count and tfidf vectors), explore the cases where the model produced false predictions, and evaluate the performance of the model using a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the following, we first uses count vector features as an example to bulid a logistic regression model and  explore the preformance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seed = 3879312\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(count_features, category, list(range(0,len(category))),test_size=0.2, random_state=seed)\n",
    "\n",
    "max_iter = 1000 # this is a relative harder problem and we have to increase the maximum iteration parameter of the logistic regression model.\n",
    "\n",
    "model = LogisticRegression(random_state=seed,max_iter=max_iter, multi_class='multinomial') # initial a logistic regression model\n",
    "model.fit(X_train, y_train) # fit the model\n",
    "model.score(X_test, y_test) # calculated the accuracy score on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales'] # this gives sorted set of unique label names\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=categories, yticklabels=categories, cmap=\"PiYG\") # creates a heatmap from the confusion matrix\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's explore some mis-classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_test) # instead of getting the direct prediction, i.e., a label, we can also get the probability distributions of labels\n",
    "\n",
    "for p_ind in range(0, 2):\n",
    "    for a_ind in range(0, 2):\n",
    "        if p_ind !=  a_ind: # when it mis-classifies\n",
    "            print(\"'{}' predicted as '{}' : {} examples.\".format(categories[a_ind], categories[p_ind],\\\n",
    "                                                                 conf_mat[a_ind, p_ind]))\n",
    "            print(\"====================================================\")\n",
    "\n",
    "            # retrieve the indices for the mis-classification\n",
    "            mis_inds = [test_indices[i] for i in range(0,len(y_test)) if \\\n",
    "                        y_pred[i] == p_ind and y_test[i] == a_ind]\n",
    "            #print out the article ID and the tokenised text content of the mis-classified examples\n",
    "            for ind in random.sample(mis_inds,2): # explore 2 examples\n",
    "                print(\"------------------------------------------------\")\n",
    "                print(joined_description[ind])\n",
    "                print(\"-----------------------------------------------\\n\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits= num_folds, random_state=seed, shuffle = True) # initialise a 5 fold validation\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(X_train,X_test,y_train, y_test,seed):\n",
    "    model = LogisticRegression(random_state=seed,max_iter = 1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_models = 3\n",
    "cv_df = pd.DataFrame(columns = ['binary','count','tfidf'],index=range(num_folds)) # creates a dataframe to store the accuracy scores in all the folds\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(list(range(0,len(category)))):\n",
    "    y_train = [str(category[i]) for i in train_index]\n",
    "    y_test = [str(category[i]) for i in test_index]\n",
    "\n",
    "    X_train_binary, X_test_binary = binary_features[train_index], binary_features[test_index]\n",
    "    cv_df.loc[fold,'binary'] = evaluate(binary_features[train_index],binary_features[test_index],y_train,y_test,seed)\n",
    "\n",
    "    X_train_count, X_test_count = count_features[train_index], count_features[test_index]\n",
    "    cv_df.loc[fold,'count'] = evaluate(count_features[train_index],count_features[test_index],y_train,y_test,seed)\n",
    "\n",
    "    X_train_tfidf, X_test_tfidf = tfidf_features[train_index], tfidf_features[test_index]\n",
    "    cv_df.loc[fold,'tfidf'] = evaluate(tfidf_features[train_index],tfidf_features[test_index],y_train,y_test,seed)\n",
    "\n",
    "    fold +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Printing the result of each fold for each vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cv_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Task 3. Job Advertisement Classification</strong></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "...... Sections and code blocks on buidling classification models based on different document feature represetations. \n",
    "Detailed comparsions and evaluations on different models to answer each question as per specification. \n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastText model trained on BBC News data \n",
    "\n",
    "trained our `Word2Vec` and `FastText` models using our BBC News dataset\n",
    "\n",
    "## 3. FastText\n",
    "\n",
    "As mentioned before, `word2vec` model does not accommodate words that do not appear in the training corpus. \n",
    "\n",
    "Here, weâ€™ll learn to work with the fastText library for training word-embedding models, and performing similarity operations & vector lookups analogous to Word2Vec. \n",
    "\n",
    "In the following block of code, we import the `FastText` model form Gensim library, then:\n",
    "1. We set the path to the corpus file. Similar as above, we use the Bbc News article as the training corpus;\n",
    "2. Initialise the `FastText` model, similar as before, we use 100 dimention vectors;\n",
    "3. Then we build the vocabulary from the copurs;\n",
    "4. Finally, we train the fasttext model based on the corpus.\n",
    "\n",
    "Finally, we experiment the FastText embeddings. \n",
    "Similar, we:\n",
    "* load the FastText model saved in our prevoius activity;\n",
    "* generate document embeddings based on the load FastText word embeddings;\n",
    "* explore the reprensentiveness of the features through tSNE;\n",
    "* bulid the logistic regression model based on the generated document embeddings for news classfication.\n",
    "\n",
    "T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin. Advances in Pre-Training Distributed Word Representations\n",
    "\n",
    "\n",
    "There are multiple pre-trained models in Gensim, see Section **Pretrained models** in https://radimrehurek.com/gensim/models/word2vec.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.fasttext import FastText\n",
    "\n",
    "# # 1. Set the corpus file names/path\n",
    "# corpus_file = './bbcNews.txt'\n",
    "\n",
    "# # 2. Initialise the Fast Text model\n",
    "# bbcFT = FastText(vector_size=100) \n",
    "\n",
    "# # 3. build the vocabulary\n",
    "# bbcFT.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# # 4. train the model\n",
    "# bbcFT.train(\n",
    "#     corpus_file=corpus_file, epochs=bbcFT.epochs,\n",
    "#     total_examples=bbcFT.corpus_count, total_words=bbcFT.corpus_total_words,\n",
    "# )\n",
    "\n",
    "# print(bbcFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can retrieve the KeyedVectors from the model as follows,\n",
    "\n",
    "# bbcFT_wv = bbcFT.wv\n",
    "# print(bbcFT_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# bbcFT.save(\"bbcFT.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained Fasttext model based on bbc News data\n",
    "from gensim.models.fasttext import FastText\n",
    "bbcFT = FastText.load(\"bbcFT.model\")\n",
    "print(bbcFT)\n",
    "bbcFT_wv= bbcFT.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ad.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this can take some time to finish running\n",
    "# generate document embeddings\n",
    "bbcFT_dvs = gen_docVecs(bbcFT_wv,job_ad['Tokenized Description'])\n",
    "bbcFT_dvs.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbcFT_dvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ----------> OBSERVATION\n",
    "\n",
    "0 null record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Understand your task by tSNE\n",
    "\n",
    "Alright! so we have the document embedding vector representation for each article now, we can proceed to the task of document classification. \n",
    "Before, we move on, a good habbit is to explore and understand how difficult the task is, whether there are too much noise in the data, making it impossible to clearly separate each category. \n",
    "\n",
    "One way to confirm that the feature space we are using is representative enough for our task (classifying articles into separate labels) to be solvable is to use dimensionality-reduction techniques: These methods project a high-dimensional vector into a lower number of dimensions, with different guarantees on this projection according to the method used. \n",
    "In this activity, we will use [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), a popular dimensionality reduction technique used in many fields, including NLP.\n",
    "\n",
    "Since we will do the same thing again and again when we try other embeddings, we will construct a function to do this again.\n",
    "The following function `plotTSNE` takes the following arugments:\n",
    "* labels, the lable/category of each article\n",
    "* features, a numpy array of document embeddings, each for an article.\n",
    "\n",
    "and projects the feature/document embedding vectors in a 2 dimension space and plot them out. \n",
    "It does the following:\n",
    "1. get the set of classes, called `categories` (5 categories)\n",
    "2. sample 30% of the data/document embeddings randomly, and record the indices selected\n",
    "3. project the selected document embeddings in 2 dimensional space using tSNE, each document embedding now corresponds to a 2 dimensional vector in `projected_features`\n",
    "4. plot them out as scatter plot and highlight different categories in different color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plotTSNE(labels,features): # features as a numpy array, each element of the array is the document embedding of an article\n",
    "    categories = sorted(labels.unique())\n",
    "    # Sampling a subset of our dataset because t-SNE is computationally expensive\n",
    "    SAMPLE_SIZE = int(len(features) * 0.3)\n",
    "    np.random.seed(0)\n",
    "    indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)\n",
    "    projected_features = TSNE(n_components=2, random_state=0).fit_transform(features[indices])\n",
    "    colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']\n",
    "    for i in range(0,len(categories)):\n",
    "        points = projected_features[(labels[indices] == categories[i])]\n",
    "        plt.scatter(points[:, 0], points[:, 1], s=30, c=colors[i], label=categories[i])\n",
    "    plt.title(\"Feature vector for each article, projected on 2 dimensions.\",\n",
    "              fontdict=dict(fontsize=15))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# # explore feature space\n",
    "# features = bbcFT_dvs.to_numpy() # convert the document vector dataframe to a numpy array\n",
    "# plotTSNE(job_ad['Category'],features) # plot the tSNE to have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classfication model and report results\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(bbcFT_dvs, job_ad['Category'], list(range(0,len(job_ad))),test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000,random_state=seed)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FasText\n",
    "\n",
    "astText does significantly better on syntactic tasks as compared to the original Word2Vec, especially when the size of the training corpus is small. Word2Vec slightly outperforms fastText on semantic tasks though. - https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    " \n",
    "import nltk\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    " \n",
    "from gensim.models.fasttext import FastText\n",
    " \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 11:46:02,437 : INFO : collecting all words and their counts\n",
      "2022-10-02 11:46:02,438 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-10-02 11:46:02,460 : INFO : collected 5218 word types from a corpus of 102975 raw words and 776 sentences\n",
      "2022-10-02 11:46:02,460 : INFO : Creating a fresh vocabulary\n",
      "2022-10-02 11:46:02,471 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 retains 2791 unique words (53.487926408585665%% of original 5218, drops 2427)', 'datetime': '2022-10-02T11:46:02.471426', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-10-02 11:46:02,472 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 leaves 96494 word corpus (93.70623937848993%% of original 102975, drops 6481)', 'datetime': '2022-10-02T11:46:02.472232', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-10-02 11:46:02,483 : INFO : deleting the raw counts dictionary of 5218 items\n",
      "2022-10-02 11:46:02,485 : INFO : sample=0.01 downsamples 0 most-common words\n",
      "2022-10-02 11:46:02,485 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 96494 word corpus (100.0%% of prior 96494)', 'datetime': '2022-10-02T11:46:02.485905', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-10-02 11:46:02,526 : INFO : estimated required memory for 2791 words, 2000000 buckets and 300 dimensions: 2408642628 bytes\n",
      "2022-10-02 11:46:02,527 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs\n",
      "Wall time: 16.2 Âµs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 11:46:06,455 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-10-02T11:46:06.455646', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-10-02 11:46:06,456 : INFO : FastText lifecycle event {'msg': 'training model with 4 workers on 2791 vocabulary and 300 features, using sg=1 hs=0 sample=0.01 negative=5 window=5 shrink_windows=True', 'datetime': '2022-10-02T11:46:06.456922', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-10-02 11:46:07,175 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:07,263 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:07,397 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:07,399 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:07,399 : INFO : EPOCH - 1 : training on 102975 raw words (96494 effective words) took 0.9s, 103061 effective words/s\n",
      "2022-10-02 11:46:07,958 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:08,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:08,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:08,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:08,239 : INFO : EPOCH - 2 : training on 102975 raw words (96494 effective words) took 0.8s, 115041 effective words/s\n",
      "2022-10-02 11:46:08,732 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:08,824 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:08,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:08,955 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:08,955 : INFO : EPOCH - 3 : training on 102975 raw words (96494 effective words) took 0.7s, 135252 effective words/s\n",
      "2022-10-02 11:46:09,451 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:09,544 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:09,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:09,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:09,673 : INFO : EPOCH - 4 : training on 102975 raw words (96494 effective words) took 0.7s, 134825 effective words/s\n",
      "2022-10-02 11:46:10,163 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:10,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:10,402 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:10,406 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:10,406 : INFO : EPOCH - 5 : training on 102975 raw words (96494 effective words) took 0.7s, 131905 effective words/s\n",
      "2022-10-02 11:46:10,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:10,997 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:11,125 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:11,133 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:11,133 : INFO : EPOCH - 6 : training on 102975 raw words (96494 effective words) took 0.7s, 133080 effective words/s\n",
      "2022-10-02 11:46:11,682 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:11,784 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:11,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:11,931 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:11,932 : INFO : EPOCH - 7 : training on 102975 raw words (96494 effective words) took 0.8s, 121051 effective words/s\n",
      "2022-10-02 11:46:12,620 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:12,710 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:12,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:12,832 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:12,832 : INFO : EPOCH - 8 : training on 102975 raw words (96494 effective words) took 0.9s, 107366 effective words/s\n",
      "2022-10-02 11:46:13,382 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:13,467 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:13,589 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:13,594 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:13,595 : INFO : EPOCH - 9 : training on 102975 raw words (96494 effective words) took 0.8s, 126953 effective words/s\n",
      "2022-10-02 11:46:14,140 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:14,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:14,387 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:14,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:14,390 : INFO : EPOCH - 10 : training on 102975 raw words (96494 effective words) took 0.8s, 121993 effective words/s\n",
      "2022-10-02 11:46:14,943 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:15,038 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:15,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:15,162 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:15,163 : INFO : EPOCH - 11 : training on 102975 raw words (96494 effective words) took 0.8s, 125617 effective words/s\n",
      "2022-10-02 11:46:15,683 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:15,780 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:15,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:15,902 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:15,902 : INFO : EPOCH - 12 : training on 102975 raw words (96494 effective words) took 0.7s, 131074 effective words/s\n",
      "2022-10-02 11:46:16,470 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:16,567 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:16,673 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:16,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:16,688 : INFO : EPOCH - 13 : training on 102975 raw words (96494 effective words) took 0.8s, 123168 effective words/s\n",
      "2022-10-02 11:46:17,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:17,275 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:17,395 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:17,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:17,404 : INFO : EPOCH - 14 : training on 102975 raw words (96494 effective words) took 0.7s, 135115 effective words/s\n",
      "2022-10-02 11:46:17,981 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:18,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:18,246 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:18,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:18,249 : INFO : EPOCH - 15 : training on 102975 raw words (96494 effective words) took 0.8s, 114776 effective words/s\n",
      "2022-10-02 11:46:19,026 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:19,182 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:19,317 : INFO : EPOCH 16 - PROGRESS: at 89.56% examples, 81841 words/s, in_qsize 1, out_qsize 1\n",
      "2022-10-02 11:46:19,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:19,328 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:19,328 : INFO : EPOCH - 16 : training on 102975 raw words (96494 effective words) took 1.1s, 89546 effective words/s\n",
      "2022-10-02 11:46:19,945 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:20,044 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:20,172 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:20,174 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:20,174 : INFO : EPOCH - 17 : training on 102975 raw words (96494 effective words) took 0.8s, 114323 effective words/s\n",
      "2022-10-02 11:46:20,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:20,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:20,905 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:20,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:20,909 : INFO : EPOCH - 18 : training on 102975 raw words (96494 effective words) took 0.7s, 131668 effective words/s\n",
      "2022-10-02 11:46:21,482 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:21,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:21,713 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:21,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:21,715 : INFO : EPOCH - 19 : training on 102975 raw words (96494 effective words) took 0.8s, 120015 effective words/s\n",
      "2022-10-02 11:46:22,474 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:22,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:22,734 : INFO : EPOCH 20 - PROGRESS: at 89.56% examples, 85763 words/s, in_qsize 1, out_qsize 1\n",
      "2022-10-02 11:46:22,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:22,739 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:22,740 : INFO : EPOCH - 20 : training on 102975 raw words (96494 effective words) took 1.0s, 94311 effective words/s\n",
      "2022-10-02 11:46:23,341 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:23,430 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:23,568 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:23,596 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:23,597 : INFO : EPOCH - 21 : training on 102975 raw words (96494 effective words) took 0.9s, 112916 effective words/s\n",
      "2022-10-02 11:46:24,376 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:24,471 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:24,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:24,609 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:24,610 : INFO : EPOCH - 22 : training on 102975 raw words (96494 effective words) took 1.0s, 96434 effective words/s\n",
      "2022-10-02 11:46:25,138 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:25,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:25,352 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:25,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:25,360 : INFO : EPOCH - 23 : training on 102975 raw words (96494 effective words) took 0.7s, 129287 effective words/s\n",
      "2022-10-02 11:46:25,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:25,959 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:26,096 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:26,102 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:26,102 : INFO : EPOCH - 24 : training on 102975 raw words (96494 effective words) took 0.7s, 130637 effective words/s\n",
      "2022-10-02 11:46:26,655 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:26,746 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:26,873 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:26,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:26,880 : INFO : EPOCH - 25 : training on 102975 raw words (96494 effective words) took 0.8s, 124250 effective words/s\n",
      "2022-10-02 11:46:27,405 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:27,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:27,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:27,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:27,639 : INFO : EPOCH - 26 : training on 102975 raw words (96494 effective words) took 0.8s, 127410 effective words/s\n",
      "2022-10-02 11:46:28,141 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:28,226 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:28,350 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:28,353 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:28,354 : INFO : EPOCH - 27 : training on 102975 raw words (96494 effective words) took 0.7s, 135296 effective words/s\n",
      "2022-10-02 11:46:28,855 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:28,941 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:29,061 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:29,063 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:29,063 : INFO : EPOCH - 28 : training on 102975 raw words (96494 effective words) took 0.7s, 136350 effective words/s\n",
      "2022-10-02 11:46:29,600 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:29,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:29,849 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:29,851 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:29,852 : INFO : EPOCH - 29 : training on 102975 raw words (96494 effective words) took 0.8s, 122591 effective words/s\n",
      "2022-10-02 11:46:30,387 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:30,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:30,627 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:30,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:30,646 : INFO : EPOCH - 30 : training on 102975 raw words (96494 effective words) took 0.8s, 121758 effective words/s\n",
      "2022-10-02 11:46:31,371 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:31,503 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:31,660 : INFO : EPOCH 31 - PROGRESS: at 89.56% examples, 86381 words/s, in_qsize 1, out_qsize 1\n",
      "2022-10-02 11:46:31,661 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:31,663 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:31,664 : INFO : EPOCH - 31 : training on 102975 raw words (96494 effective words) took 1.0s, 95193 effective words/s\n",
      "2022-10-02 11:46:32,221 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:32,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:32,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:32,468 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:32,468 : INFO : EPOCH - 32 : training on 102975 raw words (96494 effective words) took 0.8s, 120183 effective words/s\n",
      "2022-10-02 11:46:33,018 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:33,116 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:33,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:33,249 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:33,249 : INFO : EPOCH - 33 : training on 102975 raw words (96494 effective words) took 0.8s, 123830 effective words/s\n",
      "2022-10-02 11:46:33,775 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:33,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:34,013 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:34,013 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:34,014 : INFO : EPOCH - 34 : training on 102975 raw words (96494 effective words) took 0.8s, 126529 effective words/s\n",
      "2022-10-02 11:46:34,531 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:34,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:34,766 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:34,768 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:34,769 : INFO : EPOCH - 35 : training on 102975 raw words (96494 effective words) took 0.8s, 128070 effective words/s\n",
      "2022-10-02 11:46:35,305 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:35,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:35,518 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:35,520 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:35,520 : INFO : EPOCH - 36 : training on 102975 raw words (96494 effective words) took 0.8s, 128634 effective words/s\n",
      "2022-10-02 11:46:36,033 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:36,117 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:36,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:36,253 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:36,253 : INFO : EPOCH - 37 : training on 102975 raw words (96494 effective words) took 0.7s, 132038 effective words/s\n",
      "2022-10-02 11:46:36,758 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:36,861 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:36,988 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:36,997 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:36,997 : INFO : EPOCH - 38 : training on 102975 raw words (96494 effective words) took 0.7s, 130280 effective words/s\n",
      "2022-10-02 11:46:37,504 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:37,595 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:37,719 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:37,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:37,731 : INFO : EPOCH - 39 : training on 102975 raw words (96494 effective words) took 0.7s, 131886 effective words/s\n",
      "2022-10-02 11:46:38,243 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:38,337 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:38,474 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:38,474 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:38,475 : INFO : EPOCH - 40 : training on 102975 raw words (96494 effective words) took 0.7s, 129988 effective words/s\n",
      "2022-10-02 11:46:38,990 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:39,092 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:39,222 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:39,225 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:39,225 : INFO : EPOCH - 41 : training on 102975 raw words (96494 effective words) took 0.7s, 129286 effective words/s\n",
      "2022-10-02 11:46:39,760 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:39,871 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:40,017 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:40,019 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:40,019 : INFO : EPOCH - 42 : training on 102975 raw words (96494 effective words) took 0.8s, 121875 effective words/s\n",
      "2022-10-02 11:46:40,549 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:40,644 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:40,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:40,776 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:40,776 : INFO : EPOCH - 43 : training on 102975 raw words (96494 effective words) took 0.8s, 127751 effective words/s\n",
      "2022-10-02 11:46:41,303 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:41,389 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:41,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:41,512 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:41,513 : INFO : EPOCH - 44 : training on 102975 raw words (96494 effective words) took 0.7s, 131305 effective words/s\n",
      "2022-10-02 11:46:42,013 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:42,105 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:42,228 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:42,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:42,237 : INFO : EPOCH - 45 : training on 102975 raw words (96494 effective words) took 0.7s, 133830 effective words/s\n",
      "2022-10-02 11:46:42,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:42,863 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:43,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:43,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:43,010 : INFO : EPOCH - 46 : training on 102975 raw words (96494 effective words) took 0.8s, 125082 effective words/s\n",
      "2022-10-02 11:46:43,568 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:43,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:43,794 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:43,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:43,795 : INFO : EPOCH - 47 : training on 102975 raw words (96494 effective words) took 0.8s, 123218 effective words/s\n",
      "2022-10-02 11:46:44,368 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:44,464 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:44,601 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:44,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:44,601 : INFO : EPOCH - 48 : training on 102975 raw words (96494 effective words) took 0.8s, 119996 effective words/s\n",
      "2022-10-02 11:46:45,141 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:45,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:45,366 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:45,367 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:45,367 : INFO : EPOCH - 49 : training on 102975 raw words (96494 effective words) took 0.8s, 127107 effective words/s\n",
      "2022-10-02 11:46:45,868 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:45,960 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:46,083 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:46,094 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:46,094 : INFO : EPOCH - 50 : training on 102975 raw words (96494 effective words) took 0.7s, 132973 effective words/s\n",
      "2022-10-02 11:46:46,607 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:46,687 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:46,813 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:46,823 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:46,823 : INFO : EPOCH - 51 : training on 102975 raw words (96494 effective words) took 0.7s, 133036 effective words/s\n",
      "2022-10-02 11:46:47,327 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:47,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:47,541 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:47,551 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:47,551 : INFO : EPOCH - 52 : training on 102975 raw words (96494 effective words) took 0.7s, 133167 effective words/s\n",
      "2022-10-02 11:46:48,109 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:48,206 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:48,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:48,344 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:48,344 : INFO : EPOCH - 53 : training on 102975 raw words (96494 effective words) took 0.8s, 122014 effective words/s\n",
      "2022-10-02 11:46:48,902 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:48,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:49,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:49,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:49,156 : INFO : EPOCH - 54 : training on 102975 raw words (96494 effective words) took 0.8s, 119219 effective words/s\n",
      "2022-10-02 11:46:50,011 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:50,120 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:50,267 : INFO : EPOCH 55 - PROGRESS: at 89.56% examples, 78658 words/s, in_qsize 1, out_qsize 1\n",
      "2022-10-02 11:46:50,268 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:50,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:50,283 : INFO : EPOCH - 55 : training on 102975 raw words (96494 effective words) took 1.1s, 85750 effective words/s\n",
      "2022-10-02 11:46:50,937 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:51,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:51,183 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:51,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:51,186 : INFO : EPOCH - 56 : training on 102975 raw words (96494 effective words) took 0.9s, 107020 effective words/s\n",
      "2022-10-02 11:46:51,901 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:51,998 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:52,155 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:52,161 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:52,162 : INFO : EPOCH - 57 : training on 102975 raw words (96494 effective words) took 1.0s, 99093 effective words/s\n",
      "2022-10-02 11:46:52,769 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:52,871 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:53,001 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:53,014 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:53,014 : INFO : EPOCH - 58 : training on 102975 raw words (96494 effective words) took 0.8s, 113582 effective words/s\n",
      "2022-10-02 11:46:53,532 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:53,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:53,752 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:53,762 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:53,763 : INFO : EPOCH - 59 : training on 102975 raw words (96494 effective words) took 0.7s, 129189 effective words/s\n",
      "2022-10-02 11:46:54,284 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:54,371 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:54,497 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:54,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:54,498 : INFO : EPOCH - 60 : training on 102975 raw words (96494 effective words) took 0.7s, 131645 effective words/s\n",
      "2022-10-02 11:46:55,001 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:55,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:55,221 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:55,223 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:55,224 : INFO : EPOCH - 61 : training on 102975 raw words (96494 effective words) took 0.7s, 133586 effective words/s\n",
      "2022-10-02 11:46:55,770 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:55,887 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:55,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:56,029 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:56,029 : INFO : EPOCH - 62 : training on 102975 raw words (96494 effective words) took 0.8s, 120311 effective words/s\n",
      "2022-10-02 11:46:56,564 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:56,656 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:56,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:56,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:56,783 : INFO : EPOCH - 63 : training on 102975 raw words (96494 effective words) took 0.8s, 128645 effective words/s\n",
      "2022-10-02 11:46:57,296 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:57,389 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:57,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:57,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:57,515 : INFO : EPOCH - 64 : training on 102975 raw words (96494 effective words) took 0.7s, 132060 effective words/s\n",
      "2022-10-02 11:46:58,033 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:58,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:58,248 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:58,251 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:58,251 : INFO : EPOCH - 65 : training on 102975 raw words (96494 effective words) took 0.7s, 131457 effective words/s\n",
      "2022-10-02 11:46:58,794 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:58,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:59,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:59,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:59,027 : INFO : EPOCH - 66 : training on 102975 raw words (96494 effective words) took 0.8s, 124847 effective words/s\n",
      "2022-10-02 11:46:59,544 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:46:59,627 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:46:59,760 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:46:59,760 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:46:59,761 : INFO : EPOCH - 67 : training on 102975 raw words (96494 effective words) took 0.7s, 132233 effective words/s\n",
      "2022-10-02 11:47:00,282 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:00,391 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:00,523 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:00,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:00,534 : INFO : EPOCH - 68 : training on 102975 raw words (96494 effective words) took 0.8s, 125343 effective words/s\n",
      "2022-10-02 11:47:01,152 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:01,241 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:01,370 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:01,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:01,371 : INFO : EPOCH - 69 : training on 102975 raw words (96494 effective words) took 0.8s, 115798 effective words/s\n",
      "2022-10-02 11:47:01,939 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:02,023 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:02,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:02,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:02,155 : INFO : EPOCH - 70 : training on 102975 raw words (96494 effective words) took 0.8s, 123383 effective words/s\n",
      "2022-10-02 11:47:02,662 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:02,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:02,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:02,889 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:02,889 : INFO : EPOCH - 71 : training on 102975 raw words (96494 effective words) took 0.7s, 132193 effective words/s\n",
      "2022-10-02 11:47:03,398 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:03,490 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:03,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:03,619 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:03,620 : INFO : EPOCH - 72 : training on 102975 raw words (96494 effective words) took 0.7s, 132736 effective words/s\n",
      "2022-10-02 11:47:04,137 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:04,219 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:04,336 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:04,348 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:04,349 : INFO : EPOCH - 73 : training on 102975 raw words (96494 effective words) took 0.7s, 132667 effective words/s\n",
      "2022-10-02 11:47:04,886 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:04,957 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:05,088 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:05,089 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:05,089 : INFO : EPOCH - 74 : training on 102975 raw words (96494 effective words) took 0.7s, 130630 effective words/s\n",
      "2022-10-02 11:47:05,585 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:05,678 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:05,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:05,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:05,809 : INFO : EPOCH - 75 : training on 102975 raw words (96494 effective words) took 0.7s, 134475 effective words/s\n",
      "2022-10-02 11:47:06,326 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:06,410 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:06,560 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:06,562 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:06,563 : INFO : EPOCH - 76 : training on 102975 raw words (96494 effective words) took 0.8s, 128518 effective words/s\n",
      "2022-10-02 11:47:07,080 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:07,175 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:07,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:07,311 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:07,311 : INFO : EPOCH - 77 : training on 102975 raw words (96494 effective words) took 0.7s, 129569 effective words/s\n",
      "2022-10-02 11:47:07,843 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:07,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:08,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:08,071 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:08,072 : INFO : EPOCH - 78 : training on 102975 raw words (96494 effective words) took 0.8s, 127160 effective words/s\n",
      "2022-10-02 11:47:08,584 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:08,683 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:08,809 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:08,810 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:08,810 : INFO : EPOCH - 79 : training on 102975 raw words (96494 effective words) took 0.7s, 131468 effective words/s\n",
      "2022-10-02 11:47:09,334 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:09,425 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:09,559 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:09,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:09,566 : INFO : EPOCH - 80 : training on 102975 raw words (96494 effective words) took 0.8s, 127887 effective words/s\n",
      "2022-10-02 11:47:10,159 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:10,245 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:10,347 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:10,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:10,369 : INFO : EPOCH - 81 : training on 102975 raw words (96494 effective words) took 0.8s, 120706 effective words/s\n",
      "2022-10-02 11:47:10,882 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:10,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:11,110 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:11,110 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:11,111 : INFO : EPOCH - 82 : training on 102975 raw words (96494 effective words) took 0.7s, 130491 effective words/s\n",
      "2022-10-02 11:47:11,645 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:11,740 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:11,884 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:11,888 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:11,889 : INFO : EPOCH - 83 : training on 102975 raw words (96494 effective words) took 0.8s, 124364 effective words/s\n",
      "2022-10-02 11:47:12,455 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:12,554 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:12,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:12,688 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:12,688 : INFO : EPOCH - 84 : training on 102975 raw words (96494 effective words) took 0.8s, 120902 effective words/s\n",
      "2022-10-02 11:47:13,222 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:13,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:13,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:13,427 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:13,427 : INFO : EPOCH - 85 : training on 102975 raw words (96494 effective words) took 0.7s, 131296 effective words/s\n",
      "2022-10-02 11:47:13,937 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:14,015 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:14,136 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:14,145 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:14,145 : INFO : EPOCH - 86 : training on 102975 raw words (96494 effective words) took 0.7s, 134754 effective words/s\n",
      "2022-10-02 11:47:14,658 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:14,739 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:14,857 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:14,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:14,859 : INFO : EPOCH - 87 : training on 102975 raw words (96494 effective words) took 0.7s, 135816 effective words/s\n",
      "2022-10-02 11:47:15,370 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:15,461 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:15,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:15,582 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:15,582 : INFO : EPOCH - 88 : training on 102975 raw words (96494 effective words) took 0.7s, 133846 effective words/s\n",
      "2022-10-02 11:47:16,088 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:16,178 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:16,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:16,303 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:16,304 : INFO : EPOCH - 89 : training on 102975 raw words (96494 effective words) took 0.7s, 134043 effective words/s\n",
      "2022-10-02 11:47:16,814 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:16,905 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:17,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:17,028 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:17,029 : INFO : EPOCH - 90 : training on 102975 raw words (96494 effective words) took 0.7s, 133426 effective words/s\n",
      "2022-10-02 11:47:17,536 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:17,624 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:17,745 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:17,752 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:17,752 : INFO : EPOCH - 91 : training on 102975 raw words (96494 effective words) took 0.7s, 133706 effective words/s\n",
      "2022-10-02 11:47:18,271 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:18,351 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:18,461 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:18,479 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:18,480 : INFO : EPOCH - 92 : training on 102975 raw words (96494 effective words) took 0.7s, 133259 effective words/s\n",
      "2022-10-02 11:47:18,980 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:19,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:19,190 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:19,202 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:19,203 : INFO : EPOCH - 93 : training on 102975 raw words (96494 effective words) took 0.7s, 133836 effective words/s\n",
      "2022-10-02 11:47:19,705 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:19,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:19,932 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:19,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:19,932 : INFO : EPOCH - 94 : training on 102975 raw words (96494 effective words) took 0.7s, 132916 effective words/s\n",
      "2022-10-02 11:47:20,445 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:20,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:20,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:20,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:20,655 : INFO : EPOCH - 95 : training on 102975 raw words (96494 effective words) took 0.7s, 134075 effective words/s\n",
      "2022-10-02 11:47:21,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:21,334 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:21,454 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:21,455 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:21,456 : INFO : EPOCH - 96 : training on 102975 raw words (96494 effective words) took 0.8s, 120877 effective words/s\n",
      "2022-10-02 11:47:21,968 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:22,056 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:22,168 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:22,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:22,186 : INFO : EPOCH - 97 : training on 102975 raw words (96494 effective words) took 0.7s, 132402 effective words/s\n",
      "2022-10-02 11:47:22,692 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:22,785 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:22,897 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:22,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:22,910 : INFO : EPOCH - 98 : training on 102975 raw words (96494 effective words) took 0.7s, 133564 effective words/s\n",
      "2022-10-02 11:47:23,417 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:23,502 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:23,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:23,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:23,638 : INFO : EPOCH - 99 : training on 102975 raw words (96494 effective words) took 0.7s, 132875 effective words/s\n",
      "2022-10-02 11:47:24,151 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-10-02 11:47:24,242 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-10-02 11:47:24,363 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-10-02 11:47:24,366 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-10-02 11:47:24,367 : INFO : EPOCH - 100 : training on 102975 raw words (96494 effective words) took 0.7s, 133089 effective words/s\n",
      "2022-10-02 11:47:24,367 : INFO : FastText lifecycle event {'msg': 'training on 10297500 raw words (9649400 effective words) took 77.9s, 123833 effective words/s', 'datetime': '2022-10-02T11:47:24.367540', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-10-02 11:47:24,458 : INFO : FastText lifecycle event {'params': 'FastText(vocab=2791, vector_size=300, alpha=0.025)', 'datetime': '2022-10-02T11:47:24.458349', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Defining values for parameters\n",
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    " \n",
    "%time\n",
    "fast_Text_model = FastText(tk_description,\n",
    "                      vector_size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      workers = 4,\n",
    "                      sg=1,\n",
    "                      epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load Gensim FastText word embeddings Python model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 11:55:21,878 : INFO : FastText lifecycle event {'fname_or_handle': 'models/FastText/fast_Text_model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-10-02T11:55:21.877997', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2022-10-02 11:55:21,880 : INFO : storing np array 'vectors_ngrams' to models/FastText/fast_Text_model.wv.vectors_ngrams.npy\n",
      "2022-10-02 11:56:03,717 : INFO : not storing attribute buckets_word\n",
      "2022-10-02 11:56:03,718 : INFO : not storing attribute vectors\n",
      "2022-10-02 11:56:03,718 : INFO : not storing attribute cum_table\n",
      "2022-10-02 11:56:03,730 : INFO : saved models/FastText/fast_Text_model\n",
      "2022-10-02 11:56:03,731 : INFO : loading Word2Vec object from models/FastText/fast_Text_model\n",
      "2022-10-02 11:56:03,733 : INFO : loading wv recursively from models/FastText/fast_Text_model.wv.* with mmap=None\n",
      "2022-10-02 11:56:03,734 : INFO : loading vectors_ngrams from models/FastText/fast_Text_model.wv.vectors_ngrams.npy with mmap=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded fast_text_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 11:56:07,251 : INFO : setting ignored attribute buckets_word to None\n",
      "2022-10-02 11:56:07,254 : INFO : setting ignored attribute vectors to None\n",
      "2022-10-02 11:56:07,503 : INFO : setting ignored attribute cum_table to None\n",
      "2022-10-02 11:56:07,527 : INFO : FastText lifecycle event {'fname': 'models/FastText/fast_Text_model', 'datetime': '2022-10-02T11:56:07.527193', 'gensim': '4.1.2', 'python': '3.9.12 (main, Apr  5 2022, 01:53:17) \\n[Clang 12.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "fast_text_model_file = 'fast_text_model'\n",
    "\n",
    "# Save fastText gensim model\n",
    "fast_Text_model.save(\"models/FastText/fast_Text_model\")\n",
    "print(f'Successfully loaded {fast_text_model_file}')\n",
    "\n",
    "# Load saved gensim fastText model\n",
    "fast_Text_model = Word2Vec.load(\"models/FastText/fast_Text_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The .py format of the jupyter notebook\n",
    "for fname in os.listdir():\n",
    "    if fname.endswith('ipynb'):\n",
    "        os.system(f'jupyter nbconvert {fname} --to python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3 style=\"color:#ffc0cb;font-size:50px;font-family:Georgia;text-align:center;\"><strong>Summary</strong></h3>\n",
    "\n",
    "+ there are 3 different types of feature representation of documents that you need to\n",
    "build in this task, including count vector, two document embeddings (one TF-IDF weighted, and one\n",
    "unweighted version).\n",
    "\n",
    "+ Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "Bag of Words vectors are easy to interpret. However, TF-IDF usually performs better in machine learning models.\n",
    "\n",
    "+ The idea of TF-IDF is to reflect the importance of a word to its document or sentence by normalizing the words which occur frequently in the collection of documents. Recall task1, we also did a similar task by re,o,ving the top 50 most frequent words based on document frequency becase a word appears in almost every document means itâ€™s not significant for the classification.\n",
    "\n",
    "+ Either Binary, Count, and TF-IDF models completely depends on the frequency of occurrence, it doesnâ€™t take the context of words into consideration (e.g. equal and identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Discussion\n",
    "\n",
    ">> * Drawbacks of using a Bag-of-Words model is computationally expensive: If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too. Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid). We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text.\n",
    "\n",
    ">> * Task 2 only consider Unigrams (the single unique words in a sentence) which would lead to misleading for words the meaning is constructed by 2 or above number of words. Therefore, we would consider to implement Bigrams, or Trigrams.\n",
    "\n",
    ">> * The downstream analysis will determine how we should treat the text. Should we pre-process the `title` information if we put it into the classification model? Or ought we only concentrate on the `description` itself? Based on the analytical work's objectives, we can choose the appropriate downstream analysis task. For instance, we may use the whole document when doing tasks like document classification and clustering. In contrast, performing functions like document summary and information retrieval, a smaller unit, such as a paragraph or phrase, may be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Reference\n",
    "\n",
    "+ https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n",
    "+ https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
    "+ https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "+ [FastText paper from Facebook](https://arxiv.org/pdf/1607.04606.pdf)\n",
    "+ Gensimâ€™s fastText. https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
